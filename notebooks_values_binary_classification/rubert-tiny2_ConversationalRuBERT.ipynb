{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3522406",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification\n",
    "from transformers import get_linear_schedule_with_warmup # AdamW (deprec)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30103e31",
   "metadata": {},
   "source": [
    "# 1. Fine-tune rubert-tiny2, extract embeddings and train SVM, LogitBoost, and LogitRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f4b1fb",
   "metadata": {},
   "source": [
    "batch_size = 8, epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecf9506a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrained model\n",
    "BASE_BERT = 'cointegrated/rubert-tiny2'\n",
    "\n",
    "# data balance\n",
    "major_class_exrta_w = 1.2\n",
    "\n",
    "# data loading\n",
    "max_length = int(2048*0.75)\n",
    "batch_size = 8\n",
    "\n",
    "# train\n",
    "epochs = 5\n",
    "\n",
    "#not random\n",
    "seed_val = 42\n",
    "\n",
    "#save\n",
    "bert_path = f'./temp/models/v2_{batch_size}_{max_length}_{major_class_exrta_w}/'\n",
    "\n",
    "###################\n",
    "# compute on GPU #0\n",
    "device_id = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2c8b8ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.mkdir(bert_path)\n",
    "    os.mkdir(bert_path+'epochs')\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    os.mkdir(bert_path+'epochs')\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1dea7e20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=3)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:{}\".format(str(device_id)) if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffa5129",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8497c159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5035\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Думаете, что умеете пользоваться фотошопом?...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>...Самое страшное - это когда ты стоишь под х...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Друзья мои! Поддержим дочку моей подруги! Про...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Мой новый дневник, читаем, коментим :)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>РУССКИЙ  КРЫМ - МИФ для быдла! (о чем молчат ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0     Думаете, что умеете пользоваться фотошопом?...    0.0\n",
       "1   ...Самое страшное - это когда ты стоишь под х...    1.0\n",
       "2   Друзья мои! Поддержим дочку моей подруги! Про...    1.0\n",
       "3             Мой новый дневник, читаем, коментим :)    0.0\n",
       "4   РУССКИЙ  КРЫМ - МИФ для быдла! (о чем молчат ...    0.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fold='../data/data_for_binary_classification/'\n",
    "file_='chatGPT_3_instr0_withEx_temp0_train_all_updated.csv'\n",
    "\n",
    "df=pd.read_csv(fold+file_, sep=\"|\", encoding ='utf-8')[['text', 'final_label']] #'label_crowd', 'gpt_result', \n",
    "df=df.rename(columns={'final_label':'label'})\n",
    "print (df.shape[0])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7e4d8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.label = df.label.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29d71fdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    3301\n",
       "1    1734\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine 'doesn't reflect' and 'spam' classes\n",
    "df.label=df.label.replace(3, 0)\n",
    "df.label.value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b69c2d9",
   "metadata": {},
   "source": [
    "## TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5543afb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0, 1: 1}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "possible_labels = df.label.unique()\n",
    "\n",
    "label_dict = {}\n",
    "for index, possible_label in enumerate(possible_labels):\n",
    "    label_dict[possible_label] = index\n",
    "label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "792bd120",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'] = df.label.replace(label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4209a8d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.63553974, 1.45184544])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "class_w = compute_class_weight('balanced', classes=df.label.unique(), y=df.label)\n",
    "\n",
    "# fix disbalance to major class\n",
    "major_class_idx = np.argmin(class_w)\n",
    "class_w[major_class_idx] = class_w[major_class_idx]/major_class_exrta_w\n",
    "class_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "dcb2b3ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5035,), (5035,))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.text\n",
    "Y = df.label\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8c2a05c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X.values, Y.values, test_size = .20, stratify = Y.values, random_state = 87)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0be8557f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4028,), (1007,), 5035)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape , X_val.shape , X_train.shape[0] + X_val.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9277df1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.74 s, sys: 128 ms, total: 4.87 s\n",
      "Wall time: 5.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(BASE_BERT, \n",
    "                                          do_lower_case=False)\n",
    "                                          \n",
    "encoded_data_train = tokenizer.batch_encode_plus(\n",
    "    X_train, \n",
    "    add_special_tokens=True, \n",
    "    return_attention_mask=True,\n",
    "    return_token_type_ids=False,\n",
    "    padding='max_length', # можно поставить True  #'max_length'\n",
    "    truncation=True,\n",
    "    max_length=max_length, \n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "encoded_data_val = tokenizer.batch_encode_plus(\n",
    "    X_val, \n",
    "    add_special_tokens=True, \n",
    "    return_attention_mask=True,\n",
    "    return_token_type_ids=False,\n",
    "    padding='max_length',  #'max_length',\n",
    "    truncation=True,\n",
    "    max_length=max_length, \n",
    "    return_tensors='pt'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ae5d602b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_train = encoded_data_train['input_ids']\n",
    "attention_masks_train = encoded_data_train['attention_mask']\n",
    "labels_train = torch.tensor(y_train)\n",
    "\n",
    "input_ids_val = encoded_data_val['input_ids']\n",
    "attention_masks_val = encoded_data_val['attention_mask']\n",
    "labels_val = torch.tensor(y_val)\n",
    "\n",
    "dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n",
    "dataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4e44fb8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cointegrated/rubert-tiny2 were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(83828, 312, padding_idx=0)\n",
       "      (position_embeddings): Embedding(2048, 312)\n",
       "      (token_type_embeddings): Embedding(2, 312)\n",
       "      (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (key): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (value): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=312, out_features=600, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=600, out_features=312, bias=True)\n",
       "            (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (key): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (value): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=312, out_features=600, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=600, out_features=312, bias=True)\n",
       "            (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (key): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (value): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=312, out_features=600, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=600, out_features=312, bias=True)\n",
       "            (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=312, out_features=312, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=312, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(BASE_BERT,\n",
    "                                                      num_labels=len(label_dict),\n",
    "                                                      output_attentions=False,\n",
    "                                                      output_hidden_states=False)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "cd1198d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train, \n",
    "                              sampler=RandomSampler(dataset_train), #SequentialSampler\n",
    "                              batch_size=batch_size)\n",
    "\n",
    "dataloader_validation = DataLoader(dataset_val, \n",
    "                                   sampler=SequentialSampler(dataset_val), \n",
    "                                   batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c29014cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr=1e-5, \n",
    "                  eps=1e-8)\n",
    "                  \n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0,\n",
    "                                            num_training_steps=len(dataloader_train)*epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "33794116",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def f1_score_func(preds, labels):\n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return f1_score(labels_flat, preds_flat, average='weighted'), f1_score(labels_flat, preds_flat, average='macro')\n",
    "\n",
    "def accuracy_per_class(preds, labels):\n",
    "    label_dict_inverse = {v: k for k, v in label_dict.items()}\n",
    "    \n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "\n",
    "    for label in np.unique(labels_flat):\n",
    "        y_preds = preds_flat[labels_flat==label]\n",
    "        y_true = labels_flat[labels_flat==label]\n",
    "        print(f'Class: {label_dict_inverse[label]}')\n",
    "        print(f'Accuracy: {len(y_preds[y_preds==label])}/{len(y_true)} = {len(y_preds[y_preds==label])/len(y_true)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ca838004",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5d748c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader_val):\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    loss_val_total = 0\n",
    "    predictions, true_vals = [], []\n",
    "    \n",
    "    for batch in dataloader_val:\n",
    "        \n",
    "#         batch = tuple(b.to(device) for b in batch)\n",
    "        \n",
    "        inputs = {'input_ids':      batch[0].to(device),\n",
    "                  'attention_mask': batch[1].to(device),\n",
    "                  'labels':         batch[2].to(device),\n",
    "                 }\n",
    "\n",
    "        with torch.no_grad():        \n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        loss_val_total += loss.item()\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = inputs['labels'].cpu().numpy()\n",
    "        predictions.append(logits)\n",
    "        true_vals.append(label_ids)\n",
    "    \n",
    "    loss_val_avg = loss_val_total/len(dataloader_val) \n",
    "    \n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    true_vals = np.concatenate(true_vals, axis=0)\n",
    "            \n",
    "    return loss_val_avg, predictions, true_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c031e606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "def save_checpoint(model, optimizer, output_model):\n",
    "    # save\n",
    "    torch.save({'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict()}, output_model)\n",
    "\n",
    "# save(model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "67411466",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2148685f5c340398970074df279d6ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 0:   0%|          | 0/504 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0\n",
      "Training loss: 0.6455677735900122\n",
      "Validation loss: 0.6234295635469376\n",
      "F1 Score (Weighted/Macro): (0.656251957711443, 0.6468783185467086)\n",
      "Class: 0\n",
      "Accuracy: 370/660 = 0.5606060606060606\n",
      "\n",
      "Class: 1\n",
      "Accuracy: 284/347 = 0.8184438040345822\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/504 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1\n",
      "Training loss: 0.5627760663037262\n",
      "Validation loss: 0.5686113704291601\n",
      "F1 Score (Weighted/Macro): (0.7084040459663059, 0.6914517516401745)\n",
      "Class: 0\n",
      "Accuracy: 442/660 = 0.6696969696969697\n",
      "\n",
      "Class: 1\n",
      "Accuracy: 264/347 = 0.760806916426513\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/504 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2\n",
      "Training loss: 0.5233887325794924\n",
      "Validation loss: 0.556405167849291\n",
      "F1 Score (Weighted/Macro): (0.7316920650851454, 0.7120496307531641)\n",
      "Class: 0\n",
      "Accuracy: 476/660 = 0.7212121212121212\n",
      "\n",
      "Class: 1\n",
      "Accuracy: 255/347 = 0.7348703170028819\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3:   0%|          | 0/504 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3\n",
      "Training loss: 0.5005263998690579\n",
      "Validation loss: 0.563113215660292\n",
      "F1 Score (Weighted/Macro): (0.7297478046872117, 0.7099630338745637)\n",
      "Class: 0\n",
      "Accuracy: 475/660 = 0.7196969696969697\n",
      "\n",
      "Class: 1\n",
      "Accuracy: 254/347 = 0.7319884726224783\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4:   0%|          | 0/504 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4\n",
      "Training loss: 0.4895340929340039\n",
      "Validation loss: 0.5619315552333045\n",
      "F1 Score (Weighted/Macro): (0.7324875421414411, 0.7124270358566647)\n",
      "Class: 0\n",
      "Accuracy: 479/660 = 0.7257575757575757\n",
      "\n",
      "Class: 1\n",
      "Accuracy: 253/347 = 0.729106628242075\n",
      "\n"
     ]
    }
   ],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss(weight=torch.FloatTensor(class_w), reduction = 'mean').to(device)\n",
    "scores = {}\n",
    "\n",
    "for epoch in tqdm(range(0,5)): # 1,epochs+1\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    loss_train_total = 0\n",
    "\n",
    "    progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\n",
    "    for batch in progress_bar:\n",
    "\n",
    "        model.zero_grad()\n",
    "        \n",
    "#         batch = tuple(b.to(device) for b in batch)\n",
    "        \n",
    "        inputs = {'input_ids':      batch[0].to(device),\n",
    "                  'attention_mask': batch[1].to(device),\n",
    "                  'labels':         batch[2].to(device),\n",
    "                 }       \n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs['logits']\n",
    "        \n",
    "        \n",
    "        loss = criterion(logits, inputs['labels'])\n",
    "        loss_train_total += loss.item()    \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        \n",
    "        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\n",
    "                 \n",
    "        \n",
    "    tqdm.write(f'\\nEpoch {epoch}')\n",
    "    \n",
    "    loss_train_avg = loss_train_total/len(dataloader_train)            \n",
    "    tqdm.write(f'Training loss: {loss_train_avg}')\n",
    "    \n",
    "    val_loss, predictions, true_vals = evaluate(dataloader_validation)\n",
    "    val_f1_w, val_f1_macro  = f1_score_func(predictions, true_vals)\n",
    "    tqdm.write(f'Validation loss: {val_loss}')\n",
    "    tqdm.write(f'F1 Score (Weighted/Macro): {(val_f1_w, val_f1_macro)}')\n",
    "    accuracy_per_class(predictions, true_vals)\n",
    "    \n",
    "    scores[epoch] = {'Training loss':loss_train_avg,\n",
    "                     'Validation loss':val_loss,\n",
    "                     'F1 Score (Weighted/Macro)':(val_f1_w, val_f1_macro) \n",
    "                    }\n",
    "    \n",
    "    # save checkpoint\n",
    "    save_checpoint(model, optimizer, bert_path+f'epochs/{epoch}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cf1f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(bert_path+f'epochs/scores.json', 'w') as f:\n",
    "    json.dump(scores, f)\n",
    "\n",
    "# load best\n",
    "\n",
    "best_epoch = np.argmin([scores[e]['Validation loss'] for e in scores]) + 1\n",
    "print(f'best_epoch #{best_epoch}')\n",
    "\n",
    "checkpoint = torch.load(bert_path+f'epochs/{best_epoch}', map_location='cpu')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "model.save_pretrained(bert_path)\n",
    "tokenizer.save_pretrained(bert_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a1828b35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./temp/models/v2_8_1536_1.2/'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#! SAVE MODEL !\n",
    "model.save_pretrained(bert_path)\n",
    "tokenizer.save_pretrained(bert_path)\n",
    "bert_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c4c2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, predictions, true_vals = evaluate(dataloader_validation)\n",
    "accuracy_per_class(predictions, true_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c798683",
   "metadata": {},
   "source": [
    "## export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "61c7b6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rc_path = './models/v2_8_1536_1.2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b81e8d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "748c2d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# b_model = BertModel.from_pretrained(bert_path)\n",
    "# b_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "73ebf873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# b_model.save_pretrained(rc_path)\n",
    "# tokenizer.save_pretrained(rc_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "34a2bc53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./models/v2_8_1536_1.2/ were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = BertModel.from_pretrained(rc_path)\n",
    "tokenizer = BertTokenizer.from_pretrained(rc_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff48f21",
   "metadata": {},
   "source": [
    "Take embeddings forn fine-tuned rubert-tiny2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d6ac69e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_emb = model.base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "36cc8d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_bert_cls(text, model, tokenizer):\n",
    "    t = tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**{k: v.to(model.device) for k, v in t.items()})\n",
    "    embeddings = model_output.last_hidden_state[:, 0, :]\n",
    "    embeddings = torch.nn.functional.normalize(embeddings)\n",
    "    return embeddings[0].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3bae82ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a852316cffa54569bd8422fa138aa5c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5035 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28min 6s, sys: 1min 46s, total: 29min 52s\n",
      "Wall time: 44.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from tqdm.notebook import tqdm\n",
    "emb_list = list()\n",
    "it = 0\n",
    "for s in tqdm(df.text.values):\n",
    "    emb_list.append(embed_bert_cls(s, model_emb, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d9ed784a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>302</th>\n",
       "      <th>303</th>\n",
       "      <th>304</th>\n",
       "      <th>305</th>\n",
       "      <th>306</th>\n",
       "      <th>307</th>\n",
       "      <th>308</th>\n",
       "      <th>309</th>\n",
       "      <th>310</th>\n",
       "      <th>311</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.018464</td>\n",
       "      <td>-0.025443</td>\n",
       "      <td>-0.005412</td>\n",
       "      <td>-0.067210</td>\n",
       "      <td>-0.008098</td>\n",
       "      <td>-0.042848</td>\n",
       "      <td>0.048566</td>\n",
       "      <td>0.038053</td>\n",
       "      <td>-0.051750</td>\n",
       "      <td>0.016248</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013072</td>\n",
       "      <td>-0.137592</td>\n",
       "      <td>-0.063010</td>\n",
       "      <td>-0.102285</td>\n",
       "      <td>-0.017897</td>\n",
       "      <td>0.058684</td>\n",
       "      <td>-0.000319</td>\n",
       "      <td>0.078772</td>\n",
       "      <td>-0.005704</td>\n",
       "      <td>0.041218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.087185</td>\n",
       "      <td>-0.013556</td>\n",
       "      <td>0.034466</td>\n",
       "      <td>-0.041398</td>\n",
       "      <td>-0.009335</td>\n",
       "      <td>0.038518</td>\n",
       "      <td>0.072062</td>\n",
       "      <td>-0.126953</td>\n",
       "      <td>0.072556</td>\n",
       "      <td>-0.010305</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009245</td>\n",
       "      <td>0.031439</td>\n",
       "      <td>0.044652</td>\n",
       "      <td>0.031633</td>\n",
       "      <td>-0.081807</td>\n",
       "      <td>-0.021026</td>\n",
       "      <td>-0.011231</td>\n",
       "      <td>0.007678</td>\n",
       "      <td>-0.057428</td>\n",
       "      <td>0.005134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.121493</td>\n",
       "      <td>0.024607</td>\n",
       "      <td>-0.038873</td>\n",
       "      <td>-0.034820</td>\n",
       "      <td>-0.033760</td>\n",
       "      <td>-0.023758</td>\n",
       "      <td>0.055283</td>\n",
       "      <td>-0.082305</td>\n",
       "      <td>-0.030336</td>\n",
       "      <td>-0.063620</td>\n",
       "      <td>...</td>\n",
       "      <td>0.074011</td>\n",
       "      <td>0.065398</td>\n",
       "      <td>0.025788</td>\n",
       "      <td>0.005126</td>\n",
       "      <td>0.049230</td>\n",
       "      <td>-0.076790</td>\n",
       "      <td>0.114930</td>\n",
       "      <td>0.083010</td>\n",
       "      <td>0.026723</td>\n",
       "      <td>-0.053610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.020010</td>\n",
       "      <td>0.041242</td>\n",
       "      <td>-0.016424</td>\n",
       "      <td>-0.036992</td>\n",
       "      <td>-0.039034</td>\n",
       "      <td>0.003855</td>\n",
       "      <td>0.005490</td>\n",
       "      <td>0.042822</td>\n",
       "      <td>-0.038598</td>\n",
       "      <td>-0.078856</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.016536</td>\n",
       "      <td>-0.062215</td>\n",
       "      <td>-0.011497</td>\n",
       "      <td>-0.130887</td>\n",
       "      <td>0.024420</td>\n",
       "      <td>0.049948</td>\n",
       "      <td>-0.068521</td>\n",
       "      <td>-0.050156</td>\n",
       "      <td>0.030229</td>\n",
       "      <td>0.039134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.026711</td>\n",
       "      <td>-0.061871</td>\n",
       "      <td>0.018863</td>\n",
       "      <td>-0.026486</td>\n",
       "      <td>0.046739</td>\n",
       "      <td>-0.060128</td>\n",
       "      <td>0.027036</td>\n",
       "      <td>0.004546</td>\n",
       "      <td>-0.053569</td>\n",
       "      <td>0.005665</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000268</td>\n",
       "      <td>-0.096092</td>\n",
       "      <td>-0.059798</td>\n",
       "      <td>-0.059040</td>\n",
       "      <td>0.025562</td>\n",
       "      <td>0.083468</td>\n",
       "      <td>0.007707</td>\n",
       "      <td>0.097353</td>\n",
       "      <td>0.100729</td>\n",
       "      <td>-0.048607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5030</th>\n",
       "      <td>0.090720</td>\n",
       "      <td>0.068379</td>\n",
       "      <td>0.007549</td>\n",
       "      <td>-0.037703</td>\n",
       "      <td>0.008094</td>\n",
       "      <td>-0.022078</td>\n",
       "      <td>0.057416</td>\n",
       "      <td>0.074114</td>\n",
       "      <td>0.012745</td>\n",
       "      <td>0.036362</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039732</td>\n",
       "      <td>-0.056965</td>\n",
       "      <td>-0.118393</td>\n",
       "      <td>0.032156</td>\n",
       "      <td>0.018497</td>\n",
       "      <td>0.021067</td>\n",
       "      <td>0.037554</td>\n",
       "      <td>0.101765</td>\n",
       "      <td>0.023785</td>\n",
       "      <td>0.004840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5031</th>\n",
       "      <td>0.143055</td>\n",
       "      <td>0.031883</td>\n",
       "      <td>0.000566</td>\n",
       "      <td>-0.107946</td>\n",
       "      <td>-0.004803</td>\n",
       "      <td>-0.010487</td>\n",
       "      <td>0.043309</td>\n",
       "      <td>0.042991</td>\n",
       "      <td>-0.025998</td>\n",
       "      <td>0.029961</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030708</td>\n",
       "      <td>0.031169</td>\n",
       "      <td>0.063458</td>\n",
       "      <td>-0.007555</td>\n",
       "      <td>-0.065752</td>\n",
       "      <td>0.068371</td>\n",
       "      <td>-0.064800</td>\n",
       "      <td>0.034165</td>\n",
       "      <td>0.053517</td>\n",
       "      <td>-0.010068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5032</th>\n",
       "      <td>0.124135</td>\n",
       "      <td>0.004982</td>\n",
       "      <td>-0.001954</td>\n",
       "      <td>-0.043581</td>\n",
       "      <td>-0.027300</td>\n",
       "      <td>-0.040936</td>\n",
       "      <td>0.006083</td>\n",
       "      <td>-0.125258</td>\n",
       "      <td>-0.009013</td>\n",
       "      <td>-0.044418</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051877</td>\n",
       "      <td>0.048784</td>\n",
       "      <td>0.106491</td>\n",
       "      <td>0.048471</td>\n",
       "      <td>-0.019057</td>\n",
       "      <td>-0.011693</td>\n",
       "      <td>0.046124</td>\n",
       "      <td>-0.033521</td>\n",
       "      <td>0.045135</td>\n",
       "      <td>-0.047027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5033</th>\n",
       "      <td>0.050855</td>\n",
       "      <td>0.037297</td>\n",
       "      <td>0.013623</td>\n",
       "      <td>-0.040512</td>\n",
       "      <td>-0.012929</td>\n",
       "      <td>-0.042989</td>\n",
       "      <td>0.068927</td>\n",
       "      <td>0.009016</td>\n",
       "      <td>0.003902</td>\n",
       "      <td>0.060874</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009632</td>\n",
       "      <td>-0.044231</td>\n",
       "      <td>0.059576</td>\n",
       "      <td>-0.017738</td>\n",
       "      <td>-0.072191</td>\n",
       "      <td>-0.035146</td>\n",
       "      <td>-0.112178</td>\n",
       "      <td>0.042907</td>\n",
       "      <td>-0.006640</td>\n",
       "      <td>-0.064849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5034</th>\n",
       "      <td>0.008289</td>\n",
       "      <td>0.058433</td>\n",
       "      <td>0.002247</td>\n",
       "      <td>-0.033897</td>\n",
       "      <td>-0.067209</td>\n",
       "      <td>-0.010444</td>\n",
       "      <td>0.017773</td>\n",
       "      <td>-0.102263</td>\n",
       "      <td>0.044758</td>\n",
       "      <td>0.110103</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.046597</td>\n",
       "      <td>0.076197</td>\n",
       "      <td>0.102915</td>\n",
       "      <td>0.000366</td>\n",
       "      <td>-0.096565</td>\n",
       "      <td>-0.038375</td>\n",
       "      <td>-0.022783</td>\n",
       "      <td>-0.060977</td>\n",
       "      <td>-0.062677</td>\n",
       "      <td>-0.003916</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5035 rows × 312 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6    \\\n",
       "0    -0.018464 -0.025443 -0.005412 -0.067210 -0.008098 -0.042848  0.048566   \n",
       "1     0.087185 -0.013556  0.034466 -0.041398 -0.009335  0.038518  0.072062   \n",
       "2     0.121493  0.024607 -0.038873 -0.034820 -0.033760 -0.023758  0.055283   \n",
       "3    -0.020010  0.041242 -0.016424 -0.036992 -0.039034  0.003855  0.005490   \n",
       "4     0.026711 -0.061871  0.018863 -0.026486  0.046739 -0.060128  0.027036   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "5030  0.090720  0.068379  0.007549 -0.037703  0.008094 -0.022078  0.057416   \n",
       "5031  0.143055  0.031883  0.000566 -0.107946 -0.004803 -0.010487  0.043309   \n",
       "5032  0.124135  0.004982 -0.001954 -0.043581 -0.027300 -0.040936  0.006083   \n",
       "5033  0.050855  0.037297  0.013623 -0.040512 -0.012929 -0.042989  0.068927   \n",
       "5034  0.008289  0.058433  0.002247 -0.033897 -0.067209 -0.010444  0.017773   \n",
       "\n",
       "           7         8         9    ...       302       303       304  \\\n",
       "0     0.038053 -0.051750  0.016248  ...  0.013072 -0.137592 -0.063010   \n",
       "1    -0.126953  0.072556 -0.010305  ... -0.009245  0.031439  0.044652   \n",
       "2    -0.082305 -0.030336 -0.063620  ...  0.074011  0.065398  0.025788   \n",
       "3     0.042822 -0.038598 -0.078856  ... -0.016536 -0.062215 -0.011497   \n",
       "4     0.004546 -0.053569  0.005665  ... -0.000268 -0.096092 -0.059798   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "5030  0.074114  0.012745  0.036362  ...  0.039732 -0.056965 -0.118393   \n",
       "5031  0.042991 -0.025998  0.029961  ...  0.030708  0.031169  0.063458   \n",
       "5032 -0.125258 -0.009013 -0.044418  ...  0.051877  0.048784  0.106491   \n",
       "5033  0.009016  0.003902  0.060874  ...  0.009632 -0.044231  0.059576   \n",
       "5034 -0.102263  0.044758  0.110103  ... -0.046597  0.076197  0.102915   \n",
       "\n",
       "           305       306       307       308       309       310       311  \n",
       "0    -0.102285 -0.017897  0.058684 -0.000319  0.078772 -0.005704  0.041218  \n",
       "1     0.031633 -0.081807 -0.021026 -0.011231  0.007678 -0.057428  0.005134  \n",
       "2     0.005126  0.049230 -0.076790  0.114930  0.083010  0.026723 -0.053610  \n",
       "3    -0.130887  0.024420  0.049948 -0.068521 -0.050156  0.030229  0.039134  \n",
       "4    -0.059040  0.025562  0.083468  0.007707  0.097353  0.100729 -0.048607  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "5030  0.032156  0.018497  0.021067  0.037554  0.101765  0.023785  0.004840  \n",
       "5031 -0.007555 -0.065752  0.068371 -0.064800  0.034165  0.053517 -0.010068  \n",
       "5032  0.048471 -0.019057 -0.011693  0.046124 -0.033521  0.045135 -0.047027  \n",
       "5033 -0.017738 -0.072191 -0.035146 -0.112178  0.042907 -0.006640 -0.064849  \n",
       "5034  0.000366 -0.096565 -0.038375 -0.022783 -0.060977 -0.062677 -0.003916  \n",
       "\n",
       "[5035 rows x 312 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embeddings for train dataset:\n",
    "labels = df[\"label\"]\n",
    "\n",
    "emb_list_ = [np.asarray(s) for s in emb_list]\n",
    "df_emb_=pd.DataFrame(emb_list_)\n",
    "df_emb_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7bd64815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "804\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>- интересный новый сервис, где можно оставить...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>чет как-то нерадостно все это...особо на фоне...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#Repost with . ・・・ жаль что быстро убежала!!!#...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#hellomyearth #дорогажизни #разорванноекольцо</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#ВтандемеСМамой#кактампробка#😁</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label_test\n",
       "0   - интересный новый сервис, где можно оставить...           1\n",
       "1   чет как-то нерадостно все это...особо на фоне...           0\n",
       "2  #Repost with . ・・・ жаль что быстро убежала!!!#...           0\n",
       "3      #hellomyearth #дорогажизни #разорванноекольцо           0\n",
       "4                     #ВтандемеСМамой#кактампробка#😁           0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load trial dataset\n",
    "\n",
    "file_test='all_merged_temp0_instr0_withEx_test_final_label.csv'\n",
    "\n",
    "test_data=pd.read_csv(fold+file_test, sep=\"|\", encoding ='utf-8')[['text', 'final_label']]\n",
    "test_data=test_data.rename(columns={'final_label':'label_test'})\n",
    "print (test_data.shape[0])\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4ace1a40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    532\n",
       "1    272\n",
       "Name: label_test, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.replace({'label_test': {3: 0}}, inplace=True)\n",
    "test_data.label_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c4b75170",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3b59cbe17d445148153054bed8d0821",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/804 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 16s, sys: 15.8 s, total: 4min 32s\n",
      "Wall time: 6.81 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from tqdm.notebook import tqdm\n",
    "emb_list_test = list()\n",
    "it = 0\n",
    "for s in tqdm(test_data.text.values):\n",
    "    emb_list_test.append(embed_bert_cls(s, model_emb, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b32b1d45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>302</th>\n",
       "      <th>303</th>\n",
       "      <th>304</th>\n",
       "      <th>305</th>\n",
       "      <th>306</th>\n",
       "      <th>307</th>\n",
       "      <th>308</th>\n",
       "      <th>309</th>\n",
       "      <th>310</th>\n",
       "      <th>311</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.047982</td>\n",
       "      <td>-0.007989</td>\n",
       "      <td>0.019273</td>\n",
       "      <td>-0.064445</td>\n",
       "      <td>-0.047470</td>\n",
       "      <td>0.079242</td>\n",
       "      <td>-0.094084</td>\n",
       "      <td>0.037180</td>\n",
       "      <td>0.049691</td>\n",
       "      <td>-0.039148</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050658</td>\n",
       "      <td>-0.002591</td>\n",
       "      <td>-0.030989</td>\n",
       "      <td>0.011156</td>\n",
       "      <td>-0.025440</td>\n",
       "      <td>0.075424</td>\n",
       "      <td>0.062284</td>\n",
       "      <td>0.021909</td>\n",
       "      <td>0.038568</td>\n",
       "      <td>-0.067602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.048708</td>\n",
       "      <td>0.038768</td>\n",
       "      <td>0.008993</td>\n",
       "      <td>-0.043480</td>\n",
       "      <td>-0.011525</td>\n",
       "      <td>-0.074801</td>\n",
       "      <td>0.068448</td>\n",
       "      <td>-0.029761</td>\n",
       "      <td>0.038171</td>\n",
       "      <td>-0.086254</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015159</td>\n",
       "      <td>-0.086703</td>\n",
       "      <td>0.058923</td>\n",
       "      <td>-0.156870</td>\n",
       "      <td>0.034332</td>\n",
       "      <td>0.029744</td>\n",
       "      <td>-0.051479</td>\n",
       "      <td>0.020975</td>\n",
       "      <td>-0.005021</td>\n",
       "      <td>-0.045287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.046934</td>\n",
       "      <td>-0.007776</td>\n",
       "      <td>-0.034438</td>\n",
       "      <td>-0.060966</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>-0.036679</td>\n",
       "      <td>0.076556</td>\n",
       "      <td>-0.046489</td>\n",
       "      <td>-0.026749</td>\n",
       "      <td>-0.019772</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004219</td>\n",
       "      <td>-0.030804</td>\n",
       "      <td>-0.031761</td>\n",
       "      <td>-0.056468</td>\n",
       "      <td>0.068399</td>\n",
       "      <td>0.162177</td>\n",
       "      <td>-0.056490</td>\n",
       "      <td>0.092742</td>\n",
       "      <td>0.047726</td>\n",
       "      <td>-0.021333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.044973</td>\n",
       "      <td>-0.031754</td>\n",
       "      <td>0.013514</td>\n",
       "      <td>-0.088200</td>\n",
       "      <td>0.005767</td>\n",
       "      <td>0.006182</td>\n",
       "      <td>0.086622</td>\n",
       "      <td>0.048924</td>\n",
       "      <td>-0.041500</td>\n",
       "      <td>-0.050908</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.058033</td>\n",
       "      <td>-0.037874</td>\n",
       "      <td>-0.066198</td>\n",
       "      <td>-0.072266</td>\n",
       "      <td>0.083422</td>\n",
       "      <td>0.040231</td>\n",
       "      <td>-0.079380</td>\n",
       "      <td>0.028450</td>\n",
       "      <td>0.032081</td>\n",
       "      <td>0.027410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.019812</td>\n",
       "      <td>0.003509</td>\n",
       "      <td>0.010675</td>\n",
       "      <td>-0.081504</td>\n",
       "      <td>-0.021689</td>\n",
       "      <td>-0.016962</td>\n",
       "      <td>0.031124</td>\n",
       "      <td>0.072162</td>\n",
       "      <td>-0.058702</td>\n",
       "      <td>-0.041161</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.032649</td>\n",
       "      <td>-0.088169</td>\n",
       "      <td>-0.050347</td>\n",
       "      <td>-0.059466</td>\n",
       "      <td>0.042321</td>\n",
       "      <td>0.087201</td>\n",
       "      <td>-0.081285</td>\n",
       "      <td>0.027309</td>\n",
       "      <td>0.088323</td>\n",
       "      <td>-0.029691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>0.101509</td>\n",
       "      <td>-0.086583</td>\n",
       "      <td>0.037290</td>\n",
       "      <td>-0.097883</td>\n",
       "      <td>-0.067217</td>\n",
       "      <td>-0.012579</td>\n",
       "      <td>0.081658</td>\n",
       "      <td>0.014439</td>\n",
       "      <td>-0.060402</td>\n",
       "      <td>-0.037441</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020561</td>\n",
       "      <td>-0.054927</td>\n",
       "      <td>0.003964</td>\n",
       "      <td>-0.058199</td>\n",
       "      <td>0.079025</td>\n",
       "      <td>0.088163</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.089325</td>\n",
       "      <td>0.038812</td>\n",
       "      <td>0.045627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800</th>\n",
       "      <td>0.015678</td>\n",
       "      <td>-0.010740</td>\n",
       "      <td>-0.001362</td>\n",
       "      <td>-0.025951</td>\n",
       "      <td>-0.016946</td>\n",
       "      <td>0.006844</td>\n",
       "      <td>-0.015210</td>\n",
       "      <td>0.062379</td>\n",
       "      <td>0.035687</td>\n",
       "      <td>-0.057801</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.027969</td>\n",
       "      <td>-0.004778</td>\n",
       "      <td>-0.050198</td>\n",
       "      <td>-0.040739</td>\n",
       "      <td>0.081978</td>\n",
       "      <td>0.084395</td>\n",
       "      <td>0.013094</td>\n",
       "      <td>0.002331</td>\n",
       "      <td>0.059930</td>\n",
       "      <td>-0.015127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>801</th>\n",
       "      <td>0.017429</td>\n",
       "      <td>-0.018824</td>\n",
       "      <td>0.004258</td>\n",
       "      <td>-0.043123</td>\n",
       "      <td>0.043978</td>\n",
       "      <td>-0.012925</td>\n",
       "      <td>-0.038624</td>\n",
       "      <td>0.043852</td>\n",
       "      <td>-0.117673</td>\n",
       "      <td>0.061700</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001252</td>\n",
       "      <td>-0.078440</td>\n",
       "      <td>-0.072294</td>\n",
       "      <td>-0.057007</td>\n",
       "      <td>0.068025</td>\n",
       "      <td>0.063575</td>\n",
       "      <td>-0.018018</td>\n",
       "      <td>0.005897</td>\n",
       "      <td>0.067275</td>\n",
       "      <td>-0.090942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>802</th>\n",
       "      <td>-0.004265</td>\n",
       "      <td>0.044574</td>\n",
       "      <td>0.046853</td>\n",
       "      <td>-0.093753</td>\n",
       "      <td>-0.026876</td>\n",
       "      <td>0.010413</td>\n",
       "      <td>-0.004832</td>\n",
       "      <td>0.034466</td>\n",
       "      <td>-0.071080</td>\n",
       "      <td>0.073750</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043476</td>\n",
       "      <td>-0.076574</td>\n",
       "      <td>0.039202</td>\n",
       "      <td>0.005576</td>\n",
       "      <td>-0.015504</td>\n",
       "      <td>0.040497</td>\n",
       "      <td>-0.113724</td>\n",
       "      <td>0.067692</td>\n",
       "      <td>0.015220</td>\n",
       "      <td>-0.046457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>803</th>\n",
       "      <td>0.163189</td>\n",
       "      <td>0.033339</td>\n",
       "      <td>0.033599</td>\n",
       "      <td>-0.092622</td>\n",
       "      <td>-0.071083</td>\n",
       "      <td>0.030324</td>\n",
       "      <td>-0.021714</td>\n",
       "      <td>-0.060533</td>\n",
       "      <td>0.053720</td>\n",
       "      <td>0.033975</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000949</td>\n",
       "      <td>0.032454</td>\n",
       "      <td>-0.001005</td>\n",
       "      <td>0.079011</td>\n",
       "      <td>-0.049759</td>\n",
       "      <td>0.009575</td>\n",
       "      <td>-0.021592</td>\n",
       "      <td>-0.008963</td>\n",
       "      <td>-0.052697</td>\n",
       "      <td>-0.016109</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>804 rows × 312 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6    \\\n",
       "0   -0.047982 -0.007989  0.019273 -0.064445 -0.047470  0.079242 -0.094084   \n",
       "1    0.048708  0.038768  0.008993 -0.043480 -0.011525 -0.074801  0.068448   \n",
       "2    0.046934 -0.007776 -0.034438 -0.060966  0.001300 -0.036679  0.076556   \n",
       "3   -0.044973 -0.031754  0.013514 -0.088200  0.005767  0.006182  0.086622   \n",
       "4    0.019812  0.003509  0.010675 -0.081504 -0.021689 -0.016962  0.031124   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "799  0.101509 -0.086583  0.037290 -0.097883 -0.067217 -0.012579  0.081658   \n",
       "800  0.015678 -0.010740 -0.001362 -0.025951 -0.016946  0.006844 -0.015210   \n",
       "801  0.017429 -0.018824  0.004258 -0.043123  0.043978 -0.012925 -0.038624   \n",
       "802 -0.004265  0.044574  0.046853 -0.093753 -0.026876  0.010413 -0.004832   \n",
       "803  0.163189  0.033339  0.033599 -0.092622 -0.071083  0.030324 -0.021714   \n",
       "\n",
       "          7         8         9    ...       302       303       304  \\\n",
       "0    0.037180  0.049691 -0.039148  ...  0.050658 -0.002591 -0.030989   \n",
       "1   -0.029761  0.038171 -0.086254  ...  0.015159 -0.086703  0.058923   \n",
       "2   -0.046489 -0.026749 -0.019772  ...  0.004219 -0.030804 -0.031761   \n",
       "3    0.048924 -0.041500 -0.050908  ... -0.058033 -0.037874 -0.066198   \n",
       "4    0.072162 -0.058702 -0.041161  ... -0.032649 -0.088169 -0.050347   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "799  0.014439 -0.060402 -0.037441  ... -0.020561 -0.054927  0.003964   \n",
       "800  0.062379  0.035687 -0.057801  ... -0.027969 -0.004778 -0.050198   \n",
       "801  0.043852 -0.117673  0.061700  ...  0.001252 -0.078440 -0.072294   \n",
       "802  0.034466 -0.071080  0.073750  ...  0.043476 -0.076574  0.039202   \n",
       "803 -0.060533  0.053720  0.033975  ...  0.000949  0.032454 -0.001005   \n",
       "\n",
       "          305       306       307       308       309       310       311  \n",
       "0    0.011156 -0.025440  0.075424  0.062284  0.021909  0.038568 -0.067602  \n",
       "1   -0.156870  0.034332  0.029744 -0.051479  0.020975 -0.005021 -0.045287  \n",
       "2   -0.056468  0.068399  0.162177 -0.056490  0.092742  0.047726 -0.021333  \n",
       "3   -0.072266  0.083422  0.040231 -0.079380  0.028450  0.032081  0.027410  \n",
       "4   -0.059466  0.042321  0.087201 -0.081285  0.027309  0.088323 -0.029691  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "799 -0.058199  0.079025  0.088163  0.000104  0.089325  0.038812  0.045627  \n",
       "800 -0.040739  0.081978  0.084395  0.013094  0.002331  0.059930 -0.015127  \n",
       "801 -0.057007  0.068025  0.063575 -0.018018  0.005897  0.067275 -0.090942  \n",
       "802  0.005576 -0.015504  0.040497 -0.113724  0.067692  0.015220 -0.046457  \n",
       "803  0.079011 -0.049759  0.009575 -0.021592 -0.008963 -0.052697 -0.016109  \n",
       "\n",
       "[804 rows x 312 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embeddings for trial dataset:\n",
    "    \n",
    "labels_test=test_data['label_test']\n",
    "emb_list_test_ = [np.asarray(s) for s in emb_list_test]\n",
    "df_emb_test_=pd.DataFrame(emb_list_test_)\n",
    "df_emb_test_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350f3b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings were pre-saved:\n",
    "# fold='../data/data_for_binary_classification/'\n",
    "# df_emb_.to_csv(fold+'embeddings312_RuTiny2_train.csv', sep='|', encoding='utf-8', index=False)\n",
    "# df_emb_test_.to_csv(fold+'embeddings312_RuTiny2_test.csv', sep='|', encoding='utf-8', index=False)\n",
    "\n",
    "# so we can just load them:\n",
    "# df_emb_loaded=pd.read_csv(fold+'embeddings312_RuTiny2_train.csv', sep='|', encoding='utf-8')\n",
    "# df_emb_loaded_test=pd.read_csv(fold+'embeddings312_RuTiny2_test.csv', sep='|', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc39dc6",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b49d4ea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.76264768, 1.45184544])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "import numpy as np\n",
    "class_weight = compute_class_weight(\n",
    "    class_weight='balanced', classes=np.unique(labels), y=labels)\n",
    "class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1a89c0b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;background-color: white;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=SVC(class_weight={0: 0.7626476825204483,\n",
       "                                         1: 1.4518454440599768},\n",
       "                           probability=True),\n",
       "             param_grid={&#x27;C&#x27;: [1], &#x27;gamma&#x27;: [1], &#x27;kernel&#x27;: [&#x27;poly&#x27;]},\n",
       "             scoring=&#x27;f1_macro&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-19\" type=\"checkbox\" ><label for=\"sk-estimator-id-19\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=SVC(class_weight={0: 0.7626476825204483,\n",
       "                                         1: 1.4518454440599768},\n",
       "                           probability=True),\n",
       "             param_grid={&#x27;C&#x27;: [1], &#x27;gamma&#x27;: [1], &#x27;kernel&#x27;: [&#x27;poly&#x27;]},\n",
       "             scoring=&#x27;f1_macro&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-20\" type=\"checkbox\" ><label for=\"sk-estimator-id-20\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(class_weight={0: 0.7626476825204483, 1: 1.4518454440599768},\n",
       "    probability=True)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-21\" type=\"checkbox\" ><label for=\"sk-estimator-id-21\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(class_weight={0: 0.7626476825204483, 1: 1.4518454440599768},\n",
       "    probability=True)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=SVC(class_weight={0: 0.7626476825204483,\n",
       "                                         1: 1.4518454440599768},\n",
       "                           probability=True),\n",
       "             param_grid={'C': [1], 'gamma': [1], 'kernel': ['poly']},\n",
       "             scoring='f1_macro')"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameteres = {'C': [0.1,1, 10, 100], 'gamma': [1,0.1,0.01,0.001],'kernel': ['rbf', 'poly', 'sigmoid']}\n",
    "clf = GridSearchCV(SVC(class_weight={0:class_weight[0], 1:class_weight[1]}, probability=True), param_grid=parameteres , cv=5, scoring='f1_macro')\n",
    "clf.fit(df_emb_, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "40864039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters from gridsearch: {'C': 1, 'gamma': 1, 'kernel': 'poly'}\n",
      "CV score=0.759\n",
      "{'mean_fit_time': array([7.82658319]), 'std_fit_time': array([0.40208642]), 'mean_score_time': array([0.28000245]), 'std_score_time': array([0.01259353]), 'param_C': masked_array(data=[1],\n",
      "             mask=[False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_gamma': masked_array(data=[1],\n",
      "             mask=[False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_kernel': masked_array(data=['poly'],\n",
      "             mask=[False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'C': 1, 'gamma': 1, 'kernel': 'poly'}], 'split0_test_score': array([0.82165679]), 'split1_test_score': array([0.65893225]), 'split2_test_score': array([0.73444093]), 'split3_test_score': array([0.84713307]), 'split4_test_score': array([0.73153275]), 'mean_test_score': array([0.75873916]), 'std_test_score': array([0.06791778]), 'rank_test_score': array([1], dtype=int32)}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best parameters from gridsearch: {}\".format(clf.best_params_))\n",
    "print(\"CV score=%0.3f\" % clf.best_score_)\n",
    "cv_results = clf.cv_results_\n",
    "print(cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b1c42c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict=clf.predict_proba(df_emb_test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c81e164a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "def TestCutoff(df):\n",
    "\n",
    "    cut_off_list_=np.arange(0.005, 0.901, 0.005)\n",
    "#     cut_off_list = itertools.chain([0.01], cut_off_list_)\n",
    "    \n",
    "    f1score_macro_list=[]\n",
    "    f1score_list=[]\n",
    "    recall_list=[]\n",
    "    \n",
    "    predict_list_list=[[]]\n",
    "    for i, cut_off in enumerate(cut_off_list_):\n",
    "        predict_list=np.where(df['predict_1']>cut_off, 1, 0)\n",
    "        predict_list_list.append(predict_list)\n",
    "        print (cut_off)\n",
    "        precision, recall, f1score = precision_recall_fscore_support(df['label_test'], predict_list)[:3]\n",
    "        print(f'precision: {precision}, recall: {recall}, f1score: {f1score}')\n",
    "        f1score_list.append(f1score[1])\n",
    "        recall_list.append(recall[1])\n",
    "        print (\"macro:\")\n",
    "        precision, recall, f1score_macro = precision_recall_fscore_support(df['label_test'], predict_list, average='macro')[:3]\n",
    "        print(f'precision: {precision}, recall: {recall}, f1score_macro: {f1score_macro}')\n",
    "        f1score_macro_list.append(f1score_macro)\n",
    "        print (\" \")\n",
    "    \n",
    "    max_ind=f1score_macro_list.index(max(f1score_macro_list))   \n",
    "    print (\"macro\", f1score_macro_list[max_ind])\n",
    "    print (\"F1-valued:\", f1score_list[max_ind])\n",
    "    print (\"recall-valued:\", recall_list[max_ind])\n",
    "    print (cut_off_list_[max_ind])\n",
    "    \n",
    "    df['predict']=predict_list_list[max_ind+1]\n",
    "        \n",
    "    return (df)\n",
    "#         \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de725b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions0=[]\n",
    "predictions1=[]\n",
    "\n",
    "for res in y_predict:\n",
    "    predictions0.append(res[0])\n",
    "    predictions1.append(res[1])\n",
    "\n",
    "test_data['predict_0']=predictions0\n",
    "test_data['predict_1']=predictions1\n",
    "\n",
    "test_data_predict=TestCutoff(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "9042cecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: [0.875      0.78461538], recall: [0.89473684 0.75      ], f1score: [0.88475836 0.76691729]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "precision, recall, f1score = precision_recall_fscore_support(test_data.label_test, test_data['predict'])[:3]\n",
    "\n",
    "print(f'precision: {precision}, recall: {recall}, f1score: {f1score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "383dd5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.8298076923076922, recall: 0.8223684210526316, f1score_macro: 0.8258378287726752\n"
     ]
    }
   ],
   "source": [
    "precision, recall, f1score = precision_recall_fscore_support(test_data.label_test, test_data['predict'], average='macro')[:3]\n",
    "\n",
    "print(f'precision: {precision}, recall: {recall}, f1score_macro: {f1score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fc3628",
   "metadata": {},
   "source": [
    "# Prediction for the main dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "54b7b2c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Анамалия близ Чернобыля</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>и кто осмелится прекратить этот разбой? откров...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Никогда не выравайся от парня, если не уверенн...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ураааа)))Я опять у Риты.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Может, она и приврала. Может, она и слабохарак...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73276</th>\n",
       "      <td>Наши рубашки представлены на благотворительном...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73277</th>\n",
       "      <td>Айка итии ба5айы дьи!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73278</th>\n",
       "      <td>и все великие когда-то делали первые шаги на п...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73279</th>\n",
       "      <td>#Repost with . ・・・ USSR ballet The Queens proj...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73280</th>\n",
       "      <td>По поступкам видно,как тебя ценят.По звонкам —...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>73281 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text\n",
       "0                                Анамалия близ Чернобыля\n",
       "1      и кто осмелится прекратить этот разбой? откров...\n",
       "2      Никогда не выравайся от парня, если не уверенн...\n",
       "3                               Ураааа)))Я опять у Риты.\n",
       "4      Может, она и приврала. Может, она и слабохарак...\n",
       "...                                                  ...\n",
       "73276  Наши рубашки представлены на благотворительном...\n",
       "73277                              Айка итии ба5айы дьи!\n",
       "73278  и все великие когда-то делали первые шаги на п...\n",
       "73279  #Repost with . ・・・ USSR ballet The Queens proj...\n",
       "73280  По поступкам видно,как тебя ценят.По звонкам —...\n",
       "\n",
       "[73281 rows x 1 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the best model\n",
    "# Load main dataset:\n",
    "f='../data/'\n",
    "file='data_for_labeling_cleaned.csv'\n",
    "data_full=pd.read_csv(f+file, sep=\"|\", encoding ='utf-8')[['text']]\n",
    "data_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "dc85ffd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb64329674ae4057a612e82e7165e3df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73285 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6h 31min 24s, sys: 23min 34s, total: 6h 54min 58s\n",
      "Wall time: 10min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from tqdm.notebook import tqdm\n",
    "emb_list_full = list()\n",
    "it = 0\n",
    "for s in tqdm(data_full['text']):\n",
    "    emb_list_full.append(embed_bert_cls(s, model_emb, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "59fb326a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_full=clf.predict_proba(emb_list_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "3c82a0f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>predict_0</th>\n",
       "      <th>predict_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Анамалия близ Чернобыля</td>\n",
       "      <td>0.780301</td>\n",
       "      <td>0.219699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>и кто осмелится прекратить этот разбой? откров...</td>\n",
       "      <td>0.789129</td>\n",
       "      <td>0.210871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Никогда не выравайся от парня, если не уверенн...</td>\n",
       "      <td>0.400666</td>\n",
       "      <td>0.599334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ураааа)))Я опять у Риты.</td>\n",
       "      <td>0.943565</td>\n",
       "      <td>0.056435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Может, она и приврала. Может, она и слабохарак...</td>\n",
       "      <td>0.180333</td>\n",
       "      <td>0.819667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73280</th>\n",
       "      <td>Наши рубашки представлены на благотворительном...</td>\n",
       "      <td>0.801630</td>\n",
       "      <td>0.198370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73281</th>\n",
       "      <td>Айка итии ба5айы дьи!</td>\n",
       "      <td>0.936358</td>\n",
       "      <td>0.063642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73282</th>\n",
       "      <td>и все великие когда-то делали первые шаги на п...</td>\n",
       "      <td>0.343873</td>\n",
       "      <td>0.656127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73283</th>\n",
       "      <td>#Repost with . ・・・ USSR ballet The Queens proj...</td>\n",
       "      <td>0.940227</td>\n",
       "      <td>0.059773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73284</th>\n",
       "      <td>По поступкам видно,как тебя ценят.По звонкам —...</td>\n",
       "      <td>0.104769</td>\n",
       "      <td>0.895231</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>73285 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  predict_0  predict_1\n",
       "0                                Анамалия близ Чернобыля   0.780301   0.219699\n",
       "1      и кто осмелится прекратить этот разбой? откров...   0.789129   0.210871\n",
       "2      Никогда не выравайся от парня, если не уверенн...   0.400666   0.599334\n",
       "3                               Ураааа)))Я опять у Риты.   0.943565   0.056435\n",
       "4      Может, она и приврала. Может, она и слабохарак...   0.180333   0.819667\n",
       "...                                                  ...        ...        ...\n",
       "73280  Наши рубашки представлены на благотворительном...   0.801630   0.198370\n",
       "73281                              Айка итии ба5айы дьи!   0.936358   0.063642\n",
       "73282  и все великие когда-то делали первые шаги на п...   0.343873   0.656127\n",
       "73283  #Repost with . ・・・ USSR ballet The Queens proj...   0.940227   0.059773\n",
       "73284  По поступкам видно,как тебя ценят.По звонкам —...   0.104769   0.895231\n",
       "\n",
       "[73285 rows x 3 columns]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions0=[]\n",
    "predictions1=[]\n",
    "\n",
    "\n",
    "for res in y_predict_full:\n",
    "    predictions0.append(res[0])\n",
    "    predictions1.append(res[1])\n",
    "\n",
    "    \n",
    "data_full['predict_0']=predictions0\n",
    "data_full['predict_1']=predictions1\n",
    "data_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "48bfe9dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, ..., 1, 0, 1])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cut_off=0.420\n",
    "predict_list=[]\n",
    "predict_list=np.where(data_full['predict_1']>cut_off, 1, 0)\n",
    "predict_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "d8b4d59e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    49985\n",
       "1    23300\n",
       "Name: predict, dtype: int64"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_full['predict']=predict_list\n",
    "data_full.predict.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "5a278100",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.682063\n",
       "1    0.317937\n",
       "Name: predict, dtype: float64"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_full.predict.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "8d6da9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_full.to_csv(f+'predicted_73285_2classes', sep='|', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4c4983",
   "metadata": {},
   "source": [
    "# Logit Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "caa1f261",
   "metadata": {},
   "outputs": [],
   "source": [
    "from logitboost import LogitBoost\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "598a7d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [('LogitBoost', LogitBoost())]\n",
    "from sklearn.pipeline import Pipeline\n",
    "pipeline = Pipeline(steps) # define the pipeline object.\n",
    "\n",
    "parameteres = {'LogitBoost__n_estimators':range(10,100,10)}\n",
    "grid = GridSearchCV(pipeline, param_grid=parameteres, cv=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e72a4f34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5, estimator=Pipeline(steps=[(&#x27;LogitBoost&#x27;, LogitBoost())]),\n",
       "             param_grid={&#x27;LogitBoost__n_estimators&#x27;: range(10, 100, 10)})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5, estimator=Pipeline(steps=[(&#x27;LogitBoost&#x27;, LogitBoost())]),\n",
       "             param_grid={&#x27;LogitBoost__n_estimators&#x27;: range(10, 100, 10)})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;LogitBoost&#x27;, LogitBoost())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogitBoost</label><div class=\"sk-toggleable__content\"><pre>LogitBoost()</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5, estimator=Pipeline(steps=[('LogitBoost', LogitBoost())]),\n",
       "             param_grid={'LogitBoost__n_estimators': range(10, 100, 10)})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(df_emb_, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3b589f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters from gridsearch: {'LogitBoost__n_estimators': 30}\n",
      "CV score=0.775\n",
      "{'mean_fit_time': array([1.09624925, 2.16360064, 3.18853326, 4.23988729, 5.18478107,\n",
      "       6.06775427, 7.22547455, 8.31915588, 9.06779461]), 'std_fit_time': array([0.03912988, 0.04561217, 0.01365214, 0.03828939, 0.08121675,\n",
      "       0.06951414, 0.09377665, 0.033249  , 0.11800694]), 'mean_score_time': array([0.02238917, 0.04348617, 0.07130294, 0.0893476 , 0.10918365,\n",
      "       0.12896137, 0.15132327, 0.17679143, 0.19598117]), 'std_score_time': array([0.0002611 , 0.0002335 , 0.00690478, 0.00595087, 0.00303086,\n",
      "       0.00455219, 0.00535931, 0.0053402 , 0.00933154]), 'param_LogitBoost__n_estimators': masked_array(data=[10, 20, 30, 40, 50, 60, 70, 80, 90],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'LogitBoost__n_estimators': 10}, {'LogitBoost__n_estimators': 20}, {'LogitBoost__n_estimators': 30}, {'LogitBoost__n_estimators': 40}, {'LogitBoost__n_estimators': 50}, {'LogitBoost__n_estimators': 60}, {'LogitBoost__n_estimators': 70}, {'LogitBoost__n_estimators': 80}, {'LogitBoost__n_estimators': 90}], 'split0_test_score': array([0.83416087, 0.83316783, 0.83018868, 0.83714002, 0.83912612,\n",
      "       0.84210526, 0.83416087, 0.83316783, 0.83316783]), 'split1_test_score': array([0.67527309, 0.68023833, 0.68718967, 0.68123138, 0.68619662,\n",
      "       0.68321748, 0.68222443, 0.67130089, 0.67030785]), 'split2_test_score': array([0.775571  , 0.76961271, 0.77259186, 0.775571  , 0.77159881,\n",
      "       0.76464747, 0.76762661, 0.76266137, 0.76762661]), 'split3_test_score': array([0.84011917, 0.84409136, 0.84409136, 0.83515392, 0.84011917,\n",
      "       0.83813307, 0.83912612, 0.83614697, 0.83813307]), 'split4_test_score': array([0.73584906, 0.73684211, 0.74180735, 0.73386296, 0.72691162,\n",
      "       0.71797418, 0.72691162, 0.72393247, 0.71896723]), 'mean_test_score': array([0.77219464, 0.77279047, 0.77517378, 0.77259186, 0.77279047,\n",
      "       0.76921549, 0.77000993, 0.76544191, 0.76564052]), 'std_test_score': array([0.06193504, 0.06098774, 0.05768442, 0.05989327, 0.06089065,\n",
      "       0.06341096, 0.06076484, 0.06352842, 0.06493961]), 'rank_test_score': array([5, 2, 1, 4, 2, 7, 6, 9, 8], dtype=int32)}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best parameters from gridsearch: {}\".format(grid.best_params_))\n",
    "print(\"CV score=%0.3f\" % grid.best_score_)\n",
    "cv_results = grid.cv_results_\n",
    "print(cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6bdf883a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = grid.predict_proba(df_emb_test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3c2fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions0=[]\n",
    "predictions1=[]\n",
    "\n",
    "\n",
    "for res in y_pred:\n",
    "    predictions0.append(res[0])\n",
    "    predictions1.append(res[1])\n",
    "\n",
    "    \n",
    "test_data['predict_0']=predictions0\n",
    "test_data['predict_1']=predictions1\n",
    "\n",
    "test_data_predict=TestCutoff(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e8e695f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: [0.87406015 0.75367647], recall: [0.87406015 0.75367647], f1score: [0.87406015 0.75367647]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "precision, recall, f1score = precision_recall_fscore_support(test_data.label_test, test_data['predict'])[:3]\n",
    "\n",
    "print(f'precision: {precision}, recall: {recall}, f1score: {f1score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d915b40f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.8138683104820876, recall: 0.8138683104820876, f1score_macro: 0.8138683104820876\n"
     ]
    }
   ],
   "source": [
    "precision, recall, f1score = precision_recall_fscore_support(test_data.label_test, test_data['predict'], average='macro')[:3]\n",
    "\n",
    "print(f'precision: {precision}, recall: {recall}, f1score_macro: {f1score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5015a2ad",
   "metadata": {},
   "source": [
    "# Logit Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1c2afd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "eb942417",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.76264768, 1.45184544])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "class_weight = compute_class_weight(\n",
    "    class_weight='balanced', classes=np.unique(labels), y=labels)\n",
    "class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d7f93cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
      "250 fits failed out of a total of 500.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "250 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1091, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 61, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/model_selection/_search.py:953: UserWarning: One or more of the test scores are non-finite: [       nan 0.751572          nan 0.74960591        nan 0.74880614\n",
      "        nan 0.74938758        nan 0.7480217         nan 0.74695041\n",
      "        nan 0.74774372        nan 0.74611358        nan 0.74611445\n",
      "        nan 0.7471066         nan 0.74715191        nan 0.74757798\n",
      "        nan 0.74773862        nan 0.74755181        nan 0.74737264\n",
      "        nan 0.7478404         nan 0.74761226        nan 0.74669777\n",
      "        nan 0.74762           nan 0.74767808        nan 0.74827909\n",
      "        nan 0.748507          nan 0.74772786        nan 0.74717699\n",
      "        nan 0.74643959        nan 0.74643221        nan 0.74623998\n",
      "        nan 0.7455352         nan 0.74566042        nan 0.7454863\n",
      "        nan 0.74476948        nan 0.7446723         nan 0.74424205\n",
      "        nan 0.74395924        nan 0.74394867        nan 0.7437633\n",
      "        nan 0.74373719        nan 0.74454424        nan 0.74408261\n",
      "        nan 0.74420049        nan 0.74448416        nan 0.74436506\n",
      "        nan 0.7442759         nan 0.74390624        nan 0.74418023\n",
      "        nan 0.74479589        nan 0.74422693        nan 0.74442552\n",
      "        nan 0.74423036        nan 0.74466574]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=LogisticRegression(class_weight={0: 0.7626476825204483,\n",
       "                                                        1: 1.4518454440599768}),\n",
       "             param_grid={&#x27;C&#x27;: array([1.00000000e-04, 2.04179592e-01, 4.08259184e-01, 6.12338776e-01,\n",
       "       8.16418367e-01, 1.02049796e+00, 1.22457755e+00, 1.42865714e+00,\n",
       "       1.63273673e+00, 1.83681633e+00, 2.04089592e+00, 2.24497551e+00,\n",
       "       2.44905510e+00, 2.65313469e+00, 2.85721429e+...\n",
       "       5.71432857e+00, 5.91840816e+00, 6.12248776e+00, 6.32656735e+00,\n",
       "       6.53064694e+00, 6.73472653e+00, 6.93880612e+00, 7.14288571e+00,\n",
       "       7.34696531e+00, 7.55104490e+00, 7.75512449e+00, 7.95920408e+00,\n",
       "       8.16328367e+00, 8.36736327e+00, 8.57144286e+00, 8.77552245e+00,\n",
       "       8.97960204e+00, 9.18368163e+00, 9.38776122e+00, 9.59184082e+00,\n",
       "       9.79592041e+00, 1.00000000e+01]),\n",
       "                         &#x27;penalty&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;]},\n",
       "             scoring=&#x27;f1_macro&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=LogisticRegression(class_weight={0: 0.7626476825204483,\n",
       "                                                        1: 1.4518454440599768}),\n",
       "             param_grid={&#x27;C&#x27;: array([1.00000000e-04, 2.04179592e-01, 4.08259184e-01, 6.12338776e-01,\n",
       "       8.16418367e-01, 1.02049796e+00, 1.22457755e+00, 1.42865714e+00,\n",
       "       1.63273673e+00, 1.83681633e+00, 2.04089592e+00, 2.24497551e+00,\n",
       "       2.44905510e+00, 2.65313469e+00, 2.85721429e+...\n",
       "       5.71432857e+00, 5.91840816e+00, 6.12248776e+00, 6.32656735e+00,\n",
       "       6.53064694e+00, 6.73472653e+00, 6.93880612e+00, 7.14288571e+00,\n",
       "       7.34696531e+00, 7.55104490e+00, 7.75512449e+00, 7.95920408e+00,\n",
       "       8.16328367e+00, 8.36736327e+00, 8.57144286e+00, 8.77552245e+00,\n",
       "       8.97960204e+00, 9.18368163e+00, 9.38776122e+00, 9.59184082e+00,\n",
       "       9.79592041e+00, 1.00000000e+01]),\n",
       "                         &#x27;penalty&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;]},\n",
       "             scoring=&#x27;f1_macro&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(class_weight={0: 0.7626476825204483, 1: 1.4518454440599768})</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(class_weight={0: 0.7626476825204483, 1: 1.4518454440599768})</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=LogisticRegression(class_weight={0: 0.7626476825204483,\n",
       "                                                        1: 1.4518454440599768}),\n",
       "             param_grid={'C': array([1.00000000e-04, 2.04179592e-01, 4.08259184e-01, 6.12338776e-01,\n",
       "       8.16418367e-01, 1.02049796e+00, 1.22457755e+00, 1.42865714e+00,\n",
       "       1.63273673e+00, 1.83681633e+00, 2.04089592e+00, 2.24497551e+00,\n",
       "       2.44905510e+00, 2.65313469e+00, 2.85721429e+...\n",
       "       5.71432857e+00, 5.91840816e+00, 6.12248776e+00, 6.32656735e+00,\n",
       "       6.53064694e+00, 6.73472653e+00, 6.93880612e+00, 7.14288571e+00,\n",
       "       7.34696531e+00, 7.55104490e+00, 7.75512449e+00, 7.95920408e+00,\n",
       "       8.16328367e+00, 8.36736327e+00, 8.57144286e+00, 8.77552245e+00,\n",
       "       8.97960204e+00, 9.18368163e+00, 9.38776122e+00, 9.59184082e+00,\n",
       "       9.79592041e+00, 1.00000000e+01]),\n",
       "                         'penalty': ['l1', 'l2']},\n",
       "             scoring='f1_macro')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {'C': np.linspace(0.0001, 10, 50), \"penalty\":[\"l1\",\"l2\"]}  #high C means \"Trust this training data a lot\", while a low value says \"This data may not be fully representative of the real world data, so if it's telling you to make a parameter really large, don't listen to it\"\n",
    "grid_search = GridSearchCV(LogisticRegression(class_weight={0:class_weight[0], 1:class_weight[1]}), parameters, cv=5, scoring='f1_macro')\n",
    "grid_search.fit(df_emb_, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b24d9846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters from gridsearch: {'C': 0.0001, 'penalty': 'l2'}\n",
      "CV score=0.752\n",
      "{'mean_fit_time': array([0.0037147 , 0.0241519 , 0.00322347, 0.03687806, 0.00352597,\n",
      "       0.04486532, 0.0034966 , 0.05093918, 0.00346422, 0.05576649,\n",
      "       0.00345654, 0.05837436, 0.00339322, 0.05948205, 0.00349436,\n",
      "       0.06587572, 0.00339398, 0.0695591 , 0.0031878 , 0.07109642,\n",
      "       0.00351248, 0.07362995, 0.00322194, 0.07169089, 0.00315657,\n",
      "       0.07404079, 0.00317473, 0.07627969, 0.00327497, 0.07827225,\n",
      "       0.00326552, 0.08168635, 0.00330272, 0.08327537, 0.00328941,\n",
      "       0.08828707, 0.00332866, 0.0960856 , 0.0032742 , 0.09341364,\n",
      "       0.00320144, 0.10248451, 0.00319657, 0.09284182, 0.00321774,\n",
      "       0.09870801, 0.00321226, 0.09357629, 0.00322189, 0.09482098,\n",
      "       0.00321736, 0.12806931, 0.0033103 , 0.10588021, 0.00331531,\n",
      "       0.10088758, 0.00330186, 0.10186496, 0.00327821, 0.09370041,\n",
      "       0.00323653, 0.09430008, 0.00324974, 0.09446816, 0.00323725,\n",
      "       0.09610448, 0.00325623, 0.09331341, 0.00314245, 0.09266491,\n",
      "       0.00325871, 0.09546614, 0.00337486, 0.10533614, 0.00358176,\n",
      "       0.09609504, 0.00325055, 0.09120932, 0.00193806, 0.07752662,\n",
      "       0.00195208, 0.07797771, 0.00200725, 0.07835145, 0.00194588,\n",
      "       0.07873387, 0.00197215, 0.07973933, 0.0019279 , 0.07750254,\n",
      "       0.0019711 , 0.07776084, 0.00194149, 0.07809105, 0.00194974,\n",
      "       0.07684517, 0.00199494, 0.09652243, 0.00204182, 0.08354626]), 'std_fit_time': array([1.68291352e-03, 3.21410812e-03, 9.32698113e-05, 3.25440351e-03,\n",
      "       1.24265996e-04, 3.30321205e-03, 8.68073370e-05, 3.57608168e-03,\n",
      "       6.67549882e-05, 3.72022002e-03, 1.60580986e-04, 3.44332197e-03,\n",
      "       1.06690479e-04, 6.20569151e-03, 4.23324524e-05, 6.95673677e-03,\n",
      "       6.44839356e-05, 6.31278951e-03, 1.16656785e-04, 7.45744855e-03,\n",
      "       5.78311845e-04, 7.22814025e-03, 1.06746513e-04, 7.37216108e-03,\n",
      "       7.16611214e-05, 2.10066418e-03, 1.13014974e-04, 8.30727531e-03,\n",
      "       6.99806902e-05, 7.39386685e-03, 5.19278949e-05, 5.51681562e-03,\n",
      "       8.71726405e-05, 4.82360777e-03, 1.05322675e-04, 7.25152883e-03,\n",
      "       1.25661619e-04, 5.52503610e-03, 9.03413269e-05, 5.86059771e-03,\n",
      "       1.35129492e-04, 1.91256165e-02, 1.29323793e-04, 9.37702470e-03,\n",
      "       1.55331371e-04, 6.24960394e-03, 1.58347447e-04, 2.18929949e-03,\n",
      "       1.43970128e-04, 7.11028309e-03, 1.85176357e-04, 2.21861139e-02,\n",
      "       1.42896616e-04, 1.89638681e-02, 1.46432382e-04, 3.28950394e-03,\n",
      "       1.27063280e-04, 1.09639900e-02, 7.49618345e-05, 2.27968367e-03,\n",
      "       6.60039779e-05, 2.21535571e-03, 8.32160219e-05, 2.41509336e-03,\n",
      "       7.43124360e-05, 3.22903830e-03, 9.46437798e-05, 2.38740739e-03,\n",
      "       8.44080700e-05, 2.38832781e-03, 6.67771242e-05, 1.86111341e-03,\n",
      "       1.06490389e-04, 1.99692934e-02, 5.81473539e-05, 9.49603917e-03,\n",
      "       1.29578811e-04, 7.14137491e-03, 1.12279245e-04, 2.96896096e-03,\n",
      "       1.10137059e-04, 3.46670187e-03, 1.17115419e-04, 3.88750924e-03,\n",
      "       1.13137451e-04, 5.58474335e-03, 1.34500151e-04, 4.81976607e-03,\n",
      "       1.01837883e-04, 4.66091407e-03, 1.26362801e-04, 2.71886946e-03,\n",
      "       1.13394784e-04, 3.40039882e-03, 1.28625920e-04, 3.24187756e-03,\n",
      "       1.52723773e-04, 4.75959544e-03, 1.08069753e-04, 5.09764191e-03]), 'mean_score_time': array([0.        , 0.00806599, 0.        , 0.00729036, 0.        ,\n",
      "       0.00727582, 0.        , 0.00725026, 0.        , 0.00733929,\n",
      "       0.        , 0.00735917, 0.        , 0.00732007, 0.        ,\n",
      "       0.00724058, 0.        , 0.00724096, 0.        , 0.00728793,\n",
      "       0.        , 0.00709305, 0.        , 0.00696855, 0.        ,\n",
      "       0.00705414, 0.        , 0.00702057, 0.        , 0.00716801,\n",
      "       0.        , 0.00714831, 0.        , 0.0070703 , 0.        ,\n",
      "       0.00714283, 0.        , 0.00747085, 0.        , 0.00728984,\n",
      "       0.        , 0.00711684, 0.        , 0.00733891, 0.        ,\n",
      "       0.00756755, 0.        , 0.00737491, 0.        , 0.0073411 ,\n",
      "       0.        , 0.00776992, 0.        , 0.00699697, 0.        ,\n",
      "       0.00730338, 0.        , 0.00737267, 0.        , 0.0070415 ,\n",
      "       0.        , 0.00690799, 0.        , 0.00688553, 0.        ,\n",
      "       0.00714655, 0.        , 0.00679121, 0.        , 0.00695233,\n",
      "       0.        , 0.00760198, 0.        , 0.00777712, 0.        ,\n",
      "       0.00726666, 0.        , 0.00612483, 0.        , 0.00503778,\n",
      "       0.        , 0.00478101, 0.        , 0.00469418, 0.        ,\n",
      "       0.00470781, 0.        , 0.00468922, 0.        , 0.00468388,\n",
      "       0.        , 0.00468502, 0.        , 0.00518389, 0.        ,\n",
      "       0.00487127, 0.        , 0.00653024, 0.        , 0.00528522]), 'std_score_time': array([0.00000000e+00, 9.86114708e-04, 0.00000000e+00, 1.74819389e-04,\n",
      "       0.00000000e+00, 1.77210549e-04, 0.00000000e+00, 6.98584176e-05,\n",
      "       0.00000000e+00, 6.00677429e-05, 0.00000000e+00, 7.50974775e-05,\n",
      "       0.00000000e+00, 5.63916341e-05, 0.00000000e+00, 5.87554327e-05,\n",
      "       0.00000000e+00, 2.53019672e-04, 0.00000000e+00, 5.02432577e-04,\n",
      "       0.00000000e+00, 3.42859565e-04, 0.00000000e+00, 1.69853546e-04,\n",
      "       0.00000000e+00, 3.10243953e-04, 0.00000000e+00, 3.86209917e-04,\n",
      "       0.00000000e+00, 6.85539454e-04, 0.00000000e+00, 3.55893616e-04,\n",
      "       0.00000000e+00, 1.98138721e-04, 0.00000000e+00, 2.00889656e-04,\n",
      "       0.00000000e+00, 3.31438083e-04, 0.00000000e+00, 2.66327166e-04,\n",
      "       0.00000000e+00, 1.88034456e-04, 0.00000000e+00, 1.59322034e-04,\n",
      "       0.00000000e+00, 5.91154636e-04, 0.00000000e+00, 2.15021287e-04,\n",
      "       0.00000000e+00, 2.79931249e-04, 0.00000000e+00, 7.78914492e-04,\n",
      "       0.00000000e+00, 1.93957674e-04, 0.00000000e+00, 2.88371489e-04,\n",
      "       0.00000000e+00, 5.31183398e-04, 0.00000000e+00, 3.68649292e-04,\n",
      "       0.00000000e+00, 1.91143754e-04, 0.00000000e+00, 2.19306580e-04,\n",
      "       0.00000000e+00, 4.99590540e-04, 0.00000000e+00, 6.49244216e-05,\n",
      "       0.00000000e+00, 2.59841663e-04, 0.00000000e+00, 6.66330733e-04,\n",
      "       0.00000000e+00, 7.21780700e-04, 0.00000000e+00, 1.18057287e-03,\n",
      "       0.00000000e+00, 1.26632371e-03, 0.00000000e+00, 7.60629211e-04,\n",
      "       0.00000000e+00, 3.26269636e-04, 0.00000000e+00, 1.86285113e-04,\n",
      "       0.00000000e+00, 1.45830727e-04, 0.00000000e+00, 2.06588509e-04,\n",
      "       0.00000000e+00, 1.38904931e-04, 0.00000000e+00, 1.72022440e-04,\n",
      "       0.00000000e+00, 9.72160022e-04, 0.00000000e+00, 3.94335352e-04,\n",
      "       0.00000000e+00, 1.44165199e-03, 0.00000000e+00, 3.93059950e-04]), 'param_C': masked_array(data=[0.0001, 0.0001, 0.20417959183673468,\n",
      "                   0.20417959183673468, 0.40825918367346936,\n",
      "                   0.40825918367346936, 0.6123387755102041,\n",
      "                   0.6123387755102041, 0.8164183673469387,\n",
      "                   0.8164183673469387, 1.0204979591836734,\n",
      "                   1.0204979591836734, 1.2245775510204082,\n",
      "                   1.2245775510204082, 1.4286571428571428,\n",
      "                   1.4286571428571428, 1.6327367346938775,\n",
      "                   1.6327367346938775, 1.836816326530612,\n",
      "                   1.836816326530612, 2.040895918367347,\n",
      "                   2.040895918367347, 2.244975510204082,\n",
      "                   2.244975510204082, 2.4490551020408167,\n",
      "                   2.4490551020408167, 2.653134693877551,\n",
      "                   2.653134693877551, 2.857214285714286,\n",
      "                   2.857214285714286, 3.0612938775510203,\n",
      "                   3.0612938775510203, 3.265373469387755,\n",
      "                   3.265373469387755, 3.46945306122449, 3.46945306122449,\n",
      "                   3.6735326530612245, 3.6735326530612245,\n",
      "                   3.8776122448979593, 3.8776122448979593,\n",
      "                   4.081691836734693, 4.081691836734693,\n",
      "                   4.285771428571429, 4.285771428571429,\n",
      "                   4.489851020408163, 4.489851020408163,\n",
      "                   4.693930612244897, 4.693930612244897,\n",
      "                   4.898010204081633, 4.898010204081633,\n",
      "                   5.102089795918367, 5.102089795918367,\n",
      "                   5.3061693877551015, 5.3061693877551015,\n",
      "                   5.510248979591836, 5.510248979591836,\n",
      "                   5.714328571428571, 5.714328571428571,\n",
      "                   5.918408163265306, 5.918408163265306, 6.12248775510204,\n",
      "                   6.12248775510204, 6.326567346938775, 6.326567346938775,\n",
      "                   6.53064693877551, 6.53064693877551, 6.734726530612244,\n",
      "                   6.734726530612244, 6.938806122448979,\n",
      "                   6.938806122448979, 7.142885714285714,\n",
      "                   7.142885714285714, 7.346965306122448,\n",
      "                   7.346965306122448, 7.5510448979591835,\n",
      "                   7.5510448979591835, 7.755124489795918,\n",
      "                   7.755124489795918, 7.959204081632652,\n",
      "                   7.959204081632652, 8.163283673469387,\n",
      "                   8.163283673469387, 8.367363265306121,\n",
      "                   8.367363265306121, 8.571442857142857,\n",
      "                   8.571442857142857, 8.775522448979592,\n",
      "                   8.775522448979592, 8.979602040816326,\n",
      "                   8.979602040816326, 9.18368163265306, 9.18368163265306,\n",
      "                   9.387761224489795, 9.387761224489795, 9.59184081632653,\n",
      "                   9.59184081632653, 9.795920408163266, 9.795920408163266,\n",
      "                   10.0, 10.0],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_penalty': masked_array(data=['l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1',\n",
      "                   'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2',\n",
      "                   'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1',\n",
      "                   'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2',\n",
      "                   'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1',\n",
      "                   'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2',\n",
      "                   'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1',\n",
      "                   'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2',\n",
      "                   'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1',\n",
      "                   'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2',\n",
      "                   'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1',\n",
      "                   'l2'],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'C': 0.0001, 'penalty': 'l1'}, {'C': 0.0001, 'penalty': 'l2'}, {'C': 0.20417959183673468, 'penalty': 'l1'}, {'C': 0.20417959183673468, 'penalty': 'l2'}, {'C': 0.40825918367346936, 'penalty': 'l1'}, {'C': 0.40825918367346936, 'penalty': 'l2'}, {'C': 0.6123387755102041, 'penalty': 'l1'}, {'C': 0.6123387755102041, 'penalty': 'l2'}, {'C': 0.8164183673469387, 'penalty': 'l1'}, {'C': 0.8164183673469387, 'penalty': 'l2'}, {'C': 1.0204979591836734, 'penalty': 'l1'}, {'C': 1.0204979591836734, 'penalty': 'l2'}, {'C': 1.2245775510204082, 'penalty': 'l1'}, {'C': 1.2245775510204082, 'penalty': 'l2'}, {'C': 1.4286571428571428, 'penalty': 'l1'}, {'C': 1.4286571428571428, 'penalty': 'l2'}, {'C': 1.6327367346938775, 'penalty': 'l1'}, {'C': 1.6327367346938775, 'penalty': 'l2'}, {'C': 1.836816326530612, 'penalty': 'l1'}, {'C': 1.836816326530612, 'penalty': 'l2'}, {'C': 2.040895918367347, 'penalty': 'l1'}, {'C': 2.040895918367347, 'penalty': 'l2'}, {'C': 2.244975510204082, 'penalty': 'l1'}, {'C': 2.244975510204082, 'penalty': 'l2'}, {'C': 2.4490551020408167, 'penalty': 'l1'}, {'C': 2.4490551020408167, 'penalty': 'l2'}, {'C': 2.653134693877551, 'penalty': 'l1'}, {'C': 2.653134693877551, 'penalty': 'l2'}, {'C': 2.857214285714286, 'penalty': 'l1'}, {'C': 2.857214285714286, 'penalty': 'l2'}, {'C': 3.0612938775510203, 'penalty': 'l1'}, {'C': 3.0612938775510203, 'penalty': 'l2'}, {'C': 3.265373469387755, 'penalty': 'l1'}, {'C': 3.265373469387755, 'penalty': 'l2'}, {'C': 3.46945306122449, 'penalty': 'l1'}, {'C': 3.46945306122449, 'penalty': 'l2'}, {'C': 3.6735326530612245, 'penalty': 'l1'}, {'C': 3.6735326530612245, 'penalty': 'l2'}, {'C': 3.8776122448979593, 'penalty': 'l1'}, {'C': 3.8776122448979593, 'penalty': 'l2'}, {'C': 4.081691836734693, 'penalty': 'l1'}, {'C': 4.081691836734693, 'penalty': 'l2'}, {'C': 4.285771428571429, 'penalty': 'l1'}, {'C': 4.285771428571429, 'penalty': 'l2'}, {'C': 4.489851020408163, 'penalty': 'l1'}, {'C': 4.489851020408163, 'penalty': 'l2'}, {'C': 4.693930612244897, 'penalty': 'l1'}, {'C': 4.693930612244897, 'penalty': 'l2'}, {'C': 4.898010204081633, 'penalty': 'l1'}, {'C': 4.898010204081633, 'penalty': 'l2'}, {'C': 5.102089795918367, 'penalty': 'l1'}, {'C': 5.102089795918367, 'penalty': 'l2'}, {'C': 5.3061693877551015, 'penalty': 'l1'}, {'C': 5.3061693877551015, 'penalty': 'l2'}, {'C': 5.510248979591836, 'penalty': 'l1'}, {'C': 5.510248979591836, 'penalty': 'l2'}, {'C': 5.714328571428571, 'penalty': 'l1'}, {'C': 5.714328571428571, 'penalty': 'l2'}, {'C': 5.918408163265306, 'penalty': 'l1'}, {'C': 5.918408163265306, 'penalty': 'l2'}, {'C': 6.12248775510204, 'penalty': 'l1'}, {'C': 6.12248775510204, 'penalty': 'l2'}, {'C': 6.326567346938775, 'penalty': 'l1'}, {'C': 6.326567346938775, 'penalty': 'l2'}, {'C': 6.53064693877551, 'penalty': 'l1'}, {'C': 6.53064693877551, 'penalty': 'l2'}, {'C': 6.734726530612244, 'penalty': 'l1'}, {'C': 6.734726530612244, 'penalty': 'l2'}, {'C': 6.938806122448979, 'penalty': 'l1'}, {'C': 6.938806122448979, 'penalty': 'l2'}, {'C': 7.142885714285714, 'penalty': 'l1'}, {'C': 7.142885714285714, 'penalty': 'l2'}, {'C': 7.346965306122448, 'penalty': 'l1'}, {'C': 7.346965306122448, 'penalty': 'l2'}, {'C': 7.5510448979591835, 'penalty': 'l1'}, {'C': 7.5510448979591835, 'penalty': 'l2'}, {'C': 7.755124489795918, 'penalty': 'l1'}, {'C': 7.755124489795918, 'penalty': 'l2'}, {'C': 7.959204081632652, 'penalty': 'l1'}, {'C': 7.959204081632652, 'penalty': 'l2'}, {'C': 8.163283673469387, 'penalty': 'l1'}, {'C': 8.163283673469387, 'penalty': 'l2'}, {'C': 8.367363265306121, 'penalty': 'l1'}, {'C': 8.367363265306121, 'penalty': 'l2'}, {'C': 8.571442857142857, 'penalty': 'l1'}, {'C': 8.571442857142857, 'penalty': 'l2'}, {'C': 8.775522448979592, 'penalty': 'l1'}, {'C': 8.775522448979592, 'penalty': 'l2'}, {'C': 8.979602040816326, 'penalty': 'l1'}, {'C': 8.979602040816326, 'penalty': 'l2'}, {'C': 9.18368163265306, 'penalty': 'l1'}, {'C': 9.18368163265306, 'penalty': 'l2'}, {'C': 9.387761224489795, 'penalty': 'l1'}, {'C': 9.387761224489795, 'penalty': 'l2'}, {'C': 9.59184081632653, 'penalty': 'l1'}, {'C': 9.59184081632653, 'penalty': 'l2'}, {'C': 9.795920408163266, 'penalty': 'l1'}, {'C': 9.795920408163266, 'penalty': 'l2'}, {'C': 10.0, 'penalty': 'l1'}, {'C': 10.0, 'penalty': 'l2'}], 'split0_test_score': array([       nan, 0.81135899,        nan, 0.81893657,        nan,\n",
      "       0.81750634,        nan, 0.81676814,        nan, 0.81796662,\n",
      "              nan, 0.81506167,        nan, 0.81506167,        nan,\n",
      "       0.81312905,        nan, 0.81193328,        nan, 0.81432296,\n",
      "              nan, 0.81335729,        nan, 0.81432296,        nan,\n",
      "       0.81216392,        nan, 0.81239239,        nan, 0.8104649 ,\n",
      "              nan, 0.80927316,        nan, 0.8104649 ,        nan,\n",
      "       0.80807958,        nan, 0.80831109,        nan, 0.80927316,\n",
      "              nan, 0.81119957,        nan, 0.81000489,        nan,\n",
      "       0.80904185,        nan, 0.80904185,        nan, 0.80784585,\n",
      "              nan, 0.80664798,        nan, 0.80568682,        nan,\n",
      "       0.80568682,        nan, 0.80352778,        nan, 0.80352778,\n",
      "              nan, 0.80256869,        nan, 0.80256869,        nan,\n",
      "       0.80256869,        nan, 0.80256869,        nan, 0.80256869,\n",
      "              nan, 0.80256869,        nan, 0.80136875,        nan,\n",
      "       0.80256869,        nan, 0.80136875,        nan, 0.80136875,\n",
      "              nan, 0.80136875,        nan, 0.80136875,        nan,\n",
      "       0.80136875,        nan, 0.80136875,        nan, 0.80136875,\n",
      "              nan, 0.80328654,        nan, 0.80136875,        nan,\n",
      "       0.80328654,        nan, 0.80328654,        nan, 0.80448762]), 'split1_test_score': array([       nan, 0.65104636,        nan, 0.64049484,        nan,\n",
      "       0.63849202,        nan, 0.64423442,        nan, 0.6392922 ,\n",
      "              nan, 0.63915432,        nan, 0.63822148,        nan,\n",
      "       0.63728826,        nan, 0.63728826,        nan, 0.63835824,\n",
      "              nan, 0.64049484,        nan, 0.64022579,        nan,\n",
      "       0.64115902,        nan, 0.64115902,        nan, 0.64101895,\n",
      "              nan, 0.64288219,        nan, 0.6438133 ,        nan,\n",
      "       0.64273698,        nan, 0.64366702,        nan, 0.64459674,\n",
      "              nan, 0.64537478,        nan, 0.64537478,        nan,\n",
      "       0.64336557,        nan, 0.64228353,        nan, 0.64212641,\n",
      "              nan, 0.64212641,        nan, 0.64212641,        nan,\n",
      "       0.6412003 ,        nan, 0.64212641,        nan, 0.64212641,\n",
      "              nan, 0.64321038,        nan, 0.64321038,        nan,\n",
      "       0.64321038,        nan, 0.64321038,        nan, 0.64321038,\n",
      "              nan, 0.64228353,        nan, 0.64321038,        nan,\n",
      "       0.64413693,        nan, 0.64413693,        nan, 0.64522047,\n",
      "              nan, 0.64522047,        nan, 0.64614746,        nan,\n",
      "       0.64614746,        nan, 0.64614746,        nan, 0.64614746,\n",
      "              nan, 0.64614746,        nan, 0.64522047,        nan,\n",
      "       0.64522047,        nan, 0.64522047,        nan, 0.64522047]), 'split2_test_score': array([       nan, 0.73852166,        nan, 0.74227548,        nan,\n",
      "       0.74040605,        nan, 0.73945484,        nan, 0.73852166,\n",
      "              nan, 0.73945484,        nan, 0.74132455,        nan,\n",
      "       0.73990732,        nan, 0.74132455,        nan, 0.7422611 ,\n",
      "              nan, 0.74038914,        nan, 0.74038914,        nan,\n",
      "       0.74180209,        nan, 0.74180209,        nan, 0.74180209,\n",
      "              nan, 0.74273856,        nan, 0.74132455,        nan,\n",
      "       0.74132455,        nan, 0.74273856,        nan, 0.74132455,\n",
      "              nan, 0.74273856,        nan, 0.74414935,        nan,\n",
      "       0.74414935,        nan, 0.74321185,        nan, 0.74180209,\n",
      "              nan, 0.74086675,        nan, 0.74086675,        nan,\n",
      "       0.73993253,        nan, 0.74086675,        nan, 0.74086675,\n",
      "              nan, 0.73993253,        nan, 0.73852166,        nan,\n",
      "       0.73852166,        nan, 0.73710759,        nan, 0.73710759,\n",
      "              nan, 0.73710759,        nan, 0.73710759,        nan,\n",
      "       0.73803972,        nan, 0.73803972,        nan, 0.73662136,\n",
      "              nan, 0.73803972,        nan, 0.73662136,        nan,\n",
      "       0.73519976,        nan, 0.73519976,        nan, 0.73662136,\n",
      "              nan, 0.73662136,        nan, 0.73662136,        nan,\n",
      "       0.73662136,        nan, 0.73662136,        nan, 0.73662136]), 'split3_test_score': array([       nan, 0.84726257,        nan, 0.84159974,        nan,\n",
      "       0.84492583,        nan, 0.84394255,        nan, 0.84179003,\n",
      "              nan, 0.83963751,        nan, 0.84080888,        nan,\n",
      "       0.83787037,        nan, 0.83689249,        nan, 0.83670038,\n",
      "              nan, 0.83670038,        nan, 0.83552865,        nan,\n",
      "       0.83650642,        nan, 0.83552865,        nan, 0.83670038,\n",
      "              nan, 0.83650642,        nan, 0.83650642,        nan,\n",
      "       0.83650642,        nan, 0.83650642,        nan, 0.83650642,\n",
      "              nan, 0.83650642,        nan, 0.83650642,        nan,\n",
      "       0.83650642,        nan, 0.83650642,        nan, 0.83650642,\n",
      "              nan, 0.83767861,        nan, 0.83767861,        nan,\n",
      "       0.83767861,        nan, 0.83767861,        nan, 0.83865765,\n",
      "              nan, 0.83865765,        nan, 0.83865765,        nan,\n",
      "       0.83650642,        nan, 0.83650642,        nan, 0.83552865,\n",
      "              nan, 0.83552865,        nan, 0.83474634,        nan,\n",
      "       0.83572296,        nan, 0.83572296,        nan, 0.83572296,\n",
      "              nan, 0.83572296,        nan, 0.83377051,        nan,\n",
      "       0.83474634,        nan, 0.83474634,        nan, 0.83377051,\n",
      "              nan, 0.83474634,        nan, 0.83474634,        nan,\n",
      "       0.83474634,        nan, 0.83377051,        nan, 0.83474634]), 'split4_test_score': array([       nan, 0.70967044,        nan, 0.70472292,        nan,\n",
      "       0.70270048,        nan, 0.70253798,        nan, 0.70253798,\n",
      "              nan, 0.70144371,        nan, 0.70330203,        nan,\n",
      "       0.70237291,        nan, 0.70313366,        nan, 0.70389034,\n",
      "              nan, 0.70481791,        nan, 0.70742337,        nan,\n",
      "       0.70706165,        nan, 0.7068769 ,        nan, 0.7068769 ,\n",
      "              nan, 0.70780166,        nan, 0.70595213,        nan,\n",
      "       0.70484132,        nan, 0.7068769 ,        nan, 0.70668954,\n",
      "              nan, 0.70557611,        nan, 0.70649956,        nan,\n",
      "       0.70557611,        nan, 0.70484132,        nan, 0.7039172 ,\n",
      "              nan, 0.70484132,        nan, 0.70484132,        nan,\n",
      "       0.70317774,        nan, 0.70410256,        nan, 0.7022529 ,\n",
      "              nan, 0.69947815,        nan, 0.70040311,        nan,\n",
      "       0.70040311,        nan, 0.70040311,        nan, 0.70132802,\n",
      "              nan, 0.70132802,        nan, 0.7022529 ,        nan,\n",
      "       0.7022529 ,        nan, 0.70114471,        nan, 0.7020689 ,\n",
      "              nan, 0.7020689 ,        nan, 0.7039172 ,        nan,\n",
      "       0.7039172 ,        nan, 0.7020689 ,        nan, 0.70299306,\n",
      "              nan, 0.70317774,        nan, 0.70317774,        nan,\n",
      "       0.7022529 ,        nan, 0.7022529 ,        nan, 0.7022529 ]), 'mean_test_score': array([       nan, 0.751572  ,        nan, 0.74960591,        nan,\n",
      "       0.74880614,        nan, 0.74938758,        nan, 0.7480217 ,\n",
      "              nan, 0.74695041,        nan, 0.74774372,        nan,\n",
      "       0.74611358,        nan, 0.74611445,        nan, 0.7471066 ,\n",
      "              nan, 0.74715191,        nan, 0.74757798,        nan,\n",
      "       0.74773862,        nan, 0.74755181,        nan, 0.74737264,\n",
      "              nan, 0.7478404 ,        nan, 0.74761226,        nan,\n",
      "       0.74669777,        nan, 0.74762   ,        nan, 0.74767808,\n",
      "              nan, 0.74827909,        nan, 0.748507  ,        nan,\n",
      "       0.74772786,        nan, 0.74717699,        nan, 0.74643959,\n",
      "              nan, 0.74643221,        nan, 0.74623998,        nan,\n",
      "       0.7455352 ,        nan, 0.74566042,        nan, 0.7454863 ,\n",
      "              nan, 0.74476948,        nan, 0.7446723 ,        nan,\n",
      "       0.74424205,        nan, 0.74395924,        nan, 0.74394867,\n",
      "              nan, 0.7437633 ,        nan, 0.74373719,        nan,\n",
      "       0.74454424,        nan, 0.74408261,        nan, 0.74420049,\n",
      "              nan, 0.74448416,        nan, 0.74436506,        nan,\n",
      "       0.7442759 ,        nan, 0.74390624,        nan, 0.74418023,\n",
      "              nan, 0.74479589,        nan, 0.74422693,        nan,\n",
      "       0.74442552,        nan, 0.74423036,        nan, 0.74466574]), 'std_test_score': array([       nan, 0.07037418,        nan, 0.0738147 ,        nan,\n",
      "       0.07526514,        nan, 0.07325505,        nan, 0.07438537,\n",
      "              nan, 0.0734535 ,        nan, 0.07376208,        nan,\n",
      "       0.07308228,        nan, 0.07250683,        nan, 0.07247402,\n",
      "              nan, 0.07157613,        nan, 0.07124434,        nan,\n",
      "       0.07082289,        nan, 0.07064157,        nan, 0.07062878,\n",
      "              nan, 0.06968644,        nan, 0.06985979,        nan,\n",
      "       0.06988848,        nan, 0.06938913,        nan, 0.06932621,\n",
      "              nan, 0.06955083,        nan, 0.06920369,        nan,\n",
      "       0.06974539,        nan, 0.07016791,        nan, 0.07013368,\n",
      "              nan, 0.07013176,        nan, 0.06996757,        nan,\n",
      "       0.07045557,        nan, 0.06969262,        nan, 0.07017613,\n",
      "              nan, 0.07006381,        nan, 0.06996764,        nan,\n",
      "       0.06939263,        nan, 0.06941824,        nan, 0.06904306,\n",
      "              nan, 0.069314  ,        nan, 0.06851948,        nan,\n",
      "       0.06869191,        nan, 0.06862794,        nan, 0.06822629,\n",
      "              nan, 0.06819713,        nan, 0.0672093 ,        nan,\n",
      "       0.06750539,        nan, 0.06773007,        nan, 0.06732145,\n",
      "              nan, 0.06788615,        nan, 0.06782936,        nan,\n",
      "       0.06826939,        nan, 0.06801181,        nan, 0.06847787]), 'rank_test_score': array([100,   1,  62,   2,  98,   4,  96,   3,  94,   7,  92,  21,  90,\n",
      "         9,  88,  27,  86,  26,  82,  20,  80,  19,  78,  15,  76,  10,\n",
      "        74,  16,  72,  17,  70,   8,  99,  14,  56,  22,  71,  13,  73,\n",
      "        12,  75,   6,  77,   5,  79,  11,  81,  18,  83,  23,  85,  24,\n",
      "        87,  25,  89,  29,  91,  28,  93,  30,  95,  32,  97,  33,  69,\n",
      "        40,  68,  46,  54,  47,  53,  49,  52,  50,  51,  35,  55,  45,\n",
      "        66,  43,  57,  36,  67,  38,  58,  39,  59,  48,  60,  44,  61,\n",
      "        31,  63,  42,  64,  37,  65,  41,  84,  34], dtype=int32)}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best parameters from gridsearch: {}\".format(grid_search.best_params_))\n",
    "print(\"CV score=%0.3f\" % grid_search.best_score_)\n",
    "cv_resultsl = grid_search.cv_results_\n",
    "print(cv_resultsl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fb5618f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = grid_search.predict_proba(df_emb_test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61960c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions0=[]\n",
    "predictions1=[]\n",
    "\n",
    "\n",
    "for res in y_pred:\n",
    "    predictions0.append(res[0])\n",
    "    predictions1.append(res[1])\n",
    "\n",
    "    \n",
    "test_data['predict_0']=predictions0\n",
    "test_data['predict_1']=predictions1\n",
    "\n",
    "test_data_predict=TestCutoff(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "05439ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: [0.87977099 0.74642857], recall: [0.86654135 0.76838235], f1score: [0.87310606 0.75724638]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "precision, recall, f1score = precision_recall_fscore_support(test_data.label_test, test_data['predict'])[:3]\n",
    "\n",
    "print(f'precision: {precision}, recall: {recall}, f1score: {f1score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "92362b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.8130997818974919, recall: 0.8174618531623176, f1score_macro: 0.8151762187088274\n"
     ]
    }
   ],
   "source": [
    "precision, recall, f1score = precision_recall_fscore_support(test_data.label_test, test_data['predict'], average='macro')[:3]\n",
    "\n",
    "print(f'precision: {precision}, recall: {recall}, f1score_macro: {f1score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c506b6",
   "metadata": {},
   "source": [
    "# 2. Train models based on 768-embeddings from rubert-base-cased-conversational"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "af6f483c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased-conversational were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"DeepPavlov/rubert-base-cased-conversational\", model_max_length=512) \n",
    "model = AutoModel.from_pretrained(\"DeepPavlov/rubert-base-cased-conversational\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "51a11bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_bert_cls(text, model, tokenizer):\n",
    "    t = tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**{k: v.to(model.device) for k, v in t.items()})\n",
    "    embeddings = model_output.last_hidden_state[:, 0, :]\n",
    "    embeddings = torch.nn.functional.normalize(embeddings)\n",
    "    return embeddings[0].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d47a6d76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e113d8575dad41c693b3afef1e900ef1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5035 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 49min 11s, sys: 29.2 s, total: 1h 49min 41s\n",
      "Wall time: 2min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from tqdm.notebook import tqdm\n",
    "emb_list = list()\n",
    "it = 0\n",
    "for s in tqdm(df.text):\n",
    "    emb_list.append(embed_bert_cls(s, model, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f1436799",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05d05253ba2c4308ba71ad91629d8593",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/804 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15min 33s, sys: 2.04 s, total: 15min 35s\n",
      "Wall time: 19.5 s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%time\n",
    "from tqdm.notebook import tqdm\n",
    "emb_list_test = list()\n",
    "it = 0\n",
    "for s in tqdm(test_data.text):\n",
    "    emb_list_test.append(embed_bert_cls(s, model, tokenizer))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "76cc17a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>758</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.018975</td>\n",
       "      <td>-0.001351</td>\n",
       "      <td>-0.036334</td>\n",
       "      <td>0.015108</td>\n",
       "      <td>0.000549</td>\n",
       "      <td>0.070605</td>\n",
       "      <td>0.066554</td>\n",
       "      <td>0.029897</td>\n",
       "      <td>-0.054324</td>\n",
       "      <td>0.017511</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016930</td>\n",
       "      <td>-0.002973</td>\n",
       "      <td>-0.052835</td>\n",
       "      <td>0.018066</td>\n",
       "      <td>0.025775</td>\n",
       "      <td>0.002032</td>\n",
       "      <td>0.038878</td>\n",
       "      <td>0.027940</td>\n",
       "      <td>0.015232</td>\n",
       "      <td>-0.037016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.019812</td>\n",
       "      <td>-0.028492</td>\n",
       "      <td>-0.041963</td>\n",
       "      <td>0.012508</td>\n",
       "      <td>0.050236</td>\n",
       "      <td>-0.015364</td>\n",
       "      <td>-0.011181</td>\n",
       "      <td>0.004569</td>\n",
       "      <td>0.040616</td>\n",
       "      <td>-0.001826</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007038</td>\n",
       "      <td>0.053429</td>\n",
       "      <td>-0.044758</td>\n",
       "      <td>-0.001870</td>\n",
       "      <td>0.011583</td>\n",
       "      <td>0.002814</td>\n",
       "      <td>-0.034485</td>\n",
       "      <td>-0.001803</td>\n",
       "      <td>0.025618</td>\n",
       "      <td>-0.038578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.012166</td>\n",
       "      <td>0.011635</td>\n",
       "      <td>-0.017159</td>\n",
       "      <td>0.002655</td>\n",
       "      <td>0.044597</td>\n",
       "      <td>-0.002403</td>\n",
       "      <td>-0.000588</td>\n",
       "      <td>-0.000590</td>\n",
       "      <td>0.006924</td>\n",
       "      <td>-0.033347</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000399</td>\n",
       "      <td>-0.016995</td>\n",
       "      <td>-0.054655</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>-0.000498</td>\n",
       "      <td>-0.014621</td>\n",
       "      <td>0.002692</td>\n",
       "      <td>-0.017654</td>\n",
       "      <td>0.016672</td>\n",
       "      <td>0.000137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.025476</td>\n",
       "      <td>-0.018142</td>\n",
       "      <td>-0.087044</td>\n",
       "      <td>0.013581</td>\n",
       "      <td>0.045457</td>\n",
       "      <td>0.025133</td>\n",
       "      <td>-0.006145</td>\n",
       "      <td>0.012807</td>\n",
       "      <td>-0.035447</td>\n",
       "      <td>0.023708</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007615</td>\n",
       "      <td>0.026365</td>\n",
       "      <td>-0.060281</td>\n",
       "      <td>-0.017085</td>\n",
       "      <td>0.006862</td>\n",
       "      <td>-0.012635</td>\n",
       "      <td>0.010918</td>\n",
       "      <td>-0.003479</td>\n",
       "      <td>0.002557</td>\n",
       "      <td>-0.004744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.067215</td>\n",
       "      <td>-0.015028</td>\n",
       "      <td>-0.102502</td>\n",
       "      <td>0.009260</td>\n",
       "      <td>0.062075</td>\n",
       "      <td>0.012592</td>\n",
       "      <td>0.008576</td>\n",
       "      <td>0.021265</td>\n",
       "      <td>-0.046282</td>\n",
       "      <td>0.038958</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020768</td>\n",
       "      <td>0.039588</td>\n",
       "      <td>-0.055771</td>\n",
       "      <td>-0.049322</td>\n",
       "      <td>0.053859</td>\n",
       "      <td>-0.026940</td>\n",
       "      <td>0.021567</td>\n",
       "      <td>0.000866</td>\n",
       "      <td>0.027638</td>\n",
       "      <td>-0.065772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5030</th>\n",
       "      <td>-0.016395</td>\n",
       "      <td>-0.030579</td>\n",
       "      <td>-0.011700</td>\n",
       "      <td>-0.002913</td>\n",
       "      <td>-0.001894</td>\n",
       "      <td>0.033167</td>\n",
       "      <td>0.025396</td>\n",
       "      <td>-0.002383</td>\n",
       "      <td>-0.084536</td>\n",
       "      <td>0.012031</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021289</td>\n",
       "      <td>0.033067</td>\n",
       "      <td>-0.025002</td>\n",
       "      <td>-0.007566</td>\n",
       "      <td>0.049071</td>\n",
       "      <td>-0.008988</td>\n",
       "      <td>0.049975</td>\n",
       "      <td>0.048090</td>\n",
       "      <td>0.001161</td>\n",
       "      <td>-0.061774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5031</th>\n",
       "      <td>-0.002448</td>\n",
       "      <td>-0.005312</td>\n",
       "      <td>0.010221</td>\n",
       "      <td>0.041342</td>\n",
       "      <td>0.038213</td>\n",
       "      <td>0.014441</td>\n",
       "      <td>-0.044771</td>\n",
       "      <td>0.064974</td>\n",
       "      <td>-0.050657</td>\n",
       "      <td>0.013742</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009452</td>\n",
       "      <td>0.069418</td>\n",
       "      <td>-0.041314</td>\n",
       "      <td>-0.028628</td>\n",
       "      <td>0.010116</td>\n",
       "      <td>-0.002661</td>\n",
       "      <td>0.040237</td>\n",
       "      <td>0.023575</td>\n",
       "      <td>0.012390</td>\n",
       "      <td>-0.065933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5032</th>\n",
       "      <td>-0.049334</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>-0.075015</td>\n",
       "      <td>0.012169</td>\n",
       "      <td>0.025788</td>\n",
       "      <td>0.034515</td>\n",
       "      <td>-0.013789</td>\n",
       "      <td>0.051053</td>\n",
       "      <td>-0.015268</td>\n",
       "      <td>0.065273</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036013</td>\n",
       "      <td>0.036438</td>\n",
       "      <td>-0.067699</td>\n",
       "      <td>-0.045244</td>\n",
       "      <td>0.013065</td>\n",
       "      <td>-0.000260</td>\n",
       "      <td>-0.018512</td>\n",
       "      <td>0.005339</td>\n",
       "      <td>0.040650</td>\n",
       "      <td>-0.019662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5033</th>\n",
       "      <td>0.008010</td>\n",
       "      <td>0.003892</td>\n",
       "      <td>0.034859</td>\n",
       "      <td>-0.024992</td>\n",
       "      <td>-0.019697</td>\n",
       "      <td>0.008717</td>\n",
       "      <td>-0.005210</td>\n",
       "      <td>0.037962</td>\n",
       "      <td>-0.004088</td>\n",
       "      <td>0.024534</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032225</td>\n",
       "      <td>-0.013915</td>\n",
       "      <td>-0.017380</td>\n",
       "      <td>-0.023246</td>\n",
       "      <td>0.010574</td>\n",
       "      <td>-0.000123</td>\n",
       "      <td>0.009516</td>\n",
       "      <td>-0.019785</td>\n",
       "      <td>-0.003043</td>\n",
       "      <td>-0.040094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5034</th>\n",
       "      <td>-0.001773</td>\n",
       "      <td>0.022180</td>\n",
       "      <td>0.034181</td>\n",
       "      <td>-0.013939</td>\n",
       "      <td>0.034068</td>\n",
       "      <td>0.003047</td>\n",
       "      <td>0.024510</td>\n",
       "      <td>-0.046417</td>\n",
       "      <td>-0.004134</td>\n",
       "      <td>-0.000196</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008594</td>\n",
       "      <td>0.004026</td>\n",
       "      <td>-0.042404</td>\n",
       "      <td>-0.054164</td>\n",
       "      <td>0.028859</td>\n",
       "      <td>0.032703</td>\n",
       "      <td>0.011486</td>\n",
       "      <td>0.021293</td>\n",
       "      <td>0.029777</td>\n",
       "      <td>-0.018504</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5035 rows × 768 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6    \\\n",
       "0     0.018975 -0.001351 -0.036334  0.015108  0.000549  0.070605  0.066554   \n",
       "1     0.019812 -0.028492 -0.041963  0.012508  0.050236 -0.015364 -0.011181   \n",
       "2    -0.012166  0.011635 -0.017159  0.002655  0.044597 -0.002403 -0.000588   \n",
       "3    -0.025476 -0.018142 -0.087044  0.013581  0.045457  0.025133 -0.006145   \n",
       "4    -0.067215 -0.015028 -0.102502  0.009260  0.062075  0.012592  0.008576   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "5030 -0.016395 -0.030579 -0.011700 -0.002913 -0.001894  0.033167  0.025396   \n",
       "5031 -0.002448 -0.005312  0.010221  0.041342  0.038213  0.014441 -0.044771   \n",
       "5032 -0.049334  0.000191 -0.075015  0.012169  0.025788  0.034515 -0.013789   \n",
       "5033  0.008010  0.003892  0.034859 -0.024992 -0.019697  0.008717 -0.005210   \n",
       "5034 -0.001773  0.022180  0.034181 -0.013939  0.034068  0.003047  0.024510   \n",
       "\n",
       "           7         8         9    ...       758       759       760  \\\n",
       "0     0.029897 -0.054324  0.017511  ...  0.016930 -0.002973 -0.052835   \n",
       "1     0.004569  0.040616 -0.001826  ...  0.007038  0.053429 -0.044758   \n",
       "2    -0.000590  0.006924 -0.033347  ... -0.000399 -0.016995 -0.054655   \n",
       "3     0.012807 -0.035447  0.023708  ...  0.007615  0.026365 -0.060281   \n",
       "4     0.021265 -0.046282  0.038958  ...  0.020768  0.039588 -0.055771   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "5030 -0.002383 -0.084536  0.012031  ...  0.021289  0.033067 -0.025002   \n",
       "5031  0.064974 -0.050657  0.013742  ...  0.009452  0.069418 -0.041314   \n",
       "5032  0.051053 -0.015268  0.065273  ... -0.036013  0.036438 -0.067699   \n",
       "5033  0.037962 -0.004088  0.024534  ...  0.032225 -0.013915 -0.017380   \n",
       "5034 -0.046417 -0.004134 -0.000196  ...  0.008594  0.004026 -0.042404   \n",
       "\n",
       "           761       762       763       764       765       766       767  \n",
       "0     0.018066  0.025775  0.002032  0.038878  0.027940  0.015232 -0.037016  \n",
       "1    -0.001870  0.011583  0.002814 -0.034485 -0.001803  0.025618 -0.038578  \n",
       "2     0.000128 -0.000498 -0.014621  0.002692 -0.017654  0.016672  0.000137  \n",
       "3    -0.017085  0.006862 -0.012635  0.010918 -0.003479  0.002557 -0.004744  \n",
       "4    -0.049322  0.053859 -0.026940  0.021567  0.000866  0.027638 -0.065772  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "5030 -0.007566  0.049071 -0.008988  0.049975  0.048090  0.001161 -0.061774  \n",
       "5031 -0.028628  0.010116 -0.002661  0.040237  0.023575  0.012390 -0.065933  \n",
       "5032 -0.045244  0.013065 -0.000260 -0.018512  0.005339  0.040650 -0.019662  \n",
       "5033 -0.023246  0.010574 -0.000123  0.009516 -0.019785 -0.003043 -0.040094  \n",
       "5034 -0.054164  0.028859  0.032703  0.011486  0.021293  0.029777 -0.018504  \n",
       "\n",
       "[5035 rows x 768 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = df[\"label\"]\n",
    "\n",
    "emb_list_ = [np.asarray(s) for s in emb_list]\n",
    "df_emb_=pd.DataFrame(emb_list_)\n",
    "df_emb_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d74beddf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>758</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.054716</td>\n",
       "      <td>-0.045481</td>\n",
       "      <td>-0.023535</td>\n",
       "      <td>-0.000155</td>\n",
       "      <td>-0.006323</td>\n",
       "      <td>0.047523</td>\n",
       "      <td>-0.025077</td>\n",
       "      <td>-0.003400</td>\n",
       "      <td>-0.032581</td>\n",
       "      <td>-0.009139</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010738</td>\n",
       "      <td>0.006308</td>\n",
       "      <td>-0.068138</td>\n",
       "      <td>-0.055364</td>\n",
       "      <td>0.028434</td>\n",
       "      <td>-0.015068</td>\n",
       "      <td>-0.015639</td>\n",
       "      <td>-0.006466</td>\n",
       "      <td>0.031062</td>\n",
       "      <td>-0.048617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.028712</td>\n",
       "      <td>-0.043620</td>\n",
       "      <td>-0.051715</td>\n",
       "      <td>-0.027764</td>\n",
       "      <td>0.080676</td>\n",
       "      <td>-0.039356</td>\n",
       "      <td>-0.046797</td>\n",
       "      <td>0.057568</td>\n",
       "      <td>-0.024146</td>\n",
       "      <td>-0.034063</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023295</td>\n",
       "      <td>0.058056</td>\n",
       "      <td>-0.054193</td>\n",
       "      <td>-0.101050</td>\n",
       "      <td>0.006665</td>\n",
       "      <td>-0.029989</td>\n",
       "      <td>-0.019366</td>\n",
       "      <td>-0.080597</td>\n",
       "      <td>-0.002285</td>\n",
       "      <td>-0.030322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.016238</td>\n",
       "      <td>-0.010530</td>\n",
       "      <td>0.011108</td>\n",
       "      <td>-0.004964</td>\n",
       "      <td>0.079042</td>\n",
       "      <td>0.044689</td>\n",
       "      <td>-0.043167</td>\n",
       "      <td>-0.001452</td>\n",
       "      <td>-0.027746</td>\n",
       "      <td>0.011876</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020584</td>\n",
       "      <td>0.043124</td>\n",
       "      <td>-0.041857</td>\n",
       "      <td>0.006142</td>\n",
       "      <td>-0.021689</td>\n",
       "      <td>-0.022326</td>\n",
       "      <td>0.004118</td>\n",
       "      <td>0.018608</td>\n",
       "      <td>0.052731</td>\n",
       "      <td>-0.060148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.066568</td>\n",
       "      <td>0.027526</td>\n",
       "      <td>-0.023566</td>\n",
       "      <td>-0.004859</td>\n",
       "      <td>0.039345</td>\n",
       "      <td>0.018339</td>\n",
       "      <td>-0.058902</td>\n",
       "      <td>0.016316</td>\n",
       "      <td>-0.032561</td>\n",
       "      <td>-0.021920</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002746</td>\n",
       "      <td>0.079601</td>\n",
       "      <td>-0.076817</td>\n",
       "      <td>-0.012250</td>\n",
       "      <td>0.004977</td>\n",
       "      <td>0.024742</td>\n",
       "      <td>-0.002690</td>\n",
       "      <td>0.041313</td>\n",
       "      <td>0.074336</td>\n",
       "      <td>-0.044611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.019350</td>\n",
       "      <td>0.001813</td>\n",
       "      <td>-0.006678</td>\n",
       "      <td>-0.002628</td>\n",
       "      <td>0.065334</td>\n",
       "      <td>0.033174</td>\n",
       "      <td>-0.011546</td>\n",
       "      <td>0.049876</td>\n",
       "      <td>-0.026978</td>\n",
       "      <td>-0.001274</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018426</td>\n",
       "      <td>0.044450</td>\n",
       "      <td>-0.089290</td>\n",
       "      <td>0.004893</td>\n",
       "      <td>0.029819</td>\n",
       "      <td>-0.006972</td>\n",
       "      <td>0.000377</td>\n",
       "      <td>-0.013762</td>\n",
       "      <td>0.091146</td>\n",
       "      <td>-0.035349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>-0.033637</td>\n",
       "      <td>-0.005506</td>\n",
       "      <td>0.038416</td>\n",
       "      <td>-0.018379</td>\n",
       "      <td>0.051197</td>\n",
       "      <td>0.009491</td>\n",
       "      <td>-0.017781</td>\n",
       "      <td>0.052079</td>\n",
       "      <td>-0.037465</td>\n",
       "      <td>-0.013878</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016810</td>\n",
       "      <td>0.075761</td>\n",
       "      <td>-0.067668</td>\n",
       "      <td>-0.011157</td>\n",
       "      <td>-0.031862</td>\n",
       "      <td>-0.024369</td>\n",
       "      <td>0.043494</td>\n",
       "      <td>0.009058</td>\n",
       "      <td>0.051453</td>\n",
       "      <td>-0.036159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800</th>\n",
       "      <td>-0.028547</td>\n",
       "      <td>-0.045570</td>\n",
       "      <td>-0.068305</td>\n",
       "      <td>-0.007125</td>\n",
       "      <td>0.030637</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>0.019804</td>\n",
       "      <td>0.057383</td>\n",
       "      <td>-0.019331</td>\n",
       "      <td>0.030423</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014081</td>\n",
       "      <td>-0.007704</td>\n",
       "      <td>-0.040679</td>\n",
       "      <td>0.018558</td>\n",
       "      <td>0.017199</td>\n",
       "      <td>0.000757</td>\n",
       "      <td>0.007542</td>\n",
       "      <td>0.022014</td>\n",
       "      <td>0.029671</td>\n",
       "      <td>0.006060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>801</th>\n",
       "      <td>-0.025502</td>\n",
       "      <td>-0.001768</td>\n",
       "      <td>0.025397</td>\n",
       "      <td>0.016272</td>\n",
       "      <td>-0.003197</td>\n",
       "      <td>0.029201</td>\n",
       "      <td>0.000391</td>\n",
       "      <td>-0.004197</td>\n",
       "      <td>-0.005269</td>\n",
       "      <td>0.046244</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.017122</td>\n",
       "      <td>-0.004484</td>\n",
       "      <td>-0.029274</td>\n",
       "      <td>-0.000227</td>\n",
       "      <td>-0.028656</td>\n",
       "      <td>-0.014738</td>\n",
       "      <td>0.026732</td>\n",
       "      <td>0.028872</td>\n",
       "      <td>-0.014812</td>\n",
       "      <td>-0.007151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>802</th>\n",
       "      <td>-0.035723</td>\n",
       "      <td>0.011342</td>\n",
       "      <td>0.102803</td>\n",
       "      <td>-0.032850</td>\n",
       "      <td>-0.006633</td>\n",
       "      <td>-0.002737</td>\n",
       "      <td>-0.024466</td>\n",
       "      <td>0.003412</td>\n",
       "      <td>-0.032806</td>\n",
       "      <td>0.058926</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002785</td>\n",
       "      <td>0.057524</td>\n",
       "      <td>-0.003627</td>\n",
       "      <td>-0.007116</td>\n",
       "      <td>0.034572</td>\n",
       "      <td>0.005385</td>\n",
       "      <td>0.015048</td>\n",
       "      <td>-0.005996</td>\n",
       "      <td>0.033237</td>\n",
       "      <td>-0.015909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>803</th>\n",
       "      <td>0.004340</td>\n",
       "      <td>-0.013808</td>\n",
       "      <td>0.025314</td>\n",
       "      <td>0.002490</td>\n",
       "      <td>0.070841</td>\n",
       "      <td>-0.014354</td>\n",
       "      <td>-0.048107</td>\n",
       "      <td>0.021246</td>\n",
       "      <td>-0.038268</td>\n",
       "      <td>0.000188</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001595</td>\n",
       "      <td>0.052417</td>\n",
       "      <td>-0.052095</td>\n",
       "      <td>-0.029709</td>\n",
       "      <td>0.023515</td>\n",
       "      <td>-0.012521</td>\n",
       "      <td>0.030994</td>\n",
       "      <td>0.008346</td>\n",
       "      <td>0.052297</td>\n",
       "      <td>-0.054560</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>804 rows × 768 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6    \\\n",
       "0   -0.054716 -0.045481 -0.023535 -0.000155 -0.006323  0.047523 -0.025077   \n",
       "1   -0.028712 -0.043620 -0.051715 -0.027764  0.080676 -0.039356 -0.046797   \n",
       "2   -0.016238 -0.010530  0.011108 -0.004964  0.079042  0.044689 -0.043167   \n",
       "3    0.066568  0.027526 -0.023566 -0.004859  0.039345  0.018339 -0.058902   \n",
       "4    0.019350  0.001813 -0.006678 -0.002628  0.065334  0.033174 -0.011546   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "799 -0.033637 -0.005506  0.038416 -0.018379  0.051197  0.009491 -0.017781   \n",
       "800 -0.028547 -0.045570 -0.068305 -0.007125  0.030637  0.000143  0.019804   \n",
       "801 -0.025502 -0.001768  0.025397  0.016272 -0.003197  0.029201  0.000391   \n",
       "802 -0.035723  0.011342  0.102803 -0.032850 -0.006633 -0.002737 -0.024466   \n",
       "803  0.004340 -0.013808  0.025314  0.002490  0.070841 -0.014354 -0.048107   \n",
       "\n",
       "          7         8         9    ...       758       759       760  \\\n",
       "0   -0.003400 -0.032581 -0.009139  ... -0.010738  0.006308 -0.068138   \n",
       "1    0.057568 -0.024146 -0.034063  ...  0.023295  0.058056 -0.054193   \n",
       "2   -0.001452 -0.027746  0.011876  ... -0.020584  0.043124 -0.041857   \n",
       "3    0.016316 -0.032561 -0.021920  ... -0.002746  0.079601 -0.076817   \n",
       "4    0.049876 -0.026978 -0.001274  ... -0.018426  0.044450 -0.089290   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "799  0.052079 -0.037465 -0.013878  ...  0.016810  0.075761 -0.067668   \n",
       "800  0.057383 -0.019331  0.030423  ... -0.014081 -0.007704 -0.040679   \n",
       "801 -0.004197 -0.005269  0.046244  ... -0.017122 -0.004484 -0.029274   \n",
       "802  0.003412 -0.032806  0.058926  ...  0.002785  0.057524 -0.003627   \n",
       "803  0.021246 -0.038268  0.000188  ... -0.001595  0.052417 -0.052095   \n",
       "\n",
       "          761       762       763       764       765       766       767  \n",
       "0   -0.055364  0.028434 -0.015068 -0.015639 -0.006466  0.031062 -0.048617  \n",
       "1   -0.101050  0.006665 -0.029989 -0.019366 -0.080597 -0.002285 -0.030322  \n",
       "2    0.006142 -0.021689 -0.022326  0.004118  0.018608  0.052731 -0.060148  \n",
       "3   -0.012250  0.004977  0.024742 -0.002690  0.041313  0.074336 -0.044611  \n",
       "4    0.004893  0.029819 -0.006972  0.000377 -0.013762  0.091146 -0.035349  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "799 -0.011157 -0.031862 -0.024369  0.043494  0.009058  0.051453 -0.036159  \n",
       "800  0.018558  0.017199  0.000757  0.007542  0.022014  0.029671  0.006060  \n",
       "801 -0.000227 -0.028656 -0.014738  0.026732  0.028872 -0.014812 -0.007151  \n",
       "802 -0.007116  0.034572  0.005385  0.015048 -0.005996  0.033237 -0.015909  \n",
       "803 -0.029709  0.023515 -0.012521  0.030994  0.008346  0.052297 -0.054560  \n",
       "\n",
       "[804 rows x 768 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_test=test_data['label_test']\n",
    "\n",
    "emb_list_test_ = [np.asarray(s) for s in emb_list_test]\n",
    "df_emb_test_=pd.DataFrame(emb_list_test_)\n",
    "df_emb_test_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c12f6c66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.76264768, 1.45184544])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "class_weight = compute_class_weight(\n",
    "    class_weight='balanced', classes=np.unique(labels), y=labels)\n",
    "class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "40870bb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=SVC(class_weight={0: 0.7626476825204483,\n",
       "                                         1: 1.4518454440599768},\n",
       "                           probability=True),\n",
       "             param_grid={&#x27;C&#x27;: [10], &#x27;gamma&#x27;: [1], &#x27;kernel&#x27;: [&#x27;rbf&#x27;]},\n",
       "             scoring=&#x27;f1_macro&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=SVC(class_weight={0: 0.7626476825204483,\n",
       "                                         1: 1.4518454440599768},\n",
       "                           probability=True),\n",
       "             param_grid={&#x27;C&#x27;: [10], &#x27;gamma&#x27;: [1], &#x27;kernel&#x27;: [&#x27;rbf&#x27;]},\n",
       "             scoring=&#x27;f1_macro&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(class_weight={0: 0.7626476825204483, 1: 1.4518454440599768},\n",
       "    probability=True)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(class_weight={0: 0.7626476825204483, 1: 1.4518454440599768},\n",
       "    probability=True)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=SVC(class_weight={0: 0.7626476825204483,\n",
       "                                         1: 1.4518454440599768},\n",
       "                           probability=True),\n",
       "             param_grid={'C': [10], 'gamma': [1], 'kernel': ['rbf']},\n",
       "             scoring='f1_macro')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameteres = {'C': [0.1,1, 10, 100], 'gamma': [1,0.1,0.01,0.001],'kernel': ['rbf', 'poly', 'sigmoid']}\n",
    "clf = GridSearchCV(SVC(class_weight={0:class_weight[0], 1:class_weight[1]}, probability=True), param_grid=parameteres , cv=5, scoring='f1_macro')\n",
    "clf.fit(df_emb_, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "719f22cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters from gridsearch: {'C': 10, 'gamma': 1, 'kernel': 'rbf'}\n",
      "CV score=0.697\n",
      "{'mean_fit_time': array([25.33409033]), 'std_fit_time': array([1.52929982]), 'mean_score_time': array([1.46485128]), 'std_score_time': array([0.12510906]), 'param_C': masked_array(data=[10],\n",
      "             mask=[False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_gamma': masked_array(data=[1],\n",
      "             mask=[False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_kernel': masked_array(data=['rbf'],\n",
      "             mask=[False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'C': 10, 'gamma': 1, 'kernel': 'rbf'}], 'split0_test_score': array([0.77092125]), 'split1_test_score': array([0.58629813]), 'split2_test_score': array([0.69376972]), 'split3_test_score': array([0.774876]), 'split4_test_score': array([0.65853477]), 'mean_test_score': array([0.69687997]), 'std_test_score': array([0.07109693]), 'rank_test_score': array([1], dtype=int32)}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best parameters from gridsearch: {}\".format(clf.best_params_))\n",
    "print(\"CV score=%0.3f\" % clf.best_score_)\n",
    "cv_results = clf.cv_results_\n",
    "print(cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9ce5d0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict=clf.predict_proba(df_emb_test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ce4e1093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.005\n",
      "precision: [0.         0.33830846], recall: [0. 1.], f1score: [0.         0.50557621]\n",
      "macro:\n",
      "precision: 0.1691542288557214, recall: 0.5, f1score_macro: 0.2527881040892193\n",
      " \n",
      "0.01\n",
      "precision: [0.         0.33830846], recall: [0. 1.], f1score: [0.         0.50557621]\n",
      "macro:\n",
      "precision: 0.1691542288557214, recall: 0.5, f1score_macro: 0.2527881040892193\n",
      " \n",
      "0.015\n",
      "precision: [1.         0.33957553], recall: [0.0056391 1.       ], f1score: [0.01121495 0.50698975]\n",
      "macro:\n",
      "precision: 0.6697877652933832, recall: 0.5028195488721805, f1score_macro: 0.2591023508200434\n",
      " \n",
      "0.02\n",
      "precision: [1.         0.34042553], recall: [0.0093985 1.       ], f1score: [0.01862197 0.50793651]\n",
      "macro:\n",
      "precision: 0.6702127659574468, recall: 0.5046992481203008, f1score_macro: 0.2632792409328722\n",
      " \n",
      "0.025\n",
      "precision: [1.         0.34170854], recall: [0.01503759 1.        ], f1score: [0.02962963 0.5093633 ]\n",
      "macro:\n",
      "precision: 0.6708542713567839, recall: 0.5075187969924813, f1score_macro: 0.26949646275488975\n",
      " \n",
      "0.030000000000000002\n",
      "precision: [0.91666667 0.34217172], recall: [0.02067669 0.99632353], f1score: [0.04044118 0.5093985 ]\n",
      "macro:\n",
      "precision: 0.6294191919191919, recall: 0.508500110570544, f1score_macro: 0.27491983635559486\n",
      " \n",
      "0.034999999999999996\n",
      "precision: [0.9375     0.34390863], recall: [0.02819549 0.99632353], f1score: [0.05474453 0.51132075]\n",
      "macro:\n",
      "precision: 0.6407043147208122, recall: 0.5122595090667846, f1score_macro: 0.2830326401322132\n",
      " \n",
      "0.04\n",
      "precision: [0.94444444 0.34478372], recall: [0.03195489 0.99632353], f1score: [0.06181818 0.51228733]\n",
      "macro:\n",
      "precision: 0.6446140797285835, recall: 0.5141392083149049, f1score_macro: 0.2870527582058773\n",
      " \n",
      "0.045\n",
      "precision: [0.96153846 0.34832905], recall: [0.04699248 0.99632353], f1score: [0.08960573 0.51619048]\n",
      "macro:\n",
      "precision: 0.6549337551908246, recall: 0.5216580053073862, f1score_macro: 0.3028981054787506\n",
      " \n",
      "0.049999999999999996\n",
      "precision: [0.96428571 0.3492268 ], recall: [0.05075188 0.99632353], f1score: [0.09642857 0.51717557]\n",
      "macro:\n",
      "precision: 0.6567562592047128, recall: 0.5235377045555064, f1score_macro: 0.30680207197382775\n",
      " \n",
      "0.055\n",
      "precision: [0.96969697 0.35149157], recall: [0.06015038 0.99632353], f1score: [0.11327434 0.51965484]\n",
      "macro:\n",
      "precision: 0.6605942695436859, recall: 0.5282369526758072, f1score_macro: 0.3164645890428393\n",
      " \n",
      "0.06\n",
      "precision: [0.97435897 0.35424837], recall: [0.07142857 0.99632353], f1score: [0.13309982 0.52266152]\n",
      "macro:\n",
      "precision: 0.6643036701860231, recall: 0.5338760504201681, f1score_macro: 0.3278806742472476\n",
      " \n",
      "0.065\n",
      "precision: [0.96078431 0.35856574], recall: [0.09210526 0.99264706], f1score: [0.16809605 0.52682927]\n",
      "macro:\n",
      "precision: 0.6596750253886415, recall: 0.5423761609907121, f1score_macro: 0.34746266159059536\n",
      " \n",
      "0.07\n",
      "precision: [0.953125   0.36351351], recall: [0.11466165 0.98897059], f1score: [0.20469799 0.53162055]\n",
      "macro:\n",
      "precision: 0.6583192567567567, recall: 0.5518161211853162, f1score_macro: 0.3681592699684325\n",
      " \n",
      "0.07500000000000001\n",
      "precision: [0.96103896 0.37001376], recall: [0.13909774 0.98897059], f1score: [0.24302135 0.53853854]\n",
      "macro:\n",
      "precision: 0.6655263580985726, recall: 0.5640341662980982, f1score_macro: 0.39077994250408044\n",
      " \n",
      "0.08\n",
      "precision: [0.96385542 0.37309293], recall: [0.15037594 0.98897059], f1score: [0.2601626  0.54179255]\n",
      "macro:\n",
      "precision: 0.6684741740888659, recall: 0.5696732640424591, f1score_macro: 0.40097757473043005\n",
      " \n",
      "0.085\n",
      "precision: [0.96629213 0.37622378], recall: [0.16165414 0.98897059], f1score: [0.27697262 0.54508612]\n",
      "macro:\n",
      "precision: 0.6712579555276185, recall: 0.57531236178682, f1score_macro: 0.41102937217645824\n",
      " \n",
      "0.09000000000000001\n",
      "precision: [0.95959596 0.38014184], recall: [0.17857143 0.98529412], f1score: [0.30110935 0.54861822]\n",
      "macro:\n",
      "precision: 0.6698689017837953, recall: 0.5819327731092437, f1score_macro: 0.42486378463779445\n",
      " \n",
      "0.095\n",
      "precision: [0.95283019 0.38252149], recall: [0.18984962 0.98161765], f1score: [0.31661442 0.55051546]\n",
      "macro:\n",
      "precision: 0.667675839325296, recall: 0.5857336355594869, f1score_macro: 0.43356494199011086\n",
      " \n",
      "0.1\n",
      "precision: [0.95762712 0.38921283], recall: [0.21240602 0.98161765], f1score: [0.34769231 0.55741127]\n",
      "macro:\n",
      "precision: 0.673419973316203, recall: 0.5970118310482088, f1score_macro: 0.4525517905893689\n",
      " \n",
      "0.10500000000000001\n",
      "precision: [0.96153846 0.39614243], recall: [0.23496241 0.98161765], f1score: [0.3776435  0.56448203]\n",
      "macro:\n",
      "precision: 0.6788404473864414, recall: 0.6082900265369305, f1score_macro: 0.4710627670650154\n",
      " \n",
      "0.11\n",
      "precision: [0.96453901 0.40271493], recall: [0.2556391  0.98161765], f1score: [0.40416048 0.57112299]\n",
      "macro:\n",
      "precision: 0.6836269696094477, recall: 0.6186283724015922, f1score_macro: 0.4876417350676594\n",
      " \n",
      "0.115\n",
      "precision: [0.96753247 0.41076923], recall: [0.28007519 0.98161765], f1score: [0.43440233 0.5791757 ]\n",
      "macro:\n",
      "precision: 0.6891508491508491, recall: 0.6308464175143742, f1score_macro: 0.506789018675335\n",
      " \n",
      "0.12000000000000001\n",
      "precision: [0.95953757 0.4199683 ], recall: [0.31203008 0.97426471], f1score: [0.47092199 0.58693245]\n",
      "macro:\n",
      "precision: 0.6897529382666288, recall: 0.6431473905351615, f1score_macro: 0.5289272166065833\n",
      " \n",
      "0.125\n",
      "precision: [0.96174863 0.42673108], recall: [0.33082707 0.97426471], f1score: [0.49230769 0.59350504]\n",
      "macro:\n",
      "precision: 0.6942398563923866, recall: 0.652545886775763, f1score_macro: 0.5429063657507106\n",
      " \n",
      "0.13\n",
      "precision: [0.96335079 0.43230016], recall: [0.34586466 0.97426471], f1score: [0.50899032 0.59887006]\n",
      "macro:\n",
      "precision: 0.6978254742362255, recall: 0.6600646837682441, f1score_macro: 0.553930187308062\n",
      " \n",
      "0.135\n",
      "precision: [0.96534653 0.44019934], recall: [0.36654135 0.97426471], f1score: [0.53133515 0.60640732]\n",
      "macro:\n",
      "precision: 0.7027729351008191, recall: 0.6704030296329058, f1score_macro: 0.5688712362591113\n",
      " \n",
      "0.14\n",
      "precision: [0.96153846 0.44295302], recall: [0.37593985 0.97058824], f1score: [0.54054054 0.60829493]\n",
      "macro:\n",
      "precision: 0.702245740836345, recall: 0.6732640424590889, f1score_macro: 0.5744177357080583\n",
      " \n",
      "0.14500000000000002\n",
      "precision: [0.95890411 0.44957265], recall: [0.39473684 0.96691176], f1score: [0.55925433 0.61376896]\n",
      "macro:\n",
      "precision: 0.7042383795808453, recall: 0.6808243034055728, f1score_macro: 0.5865116445284156\n",
      " \n",
      "0.15\n",
      "precision: [0.95594714 0.45407279], recall: [0.40789474 0.96323529], f1score: [0.57180501 0.6171967 ]\n",
      "macro:\n",
      "precision: 0.705009963429252, recall: 0.6855650154798761, f1score_macro: 0.5945008542949854\n",
      " \n",
      "0.155\n",
      "precision: [0.94047619 0.46557971], recall: [0.44548872 0.94485294], f1score: [0.60459184 0.62378641]\n",
      "macro:\n",
      "precision: 0.703027950310559, recall: 0.695170831490491, f1score_macro: 0.6141891222508422\n",
      " \n",
      "0.16\n",
      "precision: [0.93385214 0.46617916], recall: [0.45112782 0.9375    ], f1score: [0.60836502 0.62271062]\n",
      "macro:\n",
      "precision: 0.7000156495635905, recall: 0.6943139097744361, f1score_macro: 0.6155378208610147\n",
      " \n",
      "0.165\n",
      "precision: [0.93536122 0.47134935], recall: [0.46240602 0.9375    ], f1score: [0.61886792 0.62730627]\n",
      "macro:\n",
      "precision: 0.7033552848899728, recall: 0.699953007518797, f1score_macro: 0.6230870987955163\n",
      " \n",
      "0.17\n",
      "precision: [0.93333333 0.47565543], recall: [0.47368421 0.93382353], f1score: [0.62842893 0.63027295]\n",
      "macro:\n",
      "precision: 0.7044943820224719, recall: 0.7037538699690402, f1score_macro: 0.629350940267198\n",
      " \n",
      "0.17500000000000002\n",
      "precision: [0.93286219 0.48560461], recall: [0.4962406  0.93014706], f1score: [0.64785276 0.63808323]\n",
      "macro:\n",
      "precision: 0.7092333986693162, recall: 0.7131938301636445, f1score_macro: 0.6429679944916795\n",
      " \n",
      "0.18000000000000002\n",
      "precision: [0.93356643 0.48841699], recall: [0.5018797  0.93014706], f1score: [0.65281174 0.64050633]\n",
      "macro:\n",
      "precision: 0.710991710991711, recall: 0.7160133790358248, f1score_macro: 0.6466590325276222\n",
      " \n",
      "0.185\n",
      "precision: [0.92857143 0.49215686], recall: [0.51315789 0.92279412], f1score: [0.66101695 0.64194373]\n",
      "macro:\n",
      "precision: 0.7103641456582633, recall: 0.7179760061919505, f1score_macro: 0.6514803415839439\n",
      " \n",
      "0.19\n",
      "precision: [0.92384106 0.49601594], recall: [0.52443609 0.91544118], f1score: [0.66906475 0.64341085]\n",
      "macro:\n",
      "precision: 0.7099284979288145, recall: 0.7199386333480761, f1score_macro: 0.6562378004573086\n",
      " \n",
      "0.195\n",
      "precision: [0.91935484 0.5       ], recall: [0.53571429 0.90808824], f1score: [0.67695962 0.64490862]\n",
      "macro:\n",
      "precision: 0.7096774193548387, recall: 0.7219012605042017, f1score_macro: 0.6609341180702418\n",
      " \n",
      "0.2\n",
      "precision: [0.9184953  0.50721649], recall: [0.55075188 0.90441176], f1score: [0.68860165 0.64993395]\n",
      "macro:\n",
      "precision: 0.7128558963255017, recall: 0.7275818222025652, f1score_macro: 0.6692677974626169\n",
      " \n",
      "0.20500000000000002\n",
      "precision: [0.90963855 0.51271186], recall: [0.56766917 0.88970588], f1score: [0.69907407 0.65053763]\n",
      "macro:\n",
      "precision: 0.7111752093118235, recall: 0.728687527642636, f1score_macro: 0.6748058542413381\n",
      " \n",
      "0.21000000000000002\n",
      "precision: [0.91044776 0.51599147], recall: [0.57330827 0.88970588], f1score: [0.70357555 0.65317139]\n",
      "macro:\n",
      "precision: 0.7132196162046909, recall: 0.7315070765148164, f1score_macro: 0.6783734689398503\n",
      " \n",
      "0.215\n",
      "precision: [0.90909091 0.52051836], recall: [0.58270677 0.88602941], f1score: [0.71019473 0.65578231]\n",
      "macro:\n",
      "precision: 0.7148046338111133, recall: 0.7343680893409996, f1score_macro: 0.6829885218692289\n",
      " \n",
      "0.22\n",
      "precision: [0.90857143 0.52863436], recall: [0.59774436 0.88235294], f1score: [0.72108844 0.66115702]\n",
      "macro:\n",
      "precision: 0.7186028949024543, recall: 0.7400486510393631, f1score_macro: 0.691122730083769\n",
      " \n",
      "0.225\n",
      "precision: [0.90833333 0.53828829], recall: [0.61466165 0.87867647], f1score: [0.73318386 0.66759777]\n",
      "macro:\n",
      "precision: 0.7233108108108108, recall: 0.7466690623617869, f1score_macro: 0.7003908109326853\n",
      " \n",
      "0.23\n",
      "precision: [0.90217391 0.5412844 ], recall: [0.62406015 0.86764706], f1score: [0.73777778 0.66666667]\n",
      "macro:\n",
      "precision: 0.7217291583566015, recall: 0.7458536045997346, f1score_macro: 0.7022222222222222\n",
      " \n",
      "0.23500000000000001\n",
      "precision: [0.90296496 0.54503464], recall: [0.62969925 0.86764706], f1score: [0.74197121 0.66950355]\n",
      "macro:\n",
      "precision: 0.7239998008005328, recall: 0.7486731534719151, f1score_macro: 0.7057373765933885\n",
      " \n",
      "0.24000000000000002\n",
      "precision: [0.90526316 0.55660377], recall: [0.64661654 0.86764706], f1score: [0.75438596 0.67816092]\n",
      "macro:\n",
      "precision: 0.7309334657398212, recall: 0.7571318000884564, f1score_macro: 0.7162734422262553\n",
      " \n",
      "0.245\n",
      "precision: [0.90439276 0.56354916], recall: [0.65789474 0.86397059], f1score: [0.7616975  0.68214804]\n",
      "macro:\n",
      "precision: 0.733970962764672, recall: 0.7609326625386997, f1score_macro: 0.7219227689591292\n",
      " \n",
      "0.25\n",
      "precision: [0.90609137 0.57317073], recall: [0.67105263 0.86397059], f1score: [0.77105832 0.68914956]\n",
      "macro:\n",
      "precision: 0.7396310511328463, recall: 0.7675116099071208, f1score_macro: 0.7301039377260377\n",
      " \n",
      "0.255\n",
      "precision: [0.90225564 0.57530864], recall: [0.67669173 0.85661765], f1score: [0.77336198 0.68833087]\n",
      "macro:\n",
      "precision: 0.7387821405365265, recall: 0.7666546881910659, f1score_macro: 0.7308464239306856\n",
      " \n",
      "0.26\n",
      "precision: [0.9009901 0.58     ], recall: [0.68421053 0.85294118], f1score: [0.77777778 0.69047619]\n",
      "macro:\n",
      "precision: 0.7404950495049505, recall: 0.7685758513931888, f1score_macro: 0.7341269841269842\n",
      " \n",
      "0.265\n",
      "precision: [0.89588378 0.58567775], recall: [0.69548872 0.84191176], f1score: [0.78306878 0.6907994 ]\n",
      "macro:\n",
      "precision: 0.7407807633001616, recall: 0.7687002432551968, f1score_macro: 0.7369340898752663\n",
      " \n",
      "0.27\n",
      "precision: [0.89638554 0.58868895], recall: [0.69924812 0.84191176], f1score: [0.78563886 0.69288956]\n",
      "macro:\n",
      "precision: 0.7425372440920495, recall: 0.7705799425033171, f1score_macro: 0.7392642104136481\n",
      " \n",
      "0.275\n",
      "precision: [0.89047619 0.58854167], recall: [0.70300752 0.83088235], f1score: [0.78571429 0.68902439]\n",
      "macro:\n",
      "precision: 0.7395089285714285, recall: 0.7669449358690845, f1score_macro: 0.7373693379790942\n",
      " \n",
      "0.28\n",
      "precision: [0.89125296 0.59317585], recall: [0.70864662 0.83088235], f1score: [0.7895288  0.69218989]\n",
      "macro:\n",
      "precision: 0.7422144040505575, recall: 0.769764484741265, f1score_macro: 0.7408593443069842\n",
      " \n",
      "0.28500000000000003\n",
      "precision: [0.89069767 0.60160428], recall: [0.71992481 0.82720588], f1score: [0.7962578  0.69659443]\n",
      "macro:\n",
      "precision: 0.7461509762467355, recall: 0.7735653471915082, f1score_macro: 0.7464261117511892\n",
      " \n",
      "0.29000000000000004\n",
      "precision: [0.89016018 0.61035422], recall: [0.73120301 0.82352941], f1score: [0.80288958 0.70109546]\n",
      "macro:\n",
      "precision: 0.750257203249802, recall: 0.7773662096417514, f1score_macro: 0.7519925192711134\n",
      " \n",
      "0.295\n",
      "precision: [0.88939052 0.61772853], recall: [0.7406015  0.81985294], f1score: [0.80820513 0.70458136]\n",
      "macro:\n",
      "precision: 0.7535595255216573, recall: 0.7802272224679345, f1score_macro: 0.7563932434074614\n",
      " \n",
      "0.3\n",
      "precision: [0.88641425 0.62253521], recall: [0.7481203 0.8125   ], f1score: [0.81141692 0.70494418]\n",
      "macro:\n",
      "precision: 0.7544747325825778, recall: 0.7803101503759399, f1score_macro: 0.7581805500685268\n",
      " \n",
      "0.305\n",
      "precision: [0.88596491 0.63218391], recall: [0.7593985  0.80882353], f1score: [0.81781377 0.70967742]\n",
      "macro:\n",
      "precision: 0.7590744101633393, recall: 0.7841110128261831, f1score_macro: 0.7637455922685126\n",
      " \n",
      "0.31\n",
      "precision: [0.88646288 0.63583815], recall: [0.76315789 0.80882353], f1score: [0.82020202 0.71197411]\n",
      "macro:\n",
      "precision: 0.7611505161925436, recall: 0.7859907120743035, f1score_macro: 0.7660880651171913\n",
      " \n",
      "0.315\n",
      "precision: [0.88528139 0.64035088], recall: [0.76879699 0.80514706], f1score: [0.82293763 0.71335505]\n",
      "macro:\n",
      "precision: 0.762816131237184, recall: 0.7869720256523662, f1score_macro: 0.768146337307231\n",
      " \n",
      "0.32\n",
      "precision: [0.88160677 0.65256798], recall: [0.78383459 0.79411765], f1score: [0.82985075 0.71641791]\n",
      "macro:\n",
      "precision: 0.7670873705792556, recall: 0.7889761167624945, f1score_macro: 0.7731343283582088\n",
      " \n",
      "0.325\n",
      "precision: [0.87682672 0.65538462], recall: [0.78947368 0.78308824], f1score: [0.83086053 0.71356784]\n",
      "macro:\n",
      "precision: 0.76610566886141, recall: 0.7862809597523219, f1score_macro: 0.7722141866603045\n",
      " \n",
      "0.33\n",
      "precision: [0.87551867 0.65838509], recall: [0.79323308 0.77941176], f1score: [0.83234714 0.71380471]\n",
      "macro:\n",
      "precision: 0.7669518826834361, recall: 0.7863224237063247, f1score_macro: 0.7730759269220807\n",
      " \n",
      "0.335\n",
      "precision: [0.87679671 0.66876972], recall: [0.80263158 0.77941176], f1score: [0.83807655 0.71986418]\n",
      "macro:\n",
      "precision: 0.7727832153336918, recall: 0.7910216718266254, f1score_macro: 0.778970361101716\n",
      " \n",
      "0.34\n",
      "precision: [0.87730061 0.67301587], recall: [0.80639098 0.77941176], f1score: [0.8403526  0.72231687]\n",
      "macro:\n",
      "precision: 0.7751582432564028, recall: 0.7929013710747457, f1score_macro: 0.7813347304559948\n",
      " \n",
      "0.34500000000000003\n",
      "precision: [0.87525151 0.68403909], recall: [0.81766917 0.77205882], f1score: [0.84548105 0.7253886 ]\n",
      "macro:\n",
      "precision: 0.7796452985011043, recall: 0.7948639982308714, f1score_macro: 0.7854348252994758\n",
      " \n",
      "0.35000000000000003\n",
      "precision: [0.872      0.68421053], recall: [0.81954887 0.76470588], f1score: [0.84496124 0.72222222]\n",
      "macro:\n",
      "precision: 0.7781052631578947, recall: 0.7921273772666961, f1score_macro: 0.7835917312661499\n",
      " \n",
      "0.35500000000000004\n",
      "precision: [0.86811024 0.69256757], recall: [0.82894737 0.75367647], f1score: [0.84807692 0.72183099]\n",
      "macro:\n",
      "precision: 0.78033890189402, recall: 0.791311919504644, f1score_macro: 0.7849539544962081\n",
      " \n",
      "0.36\n",
      "precision: [0.86692759 0.69624573], recall: [0.83270677 0.75      ], f1score: [0.84947267 0.72212389]\n",
      "macro:\n",
      "precision: 0.781586663371693, recall: 0.7913533834586466, f1score_macro: 0.7857982843906701\n",
      " \n",
      "0.365\n",
      "precision: [0.86434109 0.70138889], recall: [0.83834586 0.74264706], f1score: [0.85114504 0.72142857]\n",
      "macro:\n",
      "precision: 0.7828649870801033, recall: 0.7904964617425918, f1score_macro: 0.7862868047982552\n",
      " \n",
      "0.37\n",
      "precision: [0.86512524 0.70877193], recall: [0.84398496 0.74264706], f1score: [0.85442436 0.72531418]\n",
      "macro:\n",
      "precision: 0.7869485853361728, recall: 0.7933160106147723, f1score_macro: 0.7898692704391987\n",
      " \n",
      "0.375\n",
      "precision: [0.86206897 0.70921986], recall: [0.84586466 0.73529412], f1score: [0.85388994 0.72202166]\n",
      "macro:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.7856444118366348, recall: 0.790579389650597, f1score_macro: 0.7879558018619115\n",
      " \n",
      "0.38\n",
      "precision: [0.85931559 0.71223022], recall: [0.84962406 0.72794118], f1score: [0.85444234 0.72      ]\n",
      "macro:\n",
      "precision: 0.7857729025904752, recall: 0.788782618310482, f1score_macro: 0.7872211720226843\n",
      " \n",
      "0.385\n",
      "precision: [0.85499058 0.71428571], recall: [0.85338346 0.71691176], f1score: [0.85418627 0.71559633]\n",
      "macro:\n",
      "precision: 0.7846381490449288, recall: 0.7851476116762495, f1score_macro: 0.7848912977810766\n",
      " \n",
      "0.39\n",
      "precision: [0.84972171 0.72075472], recall: [0.86090226 0.70220588], f1score: [0.85527544 0.7113594 ]\n",
      "macro:\n",
      "precision: 0.785238211922848, recall: 0.7815540689960194, f1score_macro: 0.7833174238037859\n",
      " \n",
      "0.395\n",
      "precision: [0.84587156 0.72586873], recall: [0.86654135 0.69117647], f1score: [0.85608171 0.70809793]\n",
      "macro:\n",
      "precision: 0.7858701427508767, recall: 0.7788589119858469, f1score_macro: 0.782089818443154\n",
      " \n",
      "0.4\n",
      "precision: [0.84489051 0.73046875], recall: [0.87030075 0.6875    ], f1score: [0.85740741 0.70833333]\n",
      "macro:\n",
      "precision: 0.7876796304744526, recall: 0.7789003759398496, f1score_macro: 0.7828703703703704\n",
      " \n",
      "0.405\n",
      "precision: [0.84448463 0.74103586], recall: [0.87781955 0.68382353], f1score: [0.86082949 0.71128107]\n",
      "macro:\n",
      "precision: 0.7927602429342305, recall: 0.7808215391419726, f1score_macro: 0.7860552819166278\n",
      " \n",
      "0.41000000000000003\n",
      "precision: [0.84135472 0.75308642], recall: [0.88721805 0.67279412], f1score: [0.86367795 0.71067961]\n",
      "macro:\n",
      "precision: 0.7972205717303756, recall: 0.7800060813799203, f1score_macro: 0.7871787811225894\n",
      " \n",
      "0.41500000000000004\n",
      "precision: [0.84070796 0.76150628], recall: [0.89285714 0.66911765], f1score: [0.86599818 0.71232877]\n",
      "macro:\n",
      "precision: 0.8011071203761988, recall: 0.7809873949579832, f1score_macro: 0.7891634719846156\n",
      " \n",
      "0.42000000000000004\n",
      "precision: [0.8377425  0.75949367], recall: [0.89285714 0.66176471], f1score: [0.8644222  0.70726916]\n",
      "macro:\n",
      "precision: 0.7986180876476235, recall: 0.7773109243697479, f1score_macro: 0.7858456786040534\n",
      " \n",
      "0.425\n",
      "precision: [0.83449477 0.76956522], recall: [0.90037594 0.65073529], f1score: [0.86618445 0.70517928]\n",
      "macro:\n",
      "precision: 0.8020299954552341, recall: 0.7755556169836355, f1score_macro: 0.7856818656657277\n",
      " \n",
      "0.43\n",
      "precision: [0.83362218 0.7753304 ], recall: [0.90413534 0.64705882], f1score: [0.86744815 0.70541082]\n",
      "macro:\n",
      "precision: 0.8044762900923048, recall: 0.7755970809376382, f1score_macro: 0.7864294865655567\n",
      " \n",
      "0.435\n",
      "precision: [0.83304647 0.78475336], recall: [0.90977444 0.64338235], f1score: [0.86972147 0.70707071]\n",
      "macro:\n",
      "precision: 0.808899917414694, recall: 0.776578394515701, f1score_macro: 0.7883960902828828\n",
      " \n",
      "0.44\n",
      "precision: [0.83361921 0.7918552 ], recall: [0.91353383 0.64338235], f1score: [0.87174888 0.70993915]\n",
      "macro:\n",
      "precision: 0.8127372072988055, recall: 0.7784580937638214, f1score_macro: 0.7908440134983945\n",
      " \n",
      "0.445\n",
      "precision: [0.82964225 0.79262673], recall: [0.91541353 0.63235294], f1score: [0.87042002 0.70347648]\n",
      "macro:\n",
      "precision: 0.811134488416458, recall: 0.7738832375055285, f1score_macro: 0.786948250245344\n",
      " \n",
      "0.45\n",
      "precision: [0.82852292 0.79534884], recall: [0.91729323 0.62867647], f1score: [0.8706512  0.70225873]\n",
      "macro:\n",
      "precision: 0.8119358787065187, recall: 0.7729848518354709, f1score_macro: 0.7864549655906377\n",
      " \n",
      "0.455\n",
      "precision: [0.82571912 0.79342723], recall: [0.91729323 0.62132353], f1score: [0.86910062 0.69690722]\n",
      "macro:\n",
      "precision: 0.8095731750911561, recall: 0.7693083812472357, f1score_macro: 0.7830039199126052\n",
      " \n",
      "0.46\n",
      "precision: [0.82077052 0.79710145], recall: [0.92105263 0.60661765], f1score: [0.8680248  0.68893528]\n",
      "macro:\n",
      "precision: 0.8089359842691719, recall: 0.7638351393188854, f1score_macro: 0.7784800412728763\n",
      " \n",
      "0.465\n",
      "precision: [0.82107023 0.80097087], recall: [0.92293233 0.60661765], f1score: [0.86902655 0.69037657]\n",
      "macro:\n",
      "precision: 0.8110205539500601, recall: 0.7647749889429456, f1score_macro: 0.7797015588551117\n",
      " \n",
      "0.47000000000000003\n",
      "precision: [0.81923715 0.81094527], recall: [0.92857143 0.59926471], f1score: [0.87048458 0.68921776]\n",
      "macro:\n",
      "precision: 0.8150912106135987, recall: 0.7639180672268908, f1score_macro: 0.7798511702414991\n",
      " \n",
      "0.47500000000000003\n",
      "precision: [0.81818182 0.81407035], recall: [0.93045113 0.59558824], f1score: [0.8707124  0.68789809]\n",
      "macro:\n",
      "precision: 0.816126084970306, recall: 0.7630196815568333, f1score_macro: 0.7793052451136917\n",
      " \n",
      "0.48000000000000004\n",
      "precision: [0.81414474 0.81122449], recall: [0.93045113 0.58455882], f1score: [0.86842105 0.67948718]\n",
      "macro:\n",
      "precision: 0.8126846133190118, recall: 0.7575049756744803, f1score_macro: 0.7739541160593792\n",
      " \n",
      "0.485\n",
      "precision: [0.81178396 0.8134715 ], recall: [0.93233083 0.57720588], f1score: [0.86789151 0.67526882]\n",
      "macro:\n",
      "precision: 0.8126277316554023, recall: 0.7547683547103051, f1score_macro: 0.7715801653825529\n",
      " \n",
      "0.49\n",
      "precision: [0.8120915  0.81770833], recall: [0.93421053 0.57720588], f1score: [0.86888112 0.67672414]\n",
      "macro:\n",
      "precision: 0.8148999183006536, recall: 0.7557082043343653, f1score_macro: 0.7728026284060767\n",
      " \n",
      "0.495\n",
      "precision: [0.8097561  0.82010582], recall: [0.93609023 0.56985294], f1score: [0.86835222 0.67245119]\n",
      "macro:\n",
      "precision: 0.814930958833398, recall: 0.7529715833701902, f1score_macro: 0.7704017081247506\n",
      " \n",
      "0.5\n",
      "precision: [0.8022508  0.81868132], recall: [0.93796992 0.54779412], f1score: [0.86481802 0.65638767]\n",
      "macro:\n",
      "precision: 0.8104660612699197, recall: 0.7428820212295445, f1score_macro: 0.7606028447308347\n",
      " \n",
      "0.505\n",
      "precision: [0.8022508  0.81868132], recall: [0.93796992 0.54779412], f1score: [0.86481802 0.65638767]\n",
      "macro:\n",
      "precision: 0.8104660612699197, recall: 0.7428820212295445, f1score_macro: 0.7606028447308347\n",
      " \n",
      "0.51\n",
      "precision: [0.8        0.82122905], recall: [0.93984962 0.54044118], f1score: [0.86430424 0.6518847 ]\n",
      "macro:\n",
      "precision: 0.8106145251396648, recall: 0.7401454002653693, f1score_macro: 0.7580944678779702\n",
      " \n",
      "0.515\n",
      "precision: [0.79936306 0.82954545], recall: [0.94360902 0.53676471], f1score: [0.86551724 0.65178571]\n",
      "macro:\n",
      "precision: 0.8144542559351478, recall: 0.740186864219372, f1score_macro: 0.7586514778325123\n",
      " \n",
      "0.52\n",
      "precision: [0.7968254  0.82758621], recall: [0.94360902 0.52941176], f1score: [0.86402754 0.64573991]\n",
      "macro:\n",
      "precision: 0.8122058018609742, recall: 0.7365103936311367, f1score_macro: 0.7548837245201176\n",
      " \n",
      "0.525\n",
      "precision: [0.79620853 0.83625731], recall: [0.94736842 0.52573529], f1score: [0.86523605 0.64559819]\n",
      "macro:\n",
      "precision: 0.8162329203736038, recall: 0.7365518575851393, f1score_macro: 0.7554171228165356\n",
      " \n",
      "0.53\n",
      "precision: [0.79499218 0.85454545], recall: [0.95488722 0.51838235], f1score: [0.8676345  0.64530892]\n",
      "macro:\n",
      "precision: 0.8247688149096599, recall: 0.7366347854931447, f1score_macro: 0.7564717124560556\n",
      " \n",
      "0.535\n",
      "precision: [0.79499218 0.85454545], recall: [0.95488722 0.51838235], f1score: [0.8676345  0.64530892]\n",
      "macro:\n",
      "precision: 0.8247688149096599, recall: 0.7366347854931447, f1score_macro: 0.7564717124560556\n",
      " \n",
      "0.54\n",
      "precision: [0.79499218 0.85454545], recall: [0.95488722 0.51838235], f1score: [0.8676345  0.64530892]\n",
      "macro:\n",
      "precision: 0.8247688149096599, recall: 0.7366347854931447, f1score_macro: 0.7564717124560556\n",
      " \n",
      "0.545\n",
      "precision: [0.7925117  0.85276074], recall: [0.95488722 0.51102941], f1score: [0.86615516 0.63908046]\n",
      "macro:\n",
      "precision: 0.8226362183321689, recall: 0.7329583149049093, f1score_macro: 0.7526178087426875\n",
      " \n",
      "0.55\n",
      "precision: [0.7925117  0.85276074], recall: [0.95488722 0.51102941], f1score: [0.86615516 0.63908046]\n",
      "macro:\n",
      "precision: 0.8226362183321689, recall: 0.7329583149049093, f1score_macro: 0.7526178087426875\n",
      " \n",
      "0.555\n",
      "precision: [0.79192547 0.8625    ], recall: [0.95864662 0.50735294], f1score: [0.86734694 0.63888889]\n",
      "macro:\n",
      "precision: 0.8272127329192547, recall: 0.732999778858912, f1score_macro: 0.7531179138321995\n",
      " \n",
      "0.56\n",
      "precision: [0.78947368 0.86075949], recall: [0.95864662 0.5       ], f1score: [0.86587436 0.63255814]\n",
      "macro:\n",
      "precision: 0.8251165889407062, recall: 0.7293233082706767, f1score_macro: 0.7492162514312788\n",
      " \n",
      "0.5650000000000001\n",
      "precision: [0.78825348 0.85987261], recall: [0.95864662 0.49632353], f1score: [0.86513995 0.62937063]\n",
      "macro:\n",
      "precision: 0.8240630445269199, recall: 0.7274850729765591, f1score_macro: 0.7472552892400222\n",
      " \n",
      "0.5700000000000001\n",
      "precision: [0.78703704 0.85897436], recall: [0.95864662 0.49264706], f1score: [0.86440678 0.62616822]\n",
      "macro:\n",
      "precision: 0.823005698005698, recall: 0.7256468376824414, f1score_macro: 0.7452875019800411\n",
      " \n",
      "0.5750000000000001\n",
      "precision: [0.78769231 0.87012987], recall: [0.96240602 0.49264706], f1score: [0.86632826 0.62910798]\n",
      "macro:\n",
      "precision: 0.8289110889110889, recall: 0.7275265369305617, f1score_macro: 0.7477181192059293\n",
      " \n",
      "0.5800000000000001\n",
      "precision: [0.78769231 0.87012987], recall: [0.96240602 0.49264706], f1score: [0.86632826 0.62910798]\n",
      "macro:\n",
      "precision: 0.8289110889110889, recall: 0.7275265369305617, f1score_macro: 0.7477181192059293\n",
      " \n",
      "0.585\n",
      "precision: [0.78713629 0.8807947 ], recall: [0.96616541 0.48897059], f1score: [0.86751055 0.62884161]\n",
      "macro:\n",
      "precision: 0.83396549800716, recall: 0.7275680008845643, f1score_macro: 0.7481760780441093\n",
      " \n",
      "0.59\n",
      "precision: [0.78506098 0.88513514], recall: [0.96804511 0.48161765], f1score: [0.86700337 0.62380952]\n",
      "macro:\n",
      "precision: 0.8350980553724456, recall: 0.7248313799203893, f1score_macro: 0.7454064454064453\n",
      " \n",
      "0.595\n",
      "precision: [0.78300455 0.88965517], recall: [0.96992481 0.47426471], f1score: [0.86649874 0.61870504]\n",
      "macro:\n",
      "precision: 0.8363298623829208, recall: 0.7220947589562141, f1score_macro: 0.7426018882626896\n",
      " \n",
      "0.6\n",
      "precision: [0.77828054 0.88652482], recall: [0.96992481 0.45955882], f1score: [0.86359833 0.60532688]\n",
      "macro:\n",
      "precision: 0.8324026828407304, recall: 0.7147418177797434, f1score_macro: 0.7344626014365749\n",
      " \n",
      "0.605\n",
      "precision: [0.77511244 0.89051095], recall: [0.97180451 0.44852941], f1score: [0.86238532 0.59657702]\n",
      "macro:\n",
      "precision: 0.8328116963416102, recall: 0.7101669615214506, f1score_macro: 0.7294811691079159\n",
      " \n",
      "0.61\n",
      "precision: [0.77151335 0.90769231], recall: [0.97744361 0.43382353], f1score: [0.86235489 0.58706468]\n",
      "macro:\n",
      "precision: 0.8396028304040173, recall: 0.7056335692171605, f1score_macro: 0.7247097844112769\n",
      " \n",
      "0.615\n",
      "precision: [0.77037037 0.90697674], recall: [0.97744361 0.43014706], f1score: [0.86164043 0.58354115]\n",
      "macro:\n",
      "precision: 0.8386735572782085, recall: 0.7037953339230429, f1score_macro: 0.7225907889761926\n",
      " \n",
      "0.62\n",
      "precision: [0.77037037 0.90697674], recall: [0.97744361 0.43014706], f1score: [0.86164043 0.58354115]\n",
      "macro:\n",
      "precision: 0.8386735572782085, recall: 0.7037953339230429, f1score_macro: 0.7225907889761926\n",
      " \n",
      "0.625\n",
      "precision: [0.76957164 0.91338583], recall: [0.97932331 0.42647059], f1score: [0.86186931 0.58145363]\n",
      "macro:\n",
      "precision: 0.8414787331790321, recall: 0.7028969482529854, f1score_macro: 0.7216614737837148\n",
      " \n",
      "0.63\n",
      "precision: [0.76392962 0.90983607], recall: [0.97932331 0.40808824], f1score: [0.8583196  0.56345178]\n",
      "macro:\n",
      "precision: 0.8368828421710495, recall: 0.6937057717823971, f1score_macro: 0.7108856906312981\n",
      " \n",
      "0.635\n",
      "precision: [0.76281113 0.90909091], recall: [0.97932331 0.40441176], f1score: [0.85761317 0.55979644]\n",
      "macro:\n",
      "precision: 0.8359510182350592, recall: 0.6918675364882796, f1score_macro: 0.7087048031916565\n",
      " \n",
      "0.64\n",
      "precision: [0.76169591 0.90833333], recall: [0.97932331 0.40073529], f1score: [0.85690789 0.55612245]\n",
      "macro:\n",
      "precision: 0.8350146198830409, recall: 0.6900293011941618, f1score_macro: 0.706515171858217\n",
      " \n",
      "0.645\n",
      "precision: [0.75947522 0.90677966], recall: [0.97932331 0.39338235], f1score: [0.85550082 0.54871795]\n",
      "macro:\n",
      "precision: 0.8331274398379207, recall: 0.6863528306059266, f1score_macro: 0.7021093848680056\n",
      " \n",
      "0.65\n",
      "precision: [0.75982533 0.91452991], recall: [0.98120301 0.39338235], f1score: [0.8564397  0.55012853]\n",
      "macro:\n",
      "precision: 0.8371776210204158, recall: 0.6872926802299867, f1score_macro: 0.703284119690167\n",
      " \n",
      "0.655\n",
      "precision: [0.75872093 0.9137931 ], recall: [0.98120301 0.38970588], f1score: [0.8557377  0.54639175]\n",
      "macro:\n",
      "precision: 0.8362570168404171, recall: 0.685454444935869, f1score_macro: 0.7010647287476762\n",
      " \n",
      "0.66\n",
      "precision: [0.75872093 0.9137931 ], recall: [0.98120301 0.38970588], f1score: [0.8557377  0.54639175]\n",
      "macro:\n",
      "precision: 0.8362570168404171, recall: 0.685454444935869, f1score_macro: 0.7010647287476762\n",
      " \n",
      "0.665\n",
      "precision: [0.75907112 0.92173913], recall: [0.98308271 0.38970588], f1score: [0.85667486 0.54780362]\n",
      "macro:\n",
      "precision: 0.8404051239982331, recall: 0.6863942945599293, f1score_macro: 0.702239237122958\n",
      " \n",
      "0.67\n",
      "precision: [0.75360231 0.91818182], recall: [0.98308271 0.37132353], f1score: [0.85318108 0.52879581]\n",
      "macro:\n",
      "precision: 0.8358920618286613, recall: 0.677203118089341, f1score_macro: 0.6909884440952144\n",
      " \n",
      "0.675\n",
      "precision: [0.75395683 0.9266055 ], recall: [0.98496241 0.37132353], f1score: [0.85411573 0.53018373]\n",
      "macro:\n",
      "precision: 0.840281169559765, recall: 0.6781429677134012, f1score_macro: 0.6921497282277369\n",
      " \n",
      "0.68\n",
      "precision: [0.7517934  0.92523364], recall: [0.98496241 0.36397059], f1score: [0.85272579 0.52242744]\n",
      "macro:\n",
      "precision: 0.8385135225733786, recall: 0.6744664971251658, f1score_macro: 0.687576616980577\n",
      " \n",
      "0.685\n",
      "precision: [0.74964235 0.92380952], recall: [0.98496241 0.35661765], f1score: [0.85134037 0.51458886]\n",
      "macro:\n",
      "precision: 0.8367259350091969, recall: 0.6707900265369305, f1score_macro: 0.6829646165481903\n",
      " \n",
      "0.6900000000000001\n",
      "precision: [0.75       0.93269231], recall: [0.98684211 0.35661765], f1score: [0.85227273 0.51595745]\n",
      "macro:\n",
      "precision: 0.8413461538461539, recall: 0.6717298761609907, f1score_macro: 0.684115087040619\n",
      " \n",
      "0.6950000000000001\n",
      "precision: [0.74928775 0.94117647], recall: [0.9887218  0.35294118], f1score: [0.85251216 0.51336898]\n",
      "macro:\n",
      "precision: 0.8452321099379922, recall: 0.6708314904909333, f1score_macro: 0.6829405697743958\n",
      " \n",
      "0.7000000000000001\n",
      "precision: [0.74715909 0.94      ], recall: [0.9887218  0.34558824], f1score: [0.85113269 0.50537634]\n",
      "macro:\n",
      "precision: 0.8435795454545454, recall: 0.6671550199026979, f1score_macro: 0.6782545150850819\n",
      " \n",
      "0.7050000000000001\n",
      "precision: [0.74715909 0.94      ], recall: [0.9887218  0.34558824], f1score: [0.85113269 0.50537634]\n",
      "macro:\n",
      "precision: 0.8435795454545454, recall: 0.6671550199026979, f1score_macro: 0.6782545150850819\n",
      " \n",
      "0.71\n",
      "precision: [0.74715909 0.94      ], recall: [0.9887218  0.34558824], f1score: [0.85113269 0.50537634]\n",
      "macro:\n",
      "precision: 0.8435795454545454, recall: 0.6671550199026979, f1score_macro: 0.6782545150850819\n",
      " \n",
      "0.715\n",
      "precision: [0.74504249 0.93877551], recall: [0.9887218  0.33823529], f1score: [0.84975767 0.4972973 ]\n",
      "macro:\n",
      "precision: 0.8419090015609644, recall: 0.6634785493144626, f1score_macro: 0.6735274854822513\n",
      " \n",
      "0.72\n",
      "precision: [0.74398868 0.93814433], recall: [0.9887218  0.33455882], f1score: [0.84907183 0.49322493]\n",
      "macro:\n",
      "precision: 0.8410665072398256, recall: 0.661640314020345, f1score_macro: 0.6711483821860011\n",
      " \n",
      "0.725\n",
      "precision: [0.74330042 0.94736842], recall: [0.9906015  0.33088235], f1score: [0.84931507 0.49046322]\n",
      "macro:\n",
      "precision: 0.845334422091901, recall: 0.6607419283502874, f1score_macro: 0.6698891418760031\n",
      " \n",
      "0.73\n",
      "precision: [0.74016854 0.94565217], recall: [0.9906015  0.31985294], f1score: [0.84726688 0.47802198]\n",
      "macro:\n",
      "precision: 0.8429103566194431, recall: 0.6552272224679345, f1score_macro: 0.6626444295254584\n",
      " \n",
      "0.735\n",
      "precision: [0.73603352 0.94318182], recall: [0.9906015  0.30514706], f1score: [0.84455128 0.46111111]\n",
      "macro:\n",
      "precision: 0.8396076688674454, recall: 0.647874281291464, f1score_macro: 0.6528311965811966\n",
      " \n",
      "0.74\n",
      "precision: [0.73194444 0.94047619], recall: [0.9906015  0.29044118], f1score: [0.84185304 0.44382022]\n",
      "macro:\n",
      "precision: 0.8362103174603175, recall: 0.6405213401149934, f1score_macro: 0.6428366299314355\n",
      " \n",
      "0.745\n",
      "precision: [0.72589532 0.93589744], recall: [0.9906015  0.26838235], f1score: [0.83783784 0.41714286]\n",
      "macro:\n",
      "precision: 0.8308963763509218, recall: 0.6294919283502874, f1score_macro: 0.6274903474903475\n",
      " \n",
      "0.75\n",
      "precision: [0.72489684 0.93506494], recall: [0.9906015  0.26470588], f1score: [0.83717236 0.41260745]\n",
      "macro:\n",
      "precision: 0.8299808856892763, recall: 0.6276536930561698, f1score_macro: 0.6248899044359124\n",
      " \n",
      "0.755\n",
      "precision: [0.72191781 0.93243243], recall: [0.9906015  0.25367647], f1score: [0.83518225 0.39884393]\n",
      "macro:\n",
      "precision: 0.8271751203258053, recall: 0.6221389871738169, f1score_macro: 0.6170130905160174\n",
      " \n",
      "0.76\n",
      "precision: [0.7226776  0.95833333], recall: [0.9943609  0.25367647], f1score: [0.83702532 0.40116279]\n",
      "macro:\n",
      "precision: 0.8405054644808743, recall: 0.6240186864219373, f1score_macro: 0.6190940535766853\n",
      " \n",
      "0.765\n",
      "precision: [0.72070845 0.95714286], recall: [0.9943609  0.24632353], f1score: [0.835703   0.39181287]\n",
      "macro:\n",
      "precision: 0.8389256520046711, recall: 0.620342215833702, f1score_macro: 0.6137579335384273\n",
      " \n",
      "0.77\n",
      "precision: [0.71875    0.95588235], recall: [0.9943609  0.23897059], f1score: [0.83438486 0.38235294]\n",
      "macro:\n",
      "precision: 0.8373161764705883, recall: 0.6166657452454666, f1score_macro: 0.6083688996103174\n",
      " \n",
      "0.775\n",
      "precision: [0.71486486 0.953125  ], recall: [0.9943609  0.22426471], f1score: [0.83176101 0.36309524]\n",
      "macro:\n",
      "precision: 0.8339949324324325, recall: 0.609312804068996, f1score_macro: 0.5974281221922731\n",
      " \n",
      "0.78\n",
      "precision: [0.71486486 0.953125  ], recall: [0.9943609  0.22426471], f1score: [0.83176101 0.36309524]\n",
      "macro:\n",
      "precision: 0.8339949324324325, recall: 0.609312804068996, f1score_macro: 0.5974281221922731\n",
      " \n",
      "0.785\n",
      "precision: [0.71293801 0.9516129 ], recall: [0.9943609  0.21691176], f1score: [0.83045526 0.35329341]\n",
      "macro:\n",
      "precision: 0.8322754543083211, recall: 0.6056363334807607, f1score_macro: 0.5918743361001702\n",
      " \n",
      "0.79\n",
      "precision: [0.71197847 0.95081967], recall: [0.9943609  0.21323529], f1score: [0.82980392 0.34834835]\n",
      "macro:\n",
      "precision: 0.8313990689054123, recall: 0.6037980981866431, f1score_macro: 0.5890761349584879\n",
      " \n",
      "0.795\n",
      "precision: [0.71197847 0.95081967], recall: [0.9943609  0.21323529], f1score: [0.82980392 0.34834835]\n",
      "macro:\n",
      "precision: 0.8313990689054123, recall: 0.6037980981866431, f1score_macro: 0.5890761349584879\n",
      " \n",
      "0.8\n",
      "precision: [0.70911528 0.94827586], recall: [0.9943609  0.20220588], f1score: [0.82785603 0.33333333]\n",
      "macro:\n",
      "precision: 0.828695571785153, recall: 0.5982833923042902, f1score_macro: 0.5805946791862285\n",
      " \n",
      "0.805\n",
      "precision: [0.70666667 0.96296296], recall: [0.9962406  0.19117647], f1score: [0.82683307 0.3190184 ]\n",
      "macro:\n",
      "precision: 0.8348148148148148, recall: 0.5937085360459973, f1score_macro: 0.5729257391154541\n",
      " \n",
      "0.81\n",
      "precision: [0.7057257  0.96226415], recall: [0.9962406 0.1875   ], f1score: [0.82618862 0.31384615]\n",
      "macro:\n",
      "precision: 0.8339949250056529, recall: 0.5918703007518797, f1score_macro: 0.5700173871335211\n",
      " \n",
      "0.8150000000000001\n",
      "precision: [0.70517928 0.98039216], recall: [0.9981203  0.18382353], f1score: [0.82645914 0.30959752]\n",
      "macro:\n",
      "precision: 0.8427857198656354, recall: 0.5909719150818222, f1score_macro: 0.568028333594343\n",
      " \n",
      "0.8200000000000001\n",
      "precision: [0.70238095 0.97916667], recall: [0.9981203  0.17279412], f1score: [0.82453416 0.29375   ]\n",
      "macro:\n",
      "precision: 0.8407738095238095, recall: 0.5854572091994693, f1score_macro: 0.5591420807453417\n",
      " \n",
      "0.8250000000000001\n",
      "precision: [0.7005277  0.97826087], recall: [0.9981203  0.16544118], f1score: [0.82325581 0.28301887]\n",
      "macro:\n",
      "precision: 0.8393942870253528, recall: 0.581780738611234, f1score_macro: 0.5531373409390083\n",
      " \n",
      "0.8300000000000001\n",
      "precision: [0.69868421 0.97727273], recall: [0.9981203  0.15808824], f1score: [0.82198142 0.2721519 ]\n",
      "macro:\n",
      "precision: 0.8379784688995215, recall: 0.5781042680229986, f1score_macro: 0.547066661441392\n",
      " \n",
      "0.8350000000000001\n",
      "precision: [0.6977661  0.97674419], recall: [0.9981203  0.15441176], f1score: [0.82134571 0.26666667]\n",
      "macro:\n",
      "precision: 0.8372551416434924, recall: 0.5762660327288811, f1score_macro: 0.5440061871616397\n",
      " \n",
      "0.84\n",
      "precision: [0.69593709 0.97560976], recall: [0.9981203  0.14705882], f1score: [0.82007722 0.25559105]\n",
      "macro:\n",
      "precision: 0.8357734232650321, recall: 0.5725895621406457, f1score_macro: 0.5378341371951596\n",
      " \n",
      "0.845\n",
      "precision: [0.69411765 0.97435897], recall: [0.9981203  0.13970588], f1score: [0.81881264 0.24437299]\n",
      "macro:\n",
      "precision: 0.8342383107088989, recall: 0.5689130915524104, f1score_macro: 0.5315928174590385\n",
      " \n",
      "0.85\n",
      "precision: [0.69321149 0.97368421], recall: [0.9981203  0.13602941], f1score: [0.81818182 0.23870968]\n",
      "macro:\n",
      "precision: 0.8334478493884843, recall: 0.5670748562582928, f1score_macro: 0.5284457478005865\n",
      " \n",
      "0.855\n",
      "precision: [0.69230769 0.97297297], recall: [0.9981203  0.13235294], f1score: [0.81755196 0.23300971]\n",
      "macro:\n",
      "precision: 0.8326403326403327, recall: 0.5652366209641752, f1score_macro: 0.5252808358931815\n",
      " \n",
      "0.86\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: [0.69050715 0.97142857], recall: [0.9981203 0.125    ], f1score: [0.81629516 0.22149837]\n",
      "macro:\n",
      "precision: 0.8309678617871076, recall: 0.5615601503759399, f1score_macro: 0.518896764453302\n",
      " \n",
      "0.865\n",
      "precision: [0.68961039 0.97058824], recall: [0.9981203  0.12132353], f1score: [0.8156682  0.21568627]\n",
      "macro:\n",
      "precision: 0.8300993124522535, recall: 0.5597219150818222, f1score_macro: 0.5156772386373905\n",
      " \n",
      "0.87\n",
      "precision: [0.68871595 0.96969697], recall: [0.9981203  0.11764706], f1score: [0.81504221 0.20983607]\n",
      "macro:\n",
      "precision: 0.8292064615021814, recall: 0.5578836797877046, f1score_macro: 0.5124391379288653\n",
      " \n",
      "0.875\n",
      "precision: [0.68516129 0.96551724], recall: [0.9981203  0.10294118], f1score: [0.81254782 0.18604651]\n",
      "macro:\n",
      "precision: 0.8253392658509455, recall: 0.550530738611234, f1score_macro: 0.49929716553086245\n",
      " \n",
      "0.88\n",
      "precision: [0.68339768 0.96296296], recall: [0.9981203  0.09558824], f1score: [0.81130634 0.17391304]\n",
      "macro:\n",
      "precision: 0.8231803231803232, recall: 0.5468542680229986, f1score_macro: 0.49260969209818317\n",
      " \n",
      "0.885\n",
      "precision: [0.68339768 0.96296296], recall: [0.9981203  0.09558824], f1score: [0.81130634 0.17391304]\n",
      "macro:\n",
      "precision: 0.8231803231803232, recall: 0.5468542680229986, f1score_macro: 0.49260969209818317\n",
      " \n",
      "0.89\n",
      "precision: [0.67989757 0.95652174], recall: [0.9981203  0.08088235], f1score: [0.80883473 0.14915254]\n",
      "macro:\n",
      "precision: 0.8182096531759728, recall: 0.5395013268465281, f1score_macro: 0.4789936359998451\n",
      " \n",
      "0.895\n",
      "precision: [0.67902813 0.95454545], recall: [0.9981203  0.07720588], f1score: [0.80821918 0.14285714]\n",
      "macro:\n",
      "precision: 0.816786793768891, recall: 0.5376630915524104, f1score_macro: 0.47553816046966735\n",
      " \n",
      "0.9\n",
      "precision: [0.67729592 0.95      ], recall: [0.9981203  0.06985294], f1score: [0.80699088 0.13013699]\n",
      "macro:\n",
      "precision: 0.8136479591836734, recall: 0.5339866209641752, f1score_macro: 0.46856393388016826\n",
      " \n",
      "macro 0.7908440134983945\n",
      "F1-valued: 0.7099391480730223\n",
      "recall-valued: 0.6433823529411765\n",
      "0.44\n"
     ]
    }
   ],
   "source": [
    "predictions0=[]\n",
    "predictions1=[]\n",
    "\n",
    "for res in y_predict:\n",
    "    predictions0.append(res[0])\n",
    "    predictions1.append(res[1])\n",
    "    \n",
    "test_data['predict_0']=predictions0\n",
    "test_data['predict_1']=predictions1\n",
    "\n",
    "test_data_predict=TestCutoff(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b40ae6db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: [0.83361921 0.7918552 ], recall: [0.91353383 0.64338235], f1score: [0.87174888 0.70993915]\n"
     ]
    }
   ],
   "source": [
    "precision, recall, f1score = precision_recall_fscore_support(labels_test, test_data_predict['predict'])[:3]\n",
    "\n",
    "print(f'precision: {precision}, recall: {recall}, f1score: {f1score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "154707f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.8127372072988055, recall: 0.7784580937638214, f1score_macro: 0.7908440134983945\n"
     ]
    }
   ],
   "source": [
    "precision, recall, f1score = precision_recall_fscore_support(labels_test, test_data_predict['predict'], average='macro')[:3]\n",
    "\n",
    "print(f'precision: {precision}, recall: {recall}, f1score_macro: {f1score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5d54aa",
   "metadata": {},
   "source": [
    "# Logit Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "90e556d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [('LogitBoost', LogitBoost())]\n",
    "from sklearn.pipeline import Pipeline\n",
    "pipeline = Pipeline(steps) # define the pipeline object.\n",
    "\n",
    "parameteres = {'LogitBoost__n_estimators':range(10,100,10)}\n",
    "\n",
    "grid = GridSearchCV(pipeline, param_grid=parameteres, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4c981cd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5, estimator=Pipeline(steps=[(&#x27;LogitBoost&#x27;, LogitBoost())]),\n",
       "             param_grid={&#x27;LogitBoost__n_estimators&#x27;: range(10, 100, 10)})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" ><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5, estimator=Pipeline(steps=[(&#x27;LogitBoost&#x27;, LogitBoost())]),\n",
       "             param_grid={&#x27;LogitBoost__n_estimators&#x27;: range(10, 100, 10)})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" ><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;LogitBoost&#x27;, LogitBoost())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" ><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogitBoost</label><div class=\"sk-toggleable__content\"><pre>LogitBoost()</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5, estimator=Pipeline(steps=[('LogitBoost', LogitBoost())]),\n",
       "             param_grid={'LogitBoost__n_estimators': range(10, 100, 10)})"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(df_emb_, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fadbc21e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters from gridsearch: {'LogitBoost__n_estimators': 70}\n",
      "CV score=0.715\n",
      "{'mean_fit_time': array([ 2.79942312,  5.40478673,  8.11945777, 10.54189849, 13.23489637,\n",
      "       15.53023572, 18.09441257, 20.96763592, 23.50913448]), 'std_fit_time': array([0.03142028, 0.03856336, 0.10592107, 0.12890112, 0.20902708,\n",
      "       0.15737183, 0.22910931, 0.28103298, 0.63385288]), 'mean_score_time': array([0.04417458, 0.08697071, 0.13355494, 0.17791805, 0.22635398,\n",
      "       0.26530986, 0.32098236, 0.36329894, 0.38717256]), 'std_score_time': array([0.00176943, 0.00594447, 0.01050529, 0.00997894, 0.01408817,\n",
      "       0.00639642, 0.03723074, 0.02172834, 0.01079422]), 'param_LogitBoost__n_estimators': masked_array(data=[10, 20, 30, 40, 50, 60, 70, 80, 90],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'LogitBoost__n_estimators': 10}, {'LogitBoost__n_estimators': 20}, {'LogitBoost__n_estimators': 30}, {'LogitBoost__n_estimators': 40}, {'LogitBoost__n_estimators': 50}, {'LogitBoost__n_estimators': 60}, {'LogitBoost__n_estimators': 70}, {'LogitBoost__n_estimators': 80}, {'LogitBoost__n_estimators': 90}], 'split0_test_score': array([0.75769613, 0.76663357, 0.76961271, 0.76464747, 0.76861966,\n",
      "       0.76861966, 0.77358491, 0.77358491, 0.77259186]), 'split1_test_score': array([0.61966236, 0.61072493, 0.60774578, 0.60079444, 0.60675273,\n",
      "       0.60774578, 0.61569017, 0.61171797, 0.60278054]), 'split2_test_score': array([0.71201589, 0.72691162, 0.72989076, 0.73584906, 0.72691162,\n",
      "       0.72591857, 0.72889772, 0.73684211, 0.7428004 ]), 'split3_test_score': array([0.74478649, 0.75173784, 0.77060576, 0.7775571 , 0.77457795,\n",
      "       0.77358491, 0.7775571 , 0.7775571 , 0.77259186]), 'split4_test_score': array([0.65541212, 0.67428004, 0.67626614, 0.67825223, 0.68619662,\n",
      "       0.67626614, 0.67825223, 0.67328699, 0.67626614]), 'mean_test_score': array([0.6979146 , 0.7060576 , 0.71082423, 0.71142006, 0.71261172,\n",
      "       0.71042701, 0.71479643, 0.71459782, 0.71340616]), 'std_test_score': array([0.05272481, 0.05707741, 0.06198851, 0.06502642, 0.06178199,\n",
      "       0.06217024, 0.06123947, 0.06360661, 0.06555505]), 'rank_test_score': array([9, 8, 6, 5, 4, 7, 1, 2, 3], dtype=int32)}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best parameters from gridsearch: {}\".format(grid.best_params_))\n",
    "print(\"CV score=%0.3f\" % grid.best_score_)\n",
    "cv_results = grid.cv_results_\n",
    "print(cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4ea5a0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = grid.predict_proba(df_emb_test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0a3bb51a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.005\n",
      "precision: [0.96      0.3478819], recall: [0.04511278 0.99632353], f1score: [0.08617594 0.51569933]\n",
      "macro:\n",
      "precision: 0.6539409499358151, recall: 0.520718155683326, f1score_macro: 0.3009376382585108\n",
      " \n",
      "0.01\n",
      "precision: [0.93548387 0.36118598], recall: [0.10902256 0.98529412], f1score: [0.1952862  0.52859961]\n",
      "macro:\n",
      "precision: 0.6483349273976176, recall: 0.5471583370190182, f1score_macro: 0.3619429004044389\n",
      " \n",
      "0.015\n",
      "precision: [0.93617021 0.37464789], recall: [0.16541353 0.97794118], f1score: [0.28115016 0.54175153]\n",
      "macro:\n",
      "precision: 0.6554090500449505, recall: 0.5716773551525873, f1score_macro: 0.4114508436196586\n",
      " \n",
      "0.02\n",
      "precision: [0.94642857 0.38439306], recall: [0.19924812 0.97794118], f1score: [0.32919255 0.55186722]\n",
      "macro:\n",
      "precision: 0.6654108175061932, recall: 0.58859464838567, f1score_macro: 0.44052988325043174\n",
      " \n",
      "0.025\n",
      "precision: [0.94244604 0.39699248], recall: [0.2462406  0.97058824], f1score: [0.390462   0.56350053]\n",
      "macro:\n",
      "precision: 0.6697192621842376, recall: 0.6084144183989385, f1score_macro: 0.4769812653186518\n",
      " \n",
      "0.030000000000000002\n",
      "precision: [0.94871795 0.40740741], recall: [0.27819549 0.97058824], f1score: [0.43023256 0.57391304]\n",
      "macro:\n",
      "precision: 0.6780626780626781, recall: 0.624391862007961, f1score_macro: 0.5020728008088979\n",
      " \n",
      "0.034999999999999996\n",
      "precision: [0.95266272 0.41574803], recall: [0.30263158 0.97058824], f1score: [0.45934379 0.58213892]\n",
      "macro:\n",
      "precision: 0.6842053766947771, recall: 0.6366099071207431, f1score_macro: 0.5207413570470285\n",
      " \n",
      "0.04\n",
      "precision: [0.95767196 0.42926829], recall: [0.34022556 0.97058824], f1score: [0.50208044 0.59526494]\n",
      "macro:\n",
      "precision: 0.6934701251774422, recall: 0.655406899601946, f1score_macro: 0.5486726909106261\n",
      " \n",
      "0.045\n",
      "precision: [0.94258373 0.43697479], recall: [0.37030075 0.95588235], f1score: [0.5317139  0.59976932]\n",
      "macro:\n",
      "precision: 0.6897792609866913, recall: 0.6630915524104379, f1score_macro: 0.5657416098137278\n",
      " \n",
      "0.049999999999999996\n",
      "precision: [0.94323144 0.45043478], recall: [0.40601504 0.95220588], f1score: [0.56767411 0.61157025]\n",
      "macro:\n",
      "precision: 0.6968331118283653, recall: 0.679110459973463, f1score_macro: 0.5896221804715414\n",
      " \n",
      "0.055\n",
      "precision: [0.94262295 0.46071429], recall: [0.43233083 0.94852941], f1score: [0.59278351 0.62019231]\n",
      "macro:\n",
      "precision: 0.7016686182669789, recall: 0.6904301194161875, f1score_macro: 0.6064879064234734\n",
      " \n",
      "0.06\n",
      "precision: [0.93359375 0.46532847], recall: [0.44924812 0.9375    ], f1score: [0.60659898 0.62195122]\n",
      "macro:\n",
      "precision: 0.6994611085766423, recall: 0.693374060150376, f1score_macro: 0.6142751021418844\n",
      " \n",
      "0.065\n",
      "precision: [0.93436293 0.46788991], recall: [0.45488722 0.9375    ], f1score: [0.61188369 0.62423501]\n",
      "macro:\n",
      "precision: 0.7011264213099075, recall: 0.6961936090225564, f1score_macro: 0.6180593488248302\n",
      " \n",
      "0.07\n",
      "precision: [0.93333333 0.47565543], recall: [0.47368421 0.93382353], f1score: [0.62842893 0.63027295]\n",
      "macro:\n",
      "precision: 0.7044943820224719, recall: 0.7037538699690402, f1score_macro: 0.629350940267198\n",
      " \n",
      "0.07500000000000001\n",
      "precision: [0.92753623 0.47727273], recall: [0.48120301 0.92647059], f1score: [0.63366337 0.63      ]\n",
      "macro:\n",
      "precision: 0.7024044795783926, recall: 0.7038367978770456, f1score_macro: 0.6318316831683168\n",
      " \n",
      "0.08\n",
      "precision: [0.91637631 0.47969052], recall: [0.4943609  0.91176471], f1score: [0.64224664 0.62864385]\n",
      "macro:\n",
      "precision: 0.6980334144319614, recall: 0.703062804068996, f1score_macro: 0.635445247612548\n",
      " \n",
      "0.085\n",
      "precision: [0.91610738 0.48814229], recall: [0.51315789 0.90808824], f1score: [0.65783133 0.63496144]\n",
      "macro:\n",
      "precision: 0.702124837520227, recall: 0.7106230650154799, f1score_macro: 0.6463963824449469\n",
      " \n",
      "0.09000000000000001\n",
      "precision: [0.91089109 0.48902196], recall: [0.51879699 0.90073529], f1score: [0.66107784 0.63389392]\n",
      "macro:\n",
      "precision: 0.6999565225983676, recall: 0.7097661432994251, f1score_macro: 0.6474858820521958\n",
      " \n",
      "0.095\n",
      "precision: [0.91262136 0.49494949], recall: [0.53007519 0.90073529], f1score: [0.6706302  0.63885267]\n",
      "macro:\n",
      "precision: 0.703785427086398, recall: 0.7154052410437859, f1score_macro: 0.6547414374456435\n",
      " \n",
      "0.1\n",
      "precision: [0.90851735 0.49897331], recall: [0.54135338 0.89338235], f1score: [0.67844523 0.64031621]\n",
      "macro:\n",
      "precision: 0.7037453280562771, recall: 0.7173678681999116, f1score_macro: 0.6593807176077877\n",
      " \n",
      "0.10500000000000001\n",
      "precision: [0.90881459 0.50947368], recall: [0.56203008 0.88970588], f1score: [0.69454123 0.64792503]\n",
      "macro:\n",
      "precision: 0.70914413693809, recall: 0.7258679787704555, f1score_macro: 0.6712331322968996\n",
      " \n",
      "0.11\n",
      "precision: [0.91044776 0.51599147], recall: [0.57330827 0.88970588], f1score: [0.70357555 0.65317139]\n",
      "macro:\n",
      "precision: 0.7132196162046909, recall: 0.7315070765148164, f1score_macro: 0.6783734689398503\n",
      " \n",
      "0.115\n",
      "precision: [0.91253644 0.52494577], recall: [0.58834586 0.88970588], f1score: [0.71542857 0.66030014]\n",
      "macro:\n",
      "precision: 0.7187411066068821, recall: 0.7390258735072976, f1score_macro: 0.6878643539271097\n",
      " \n",
      "0.12000000000000001\n",
      "precision: [0.90830946 0.52747253], recall: [0.59586466 0.88235294], f1score: [0.71963678 0.66024759]\n",
      "macro:\n",
      "precision: 0.71789099152996, recall: 0.7391088014153029, f1score_macro: 0.6899421846188916\n",
      " \n",
      "0.125\n",
      "precision: [0.90756303 0.53467562], recall: [0.60902256 0.87867647], f1score: [0.72890889 0.66481224]\n",
      "macro:\n",
      "precision: 0.721119320211306, recall: 0.7438495134896064, f1score_macro: 0.696860562805171\n",
      " \n",
      "0.13\n",
      "precision: [0.90858726 0.53950339], recall: [0.61654135 0.87867647], f1score: [0.73460246 0.66853147]\n",
      "macro:\n",
      "precision: 0.7240453218111216, recall: 0.7476089119858469, f1score_macro: 0.7015669660686459\n",
      " \n",
      "0.135\n",
      "precision: [0.89918256 0.53775744], recall: [0.62030075 0.86397059], f1score: [0.73414905 0.6629055 ]\n",
      "macro:\n",
      "precision: 0.7184699991894201, recall: 0.7421356700574967, f1score_macro: 0.698527277605112\n",
      " \n",
      "0.14\n",
      "precision: [0.89516129 0.53935185], recall: [0.62593985 0.85661765], f1score: [0.73672566 0.66193182]\n",
      "macro:\n",
      "precision: 0.7172565710872163, recall: 0.7412787483414418, f1score_macro: 0.6993287409493161\n",
      " \n",
      "0.14500000000000002\n",
      "precision: [0.89501312 0.54846336], recall: [0.64097744 0.85294118], f1score: [0.74698795 0.6676259 ]\n",
      "macro:\n",
      "precision: 0.7217382401667877, recall: 0.7469593100398053, f1score_macro: 0.7073069255439022\n",
      " \n",
      "0.15\n",
      "precision: [0.89350649 0.55131265], recall: [0.64661654 0.84926471], f1score: [0.75027263 0.66859624]\n",
      "macro:\n",
      "precision: 0.7224095713355856, recall: 0.7479406236178682, f1score_macro: 0.7094344327362081\n",
      " \n",
      "0.155\n",
      "precision: [0.89175258 0.55288462], recall: [0.65037594 0.84558824], f1score: [0.75217391 0.66860465]\n",
      "macro:\n",
      "precision: 0.7223185963521015, recall: 0.7479820875718708, f1score_macro: 0.7103892821031343\n",
      " \n",
      "0.16\n",
      "precision: [0.89312977 0.55961071], recall: [0.65977444 0.84558824], f1score: [0.75891892 0.67349927]\n",
      "macro:\n",
      "precision: 0.7263702382942367, recall: 0.7526813356921715, f1score_macro: 0.7162090934272486\n",
      " \n",
      "0.165\n",
      "precision: [0.8919598  0.56403941], recall: [0.66729323 0.84191176], f1score: [0.76344086 0.67551622]\n",
      "macro:\n",
      "precision: 0.7279996039309851, recall: 0.7546024988942945, f1score_macro: 0.7194785422019221\n",
      " \n",
      "0.17\n",
      "precision: [0.89330025 0.57107232], recall: [0.67669173 0.84191176], f1score: [0.77005348 0.68053492]\n",
      "macro:\n",
      "precision: 0.7321862836704764, recall: 0.7593017470145953, f1score_macro: 0.7252941971061015\n",
      " \n",
      "0.17500000000000002\n",
      "precision: [0.88674699 0.57840617], recall: [0.69172932 0.82720588], f1score: [0.77719113 0.68078669]\n",
      "macro:\n",
      "precision: 0.7325765788088086, recall: 0.7594676028306059, f1score_macro: 0.728988908360984\n",
      " \n",
      "0.18000000000000002\n",
      "precision: [0.88679245 0.58947368], recall: [0.70676692 0.82352941], f1score: [0.78661088 0.68711656]\n",
      "macro:\n",
      "precision: 0.7381330685203575, recall: 0.7651481645289695, f1score_macro: 0.7368637215391329\n",
      " \n",
      "0.185\n",
      "precision: [0.88604651 0.59625668], recall: [0.71616541 0.81985294], f1score: [0.79209979 0.69040248]\n",
      "macro:\n",
      "precision: 0.7411515980599428, recall: 0.7680091773551525, f1score_macro: 0.741251134439989\n",
      " \n",
      "0.19\n",
      "precision: [0.8863109  0.59785523], recall: [0.71804511 0.81985294], f1score: [0.7933541  0.69147287]\n",
      "macro:\n",
      "precision: 0.7420830663772137, recall: 0.7689490269792127, f1score_macro: 0.7424134849911854\n",
      " \n",
      "0.195\n",
      "precision: [0.88657407 0.59946237], recall: [0.71992481 0.81985294], f1score: [0.79460581 0.69254658]\n",
      "macro:\n",
      "precision: 0.743018219832736, recall: 0.7698888766032729, f1score_macro: 0.7435761964897811\n",
      " \n",
      "0.2\n",
      "precision: [0.87899543 0.59836066], recall: [0.72368421 0.80514706], f1score: [0.79381443 0.68652038]\n",
      "macro:\n",
      "precision: 0.7386780447638297, recall: 0.7644156346749227, f1score_macro: 0.7401674045826196\n",
      " \n",
      "0.20500000000000002\n",
      "precision: [0.87556561 0.59944751], recall: [0.72744361 0.79779412], f1score: [0.79466119 0.68454259]\n",
      "macro:\n",
      "precision: 0.7375065623359416, recall: 0.7626188633348077, f1score_macro: 0.7396018888579405\n",
      " \n",
      "0.21000000000000002\n",
      "precision: [0.8758465  0.60110803], recall: [0.72932331 0.79779412], f1score: [0.79589744 0.68562401]\n",
      "macro:\n",
      "precision: 0.7384772671848328, recall: 0.7635587129588678, f1score_macro: 0.7407607242678333\n",
      " \n",
      "0.215\n",
      "precision: [0.87276786 0.60393258], recall: [0.73496241 0.79044118], f1score: [0.79795918 0.68471338]\n",
      "macro:\n",
      "precision: 0.73835022070626, recall: 0.7627017912428129, f1score_macro: 0.7413362797348237\n",
      " \n",
      "0.22\n",
      "precision: [0.8691796  0.60339943], recall: [0.73684211 0.78308824], f1score: [0.79755849 0.6816    ]\n",
      "macro:\n",
      "precision: 0.73628951715734, recall: 0.7599651702786377, f1score_macro: 0.7395792472024414\n",
      " \n",
      "0.225\n",
      "precision: [0.86784141 0.60571429], recall: [0.7406015  0.77941176], f1score: [0.79918864 0.68167203]\n",
      "macro:\n",
      "precision: 0.7367778477029578, recall: 0.7600066342326404, f1score_macro: 0.7404303333485518\n",
      " \n",
      "0.23\n",
      "precision: [0.86462882 0.60693642], recall: [0.7443609  0.77205882], f1score: [0.8        0.67961165]\n",
      "macro:\n",
      "precision: 0.7357826185728349, recall: 0.7582098628925255, f1score_macro: 0.7398058252427184\n",
      " \n",
      "0.23500000000000001\n",
      "precision: [0.86521739 0.61046512], recall: [0.7481203  0.77205882], f1score: [0.80241935 0.68181818]\n",
      "macro:\n",
      "precision: 0.7378412537917087, recall: 0.7600895621406458, f1score_macro: 0.7421187683284458\n",
      " \n",
      "0.24000000000000002\n",
      "precision: [0.86147186 0.60818713], recall: [0.7481203  0.76470588], f1score: [0.80080483 0.67752443]\n",
      "macro:\n",
      "precision: 0.7348294979873927, recall: 0.7564130915524104, f1score_macro: 0.7391646294706349\n",
      " \n",
      "0.245\n",
      "precision: [0.86021505 0.61061947], recall: [0.7518797  0.76102941], f1score: [0.80240722 0.67757774]\n",
      "macro:\n",
      "precision: 0.7354172613949947, recall: 0.756454555506413, f1score_macro: 0.7399924815362617\n",
      " \n",
      "0.25\n",
      "precision: [0.86051502 0.61242604], recall: [0.7537594  0.76102941], f1score: [0.80360721 0.67868852]\n",
      "macro:\n",
      "precision: 0.7364705284810931, recall: 0.7573944051304733, f1score_macro: 0.7411478695095107\n",
      " \n",
      "0.255\n",
      "precision: [0.85897436 0.61309524], recall: [0.7556391  0.75735294], f1score: [0.804      0.67763158]\n",
      "macro:\n",
      "precision: 0.7360347985347986, recall: 0.7564960194604158, f1score_macro: 0.7408157894736842\n",
      " \n",
      "0.26\n",
      "precision: [0.85774947 0.61561562], recall: [0.7593985  0.75367647], f1score: [0.80558325 0.67768595]\n",
      "macro:\n",
      "precision: 0.7366825424150265, recall: 0.7565374834144184, f1score_macro: 0.7416346003312377\n",
      " \n",
      "0.265\n",
      "precision: [0.85654008 0.61818182], recall: [0.76315789 0.75      ], f1score: [0.80715706 0.67774086]\n",
      "macro:\n",
      "precision: 0.7373609512850019, recall: 0.756578947368421, f1score_macro: 0.7424489607207254\n",
      " \n",
      "0.27\n",
      "precision: [0.8559499  0.62461538], recall: [0.77067669 0.74632353], f1score: [0.81107814 0.680067  ]\n",
      "macro:\n",
      "precision: 0.7402826401156255, recall: 0.758500110570544, f1score_macro: 0.7455725710650185\n",
      " \n",
      "0.275\n",
      "precision: [0.84742268 0.62068966], recall: [0.77255639 0.72794118], f1score: [0.80825959 0.67005076]\n",
      "macro:\n",
      "precision: 0.7340561677923925, recall: 0.7502487837240159, f1score_macro: 0.7391551742209843\n",
      " \n",
      "0.28\n",
      "precision: [0.84662577 0.62539683], recall: [0.77819549 0.72426471], f1score: [0.81096964 0.67120954]\n",
      "macro:\n",
      "precision: 0.7360112961339955, recall: 0.7512300973020787, f1score_macro: 0.7410895888221289\n",
      " \n",
      "0.28500000000000003\n",
      "precision: [0.84381339 0.62700965], recall: [0.78195489 0.71691176], f1score: [0.81170732 0.66895369]\n",
      "macro:\n",
      "precision: 0.7354115168630929, recall: 0.7494333259619637, f1score_macro: 0.7403305024473916\n",
      " \n",
      "0.29000000000000004\n",
      "precision: [0.84507042 0.63517915], recall: [0.78947368 0.71691176], f1score: [0.81632653 0.67357513]\n",
      "macro:\n",
      "precision: 0.7401247878148369, recall: 0.7531927244582044, f1score_macro: 0.7449508300729618\n",
      " \n",
      "0.295\n",
      "precision: [0.84262948 0.63907285], recall: [0.79511278 0.70955882], f1score: [0.81818182 0.67247387]\n",
      "macro:\n",
      "precision: 0.7408511648769163, recall: 0.7523358027421495, f1score_macro: 0.7453278428888186\n",
      " \n",
      "0.3\n",
      "precision: [0.8382643  0.63973064], recall: [0.79887218 0.69852941], f1score: [0.81809432 0.66783831]\n",
      "macro:\n",
      "precision: 0.7389974697667006, recall: 0.7487007961079168, f1score_macro: 0.7429663171462353\n",
      " \n",
      "0.305\n",
      "precision: [0.83953033 0.64846416], recall: [0.80639098 0.69852941], f1score: [0.82262704 0.67256637]\n",
      "macro:\n",
      "precision: 0.7439972482517716, recall: 0.7524601946041574, f1score_macro: 0.747596704536777\n",
      " \n",
      "0.31\n",
      "precision: [0.83984375 0.65068493], recall: [0.80827068 0.69852941], f1score: [0.82375479 0.67375887]\n",
      "macro:\n",
      "precision: 0.7452643407534247, recall: 0.7534000442282176, f1score_macro: 0.7487568272601288\n",
      " \n",
      "0.315\n",
      "precision: [0.84108527 0.65972222], recall: [0.81578947 0.69852941], f1score: [0.82824427 0.67857143]\n",
      "macro:\n",
      "precision: 0.7504037467700259, recall: 0.7571594427244581, f1score_macro: 0.7534078516902945\n",
      " \n",
      "0.32\n",
      "precision: [0.84139265 0.66202091], recall: [0.81766917 0.69852941], f1score: [0.8293613  0.67978533]\n",
      "macro:\n",
      "precision: 0.7517067779133166, recall: 0.7580992923485184, f1score_macro: 0.7545733137104764\n",
      " \n",
      "0.325\n",
      "precision: [0.84200385 0.66666667], recall: [0.82142857 0.69852941], f1score: [0.83158896 0.68222621]\n",
      "macro:\n",
      "precision: 0.7543352601156069, recall: 0.7599789915966386, f1score_macro: 0.7569075873708376\n",
      " \n",
      "0.33\n",
      "precision: [0.83908046 0.66666667], recall: [0.82330827 0.69117647], f1score: [0.83111954 0.67870036]\n",
      "macro:\n",
      "precision: 0.7528735632183907, recall: 0.7572423706324636, f1score_macro: 0.7549099528014303\n",
      " \n",
      "0.335\n",
      "precision: [0.8374761  0.66548043], recall: [0.82330827 0.6875    ], f1score: [0.83033175 0.67631103]\n",
      "macro:\n",
      "precision: 0.7514782632363248, recall: 0.7554041353383458, f1score_macro: 0.7533213921479565\n",
      " \n",
      "0.34\n",
      "precision: [0.83778626 0.66785714], recall: [0.82518797 0.6875    ], f1score: [0.83143939 0.67753623]\n",
      "macro:\n",
      "precision: 0.7528217011995637, recall: 0.7563439849624061, f1score_macro: 0.754487812911726\n",
      " \n",
      "0.34500000000000003\n",
      "precision: [0.83773585 0.67883212], recall: [0.83458647 0.68382353], f1score: [0.83615819 0.68131868]\n",
      "macro:\n",
      "precision: 0.7582839829224625, recall: 0.7592049977885891, f1score_macro: 0.7587384367045384\n",
      " \n",
      "0.35000000000000003\n",
      "precision: [0.83302064 0.67527675], recall: [0.83458647 0.67279412], f1score: [0.83380282 0.67403315]\n",
      "macro:\n",
      "precision: 0.7541486953331071, recall: 0.7536902919062363, f1score_macro: 0.7539179830363396\n",
      " \n",
      "0.35500000000000004\n",
      "precision: [0.83395522 0.68283582], recall: [0.84022556 0.67279412], f1score: [0.83707865 0.67777778]\n",
      "macro:\n",
      "precision: 0.7583955223880597, recall: 0.7565098407784167, f1score_macro: 0.7574282147315855\n",
      " \n",
      "0.36\n",
      "precision: [0.83487941 0.69056604], recall: [0.84586466 0.67279412], f1score: [0.84033613 0.68156425]\n",
      "macro:\n",
      "precision: 0.7627227220219134, recall: 0.759329389650597, f1score_macro: 0.7609501901319187\n",
      " \n",
      "0.365\n",
      "precision: [0.83333333 0.68939394], recall: [0.84586466 0.66911765], f1score: [0.83955224 0.67910448]\n",
      "macro:\n",
      "precision: 0.7613636363636365, recall: 0.7574911543564794, f1score_macro: 0.7593283582089553\n",
      " \n",
      "0.37\n",
      "precision: [0.8302583 0.6870229], recall: [0.84586466 0.66176471], f1score: [0.83798883 0.6741573 ]\n",
      "macro:\n",
      "precision: 0.7586406016731924, recall: 0.7538146837682441, f1score_macro: 0.7560730650932145\n",
      " \n",
      "0.375\n",
      "precision: [0.82846715 0.6953125 ], recall: [0.85338346 0.65441176], f1score: [0.84074074 0.67424242]\n",
      "macro:\n",
      "precision: 0.7618898266423357, recall: 0.7538976116762495, f1score_macro: 0.7574915824915825\n",
      " \n",
      "0.38\n",
      "precision: [0.82727273 0.69685039], recall: [0.85526316 0.65073529], f1score: [0.84103512 0.6730038 ]\n",
      "macro:\n",
      "precision: 0.7620615604867573, recall: 0.752999226006192, f1score_macro: 0.7570194612146215\n",
      " \n",
      "0.385\n",
      "precision: [0.82640145 0.70119522], recall: [0.85902256 0.64705882], f1score: [0.84239631 0.67304015]\n",
      "macro:\n",
      "precision: 0.7637983328890586, recall: 0.7530406899601947, f1score_macro: 0.7577182331638632\n",
      " \n",
      "0.39\n",
      "precision: [0.82342342 0.69879518], recall: [0.85902256 0.63970588], f1score: [0.84084637 0.66794626]\n",
      "macro:\n",
      "precision: 0.7611093020731575, recall: 0.7493642193719593, f1score_macro: 0.7543963116715255\n",
      " \n",
      "0.395\n",
      "precision: [0.82374101 0.7016129 ], recall: [0.86090226 0.63970588], f1score: [0.84191176 0.66923077]\n",
      "macro:\n",
      "precision: 0.7626769552100255, recall: 0.7503040689960194, f1score_macro: 0.7555712669683258\n",
      " \n",
      "0.4\n",
      "precision: [0.82437276 0.70731707], recall: [0.86466165 0.63970588], f1score: [0.8440367  0.67181467]\n",
      "macro:\n",
      "precision: 0.7658449165136813, recall: 0.7521837682441397, f1score_macro: 0.757925684531189\n",
      " \n",
      "0.405\n",
      "precision: [0.82352941 0.71193416], recall: [0.86842105 0.63602941], f1score: [0.84537969 0.67184466]\n",
      "macro:\n",
      "precision: 0.7677317840716533, recall: 0.7522252321981424, f1score_macro: 0.7586121745618633\n",
      " \n",
      "0.41000000000000003\n",
      "precision: [0.82206406 0.7107438 ], recall: [0.86842105 0.63235294], f1score: [0.84460695 0.6692607 ]\n",
      "macro:\n",
      "precision: 0.7664039292961972, recall: 0.7503869969040248, f1score_macro: 0.7569338236863259\n",
      " \n",
      "0.41500000000000004\n",
      "precision: [0.82269504 0.71666667], recall: [0.87218045 0.63235294], f1score: [0.84671533 0.671875  ]\n",
      "macro:\n",
      "precision: 0.7696808510638298, recall: 0.752266696152145, f1score_macro: 0.7592951642335766\n",
      " \n",
      "0.42000000000000004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: [0.82300885 0.71966527], recall: [0.87406015 0.63235294], f1score: [0.84776664 0.67318982]\n",
      "macro:\n",
      "precision: 0.7713370607620247, recall: 0.7532065457762052, f1score_macro: 0.7604782300777606\n",
      " \n",
      "0.425\n",
      "precision: [0.82394366 0.72881356], recall: [0.87969925 0.63235294], f1score: [0.85090909 0.67716535]\n",
      "macro:\n",
      "precision: 0.7763786106469324, recall: 0.7560260946483857, f1score_macro: 0.7640372226198998\n",
      " \n",
      "0.43\n",
      "precision: [0.82249561 0.72765957], recall: [0.87969925 0.62867647], f1score: [0.85013624 0.67455621]\n",
      "macro:\n",
      "precision: 0.7750775903974871, recall: 0.754187859354268, f1score_macro: 0.7623462263998839\n",
      " \n",
      "0.435\n",
      "precision: [0.81993007 0.72844828], recall: [0.88157895 0.62132353], f1score: [0.84963768 0.67063492]\n",
      "macro:\n",
      "precision: 0.7741891728960695, recall: 0.7514512383900929, f1score_macro: 0.7601363008971704\n",
      " \n",
      "0.44\n",
      "precision: [0.82086957 0.73799127], recall: [0.88721805 0.62132353], f1score: [0.85275519 0.6746507 ]\n",
      "macro:\n",
      "precision: 0.7794304157964687, recall: 0.7542707872622734, f1score_macro: 0.7637029464107017\n",
      " \n",
      "0.445\n",
      "precision: [0.82068966 0.75      ], recall: [0.89473684 0.61764706], f1score: [0.85611511 0.67741935]\n",
      "macro:\n",
      "precision: 0.7853448275862069, recall: 0.7561919504643964, f1score_macro: 0.7667672313761894\n",
      " \n",
      "0.45\n",
      "precision: [0.81818182 0.75113122], recall: [0.89661654 0.61029412], f1score: [0.85560538 0.67342799]\n",
      "macro:\n",
      "precision: 0.7846565199506377, recall: 0.7534553295002211, f1score_macro: 0.7645166865261646\n",
      " \n",
      "0.455\n",
      "precision: [0.81678082 0.75      ], recall: [0.89661654 0.60661765], f1score: [0.85483871 0.67073171]\n",
      "macro:\n",
      "precision: 0.783390410958904, recall: 0.7516170942061035, f1score_macro: 0.7627852084972463\n",
      " \n",
      "0.46\n",
      "precision: [0.81538462 0.74885845], recall: [0.89661654 0.60294118], f1score: [0.85407341 0.66802444]\n",
      "macro:\n",
      "precision: 0.7821215314365999, recall: 0.7497788589119858, f1score_macro: 0.7610489254203232\n",
      " \n",
      "0.465\n",
      "precision: [0.81431005 0.75115207], recall: [0.89849624 0.59926471], f1score: [0.85433423 0.66666667]\n",
      "macro:\n",
      "precision: 0.7827310624200221, recall: 0.7488804732419283, f1score_macro: 0.7605004468275246\n",
      " \n",
      "0.47000000000000003\n",
      "precision: [0.81355932 0.75700935], recall: [0.90225564 0.59558824], f1score: [0.85561497 0.66666667]\n",
      "macro:\n",
      "precision: 0.7852843339141454, recall: 0.748921937195931, f1score_macro: 0.7611408199643495\n",
      " \n",
      "0.47500000000000003\n",
      "precision: [0.81281619 0.76303318], recall: [0.90601504 0.59191176], f1score: [0.85688889 0.66666667]\n",
      "macro:\n",
      "precision: 0.787924682112801, recall: 0.7489634011499336, f1score_macro: 0.7617777777777779\n",
      " \n",
      "0.48000000000000004\n",
      "precision: [0.81176471 0.76555024], recall: [0.90789474 0.58823529], f1score: [0.85714286 0.66528067]\n",
      "macro:\n",
      "precision: 0.7886574725584014, recall: 0.7480650154798762, f1score_macro: 0.7612117612117612\n",
      " \n",
      "0.485\n",
      "precision: [0.81103679 0.77184466], recall: [0.91165414 0.58455882], f1score: [0.85840708 0.66527197]\n",
      "macro:\n",
      "precision: 0.7914407247459168, recall: 0.7481064794338788, f1score_macro: 0.7618395230866072\n",
      " \n",
      "0.49\n",
      "precision: [0.8096828  0.77073171], recall: [0.91165414 0.58088235], f1score: [0.8576481  0.66247379]\n",
      "macro:\n",
      "precision: 0.7902072559957654, recall: 0.7462682441397612, f1score_macro: 0.7600609467883379\n",
      " \n",
      "0.495\n",
      "precision: [0.81031614 0.77832512], recall: [0.91541353 0.58088235], f1score: [0.85966461 0.66526316]\n",
      "macro:\n",
      "precision: 0.7943206314598821, recall: 0.7481479433878815, f1score_macro: 0.7624638825660798\n",
      " \n",
      "0.5\n",
      "precision: [0.8089701  0.77722772], recall: [0.91541353 0.57720588], f1score: [0.85890653 0.66244726]\n",
      "macro:\n",
      "precision: 0.7930989112200257, recall: 0.7463097080937637, f1score_macro: 0.7606768914785792\n",
      " \n",
      "0.505\n",
      "precision: [0.80629139 0.775     ], recall: [0.91541353 0.56985294], f1score: [0.85739437 0.65677966]\n",
      "macro:\n",
      "precision: 0.7906456953642385, recall: 0.7426332375055285, f1score_macro: 0.7570870136070661\n",
      " \n",
      "0.51\n",
      "precision: [0.80363036 0.77272727], recall: [0.91541353 0.5625    ], f1score: [0.85588752 0.65106383]\n",
      "macro:\n",
      "precision: 0.7881788178817881, recall: 0.7389567669172932, f1score_macro: 0.7534756758777998\n",
      " \n",
      "0.515\n",
      "precision: [0.80098684 0.77040816], recall: [0.91541353 0.55514706], f1score: [0.85438596 0.64529915]\n",
      "macro:\n",
      "precision: 0.7856975026852846, recall: 0.7352802963290579, f1score_macro: 0.7498425551057131\n",
      " \n",
      "0.52\n",
      "precision: [0.79901961 0.77604167], recall: [0.91917293 0.54779412], f1score: [0.8548951  0.64224138]\n",
      "macro:\n",
      "precision: 0.787530637254902, recall: 0.733483524988943, f1score_macro: 0.7485682421027249\n",
      " \n",
      "0.525\n",
      "precision: [0.79901961 0.77604167], recall: [0.91917293 0.54779412], f1score: [0.8548951  0.64224138]\n",
      "macro:\n",
      "precision: 0.787530637254902, recall: 0.733483524988943, f1score_macro: 0.7485682421027249\n",
      " \n",
      "0.53\n",
      "precision: [0.7980456  0.77894737], recall: [0.92105263 0.54411765], f1score: [0.85514834 0.64069264]\n",
      "macro:\n",
      "precision: 0.788496485513458, recall: 0.7325851393188854, f1score_macro: 0.7479204913759887\n",
      " \n",
      "0.535\n",
      "precision: [0.79674797 0.77777778], recall: [0.92105263 0.54044118], f1score: [0.85440279 0.63774403]\n",
      "macro:\n",
      "precision: 0.7872628726287263, recall: 0.7307469040247678, f1score_macro: 0.7460734122969096\n",
      " \n",
      "0.54\n",
      "precision: [0.79578606 0.78074866], recall: [0.92293233 0.53676471], f1score: [0.85465622 0.63616558]\n",
      "macro:\n",
      "precision: 0.7882673623449674, recall: 0.7298485183547103, f1score_macro: 0.7454109000722424\n",
      " \n",
      "0.545\n",
      "precision: [0.79549114 0.79234973], recall: [0.92857143 0.53308824], f1score: [0.85689506 0.63736264]\n",
      "macro:\n",
      "precision: 0.7939204350465933, recall: 0.7308298319327731, f1score_macro: 0.7471288468686561\n",
      " \n",
      "0.55\n",
      "precision: [0.79549114 0.79234973], recall: [0.92857143 0.53308824], f1score: [0.85689506 0.63736264]\n",
      "macro:\n",
      "precision: 0.7939204350465933, recall: 0.7308298319327731, f1score_macro: 0.7471288468686561\n",
      " \n",
      "0.555\n",
      "precision: [0.79454254 0.79558011], recall: [0.93045113 0.52941176], f1score: [0.85714286 0.63576159]\n",
      "macro:\n",
      "precision: 0.7950613233064037, recall: 0.7299314462627156, f1score_macro: 0.7464522232734152\n",
      " \n",
      "0.56\n",
      "precision: [0.79233227 0.79775281], recall: [0.93233083 0.52205882], f1score: [0.8566494  0.63111111]\n",
      "macro:\n",
      "precision: 0.7950425386796856, recall: 0.7271948252985405, f1score_macro: 0.7438802533103052\n",
      " \n",
      "0.5650000000000001\n",
      "precision: [0.79140127 0.80113636], recall: [0.93421053 0.51838235], f1score: [0.85689655 0.62946429]\n",
      "macro:\n",
      "precision: 0.796268818760857, recall: 0.726296439628483, f1score_macro: 0.7431804187192119\n",
      " \n",
      "0.5700000000000001\n",
      "precision: [0.79080824 0.80924855], recall: [0.93796992 0.51470588], f1score: [0.85812554 0.62921348]\n",
      "macro:\n",
      "precision: 0.8000283979003875, recall: 0.7263379035824856, f1score_macro: 0.7436695102746674\n",
      " \n",
      "0.5750000000000001\n",
      "precision: [0.79113924 0.81395349], recall: [0.93984962 0.51470588], f1score: [0.85910653 0.63063063]\n",
      "macro:\n",
      "precision: 0.802546364439211, recall: 0.7272777532065458, f1score_macro: 0.7448685799201262\n",
      " \n",
      "0.5800000000000001\n",
      "precision: [0.79113924 0.81395349], recall: [0.93984962 0.51470588], f1score: [0.85910653 0.63063063]\n",
      "macro:\n",
      "precision: 0.802546364439211, recall: 0.7272777532065458, f1score_macro: 0.7448685799201262\n",
      " \n",
      "0.585\n",
      "precision: [0.78897638 0.81656805], recall: [0.94172932 0.50735294], f1score: [0.85861183 0.62585034]\n",
      "macro:\n",
      "precision: 0.802772212645017, recall: 0.7245411322423706, f1score_macro: 0.7422310826644283\n",
      " \n",
      "0.59\n",
      "precision: [0.78806907 0.82035928], recall: [0.94360902 0.50367647], f1score: [0.85885372 0.62414579]\n",
      "macro:\n",
      "precision: 0.8042141776102427, recall: 0.7236427465723132, f1score_macro: 0.7414997535030817\n",
      " \n",
      "0.595\n",
      "precision: [0.78683386 0.81927711], recall: [0.94360902 0.5       ], f1score: [0.85811966 0.62100457]\n",
      "macro:\n",
      "precision: 0.803055482116554, recall: 0.7218045112781954, f1score_macro: 0.7395621121648519\n",
      " \n",
      "0.6\n",
      "precision: [0.7875     0.82926829], recall: [0.94736842 0.5       ], f1score: [0.86006826 0.62385321]\n",
      "macro:\n",
      "precision: 0.8083841463414634, recall: 0.7236842105263157, f1score_macro: 0.7419607351974199\n",
      " \n",
      "0.605\n",
      "precision: [0.78816199 0.83950617], recall: [0.95112782 0.5       ], f1score: [0.86201022 0.62672811]\n",
      "macro:\n",
      "precision: 0.8138340833044883, recall: 0.7255639097744361, f1score_macro: 0.7443691660320775\n",
      " \n",
      "0.61\n",
      "precision: [0.78693624 0.83850932], recall: [0.95112782 0.49632353], f1score: [0.8612766  0.62355658]\n",
      "macro:\n",
      "precision: 0.8127227765810496, recall: 0.7237256744803184, f1score_macro: 0.742416588865412\n",
      " \n",
      "0.615\n",
      "precision: [0.78693624 0.83850932], recall: [0.95112782 0.49632353], f1score: [0.8612766  0.62355658]\n",
      "macro:\n",
      "precision: 0.8127227765810496, recall: 0.7237256744803184, f1score_macro: 0.742416588865412\n",
      " \n",
      "0.62\n",
      "precision: [0.78120185 0.83870968], recall: [0.95300752 0.47794118], f1score: [0.85859441 0.6088993 ]\n",
      "macro:\n",
      "precision: 0.8099557632089069, recall: 0.7154743476337904, f1score_macro: 0.7337468544697762\n",
      " \n",
      "0.625\n",
      "precision: [0.78120185 0.83870968], recall: [0.95300752 0.47794118], f1score: [0.85859441 0.6088993 ]\n",
      "macro:\n",
      "precision: 0.8099557632089069, recall: 0.7154743476337904, f1score_macro: 0.7337468544697762\n",
      " \n",
      "0.63\n",
      "precision: [0.77880184 0.83660131], recall: [0.95300752 0.47058824], f1score: [0.85714286 0.60235294]\n",
      "macro:\n",
      "precision: 0.8077015752537574, recall: 0.7117978770455551, f1score_macro: 0.7297478991596639\n",
      " \n",
      "0.635\n",
      "precision: [0.77794793 0.8410596 ], recall: [0.95488722 0.46691176], f1score: [0.85738397 0.60047281]\n",
      "macro:\n",
      "precision: 0.8095037676338448, recall: 0.7108994913754976, f1score_macro: 0.7289283897417482\n",
      " \n",
      "0.64\n",
      "precision: [0.77675841 0.84      ], recall: [0.95488722 0.46323529], f1score: [0.85666105 0.5971564 ]\n",
      "macro:\n",
      "precision: 0.8083792048929663, recall: 0.7090612560813799, f1score_macro: 0.7269087218177313\n",
      " \n",
      "0.645\n",
      "precision: [0.77709924 0.84563758], recall: [0.95676692 0.46323529], f1score: [0.85762426 0.59857482]\n",
      "macro:\n",
      "precision: 0.8113684102669194, recall: 0.7100011057054401, f1score_macro: 0.7280995423501232\n",
      " \n",
      "0.65\n",
      "precision: [0.77709924 0.84563758], recall: [0.95676692 0.46323529], f1score: [0.85762426 0.59857482]\n",
      "macro:\n",
      "precision: 0.8113684102669194, recall: 0.7100011057054401, f1score_macro: 0.7280995423501232\n",
      " \n",
      "0.655\n",
      "precision: [0.77591463 0.84459459], recall: [0.95676692 0.45955882], f1score: [0.85690236 0.5952381 ]\n",
      "macro:\n",
      "precision: 0.8102546143704681, recall: 0.7081628704113224, f1score_macro: 0.7260702260702261\n",
      " \n",
      "0.66\n",
      "precision: [0.7723824  0.84137931], recall: [0.95676692 0.44852941], f1score: [0.85474391 0.58513189]\n",
      "macro:\n",
      "precision: 0.8068808539584533, recall: 0.7026481645289695, f1score_macro: 0.719937903581417\n",
      " \n",
      "0.665\n",
      "precision: [0.77155825 0.84615385], recall: [0.95864662 0.44485294], f1score: [0.85498743 0.58313253]\n",
      "macro:\n",
      "precision: 0.8088560456185268, recall: 0.701749778858912, f1score_macro: 0.7190599783879862\n",
      " \n",
      "0.67\n",
      "precision: [0.76876877 0.85507246], recall: [0.96240602 0.43382353], f1score: [0.85475793 0.57560976]\n",
      "macro:\n",
      "precision: 0.8119206162684424, recall: 0.6981147722246793, f1score_macro: 0.7151838429903498\n",
      " \n",
      "0.675\n",
      "precision: [0.76796407 0.86029412], recall: [0.96428571 0.43014706], f1score: [0.855      0.57352941]\n",
      "macro:\n",
      "precision: 0.8141290947516732, recall: 0.6972163865546219, f1score_macro: 0.714264705882353\n",
      " \n",
      "0.68\n",
      "precision: [0.76681614 0.85925926], recall: [0.96428571 0.42647059], f1score: [0.85428809 0.57002457]\n",
      "macro:\n",
      "precision: 0.8130377013785086, recall: 0.6953781512605042, f1score_macro: 0.7121563316400952\n",
      " \n",
      "0.685\n",
      "precision: [0.76339286 0.85606061], recall: [0.96428571 0.41544118], f1score: [0.85215947 0.55940594]\n",
      "macro:\n",
      "precision: 0.8097267316017316, recall: 0.6898634453781513, f1score_macro: 0.7057827045162988\n",
      " \n",
      "0.6900000000000001\n",
      "precision: [0.76339286 0.85606061], recall: [0.96428571 0.41544118], f1score: [0.85215947 0.55940594]\n",
      "macro:\n",
      "precision: 0.8097267316017316, recall: 0.6898634453781513, f1score_macro: 0.7057827045162988\n",
      " \n",
      "0.6950000000000001\n",
      "precision: [0.76225854 0.85496183], recall: [0.96428571 0.41176471], f1score: [0.85145228 0.55583127]\n",
      "macro:\n",
      "precision: 0.8086101879473249, recall: 0.6880252100840336, f1score_macro: 0.7036417738331806\n",
      " \n",
      "0.7000000000000001\n",
      "precision: [0.76035503 0.859375  ], recall: [0.96616541 0.40441176], f1score: [0.85099338 0.55      ]\n",
      "macro:\n",
      "precision: 0.8098650147928994, recall: 0.6852885891198585, f1score_macro: 0.7004966887417219\n",
      " \n",
      "0.7050000000000001\n",
      "precision: [0.76035503 0.859375  ], recall: [0.96616541 0.40441176], f1score: [0.85099338 0.55      ]\n",
      "macro:\n",
      "precision: 0.8098650147928994, recall: 0.6852885891198585, f1score_macro: 0.7004966887417219\n",
      " \n",
      "0.71\n",
      "precision: [0.75958702 0.86507937], recall: [0.96804511 0.40073529], f1score: [0.85123967 0.54773869]\n",
      "macro:\n",
      "precision: 0.8123331928641664, recall: 0.6843902034498011, f1score_macro: 0.6994891814444122\n",
      " \n",
      "0.715\n",
      "precision: [0.75846834 0.864     ], recall: [0.96804511 0.39705882], f1score: [0.85053675 0.5440806 ]\n",
      "macro:\n",
      "precision: 0.8112341678939616, recall: 0.6825519681556833, f1score_macro: 0.6973086755122544\n",
      " \n",
      "0.72\n",
      "precision: [0.75735294 0.86290323], recall: [0.96804511 0.39338235], f1score: [0.84983498 0.54040404]\n",
      "macro:\n",
      "precision: 0.810128083491461, recall: 0.6807137328615657, f1score_macro: 0.695119511951195\n",
      " \n",
      "0.725\n",
      "precision: [0.75584795 0.875     ], recall: [0.97180451 0.38602941], f1score: [0.85032895 0.53571429]\n",
      "macro:\n",
      "precision: 0.8154239766081872, recall: 0.6789169615214506, f1score_macro: 0.6930216165413534\n",
      " \n",
      "0.73\n",
      "precision: [0.75545852 0.88888889], recall: [0.97556391 0.38235294], f1score: [0.85151764 0.53470437]\n",
      "macro:\n",
      "precision: 0.8221737020863658, recall: 0.6789584254754533, f1score_macro: 0.6931110037938298\n",
      " \n",
      "0.735\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: [0.7532656  0.88695652], recall: [0.97556391 0.375     ], f1score: [0.85012285 0.52713178]\n",
      "macro:\n",
      "precision: 0.8201110620306682, recall: 0.675281954887218, f1score_macro: 0.6886273165342933\n",
      " \n",
      "0.74\n",
      "precision: [0.75397974 0.90265487], recall: [0.97932331 0.375     ], f1score: [0.85200327 0.52987013]\n",
      "macro:\n",
      "precision: 0.8283173033822984, recall: 0.6771616541353384, f1score_macro: 0.6909367002580412\n",
      " \n",
      "0.745\n",
      "precision: [0.75397974 0.90265487], recall: [0.97932331 0.375     ], f1score: [0.85200327 0.52987013]\n",
      "macro:\n",
      "precision: 0.8283173033822984, recall: 0.6771616541353384, f1score_macro: 0.6909367002580412\n",
      " \n",
      "0.75\n",
      "precision: [0.75180375 0.9009009 ], recall: [0.97932331 0.36764706], f1score: [0.85061224 0.52219321]\n",
      "macro:\n",
      "precision: 0.8263523263523264, recall: 0.6734851835471031, f1score_macro: 0.6864027281931049\n",
      " \n",
      "0.755\n",
      "precision: [0.75216138 0.90909091], recall: [0.98120301 0.36764706], f1score: [0.85154976 0.52356021]\n",
      "macro:\n",
      "precision: 0.8306261461881058, recall: 0.6744250331711632, f1score_macro: 0.6875549823629391\n",
      " \n",
      "0.76\n",
      "precision: [0.75143678 0.91666667], recall: [0.98308271 0.36397059], f1score: [0.85179153 0.52105263]\n",
      "macro:\n",
      "precision: 0.834051724137931, recall: 0.6735266475011057, f1score_macro: 0.6864220812617865\n",
      " \n",
      "0.765\n",
      "precision: [0.74857143 0.92307692], recall: [0.98496241 0.35294118], f1score: [0.85064935 0.5106383 ]\n",
      "macro:\n",
      "precision: 0.8358241758241758, recall: 0.6689517912428129, f1score_macro: 0.6806438242608456\n",
      " \n",
      "0.77\n",
      "precision: [0.74857143 0.92307692], recall: [0.98496241 0.35294118], f1score: [0.85064935 0.5106383 ]\n",
      "macro:\n",
      "precision: 0.8358241758241758, recall: 0.6689517912428129, f1score_macro: 0.6806438242608456\n",
      " \n",
      "0.775\n",
      "precision: [0.74786325 0.93137255], recall: [0.98684211 0.34926471], f1score: [0.85089141 0.50802139]\n",
      "macro:\n",
      "precision: 0.8396178984414279, recall: 0.6680534055727554, f1score_macro: 0.6794564002114769\n",
      " \n",
      "0.78\n",
      "precision: [0.74679943 0.93069307], recall: [0.98684211 0.34558824], f1score: [0.85020243 0.50402145]\n",
      "macro:\n",
      "precision: 0.838746250158444, recall: 0.6662151702786377, f1score_macro: 0.6771119384354884\n",
      " \n",
      "0.785\n",
      "precision: [0.74468085 0.92929293], recall: [0.98684211 0.33823529], f1score: [0.84882781 0.49595687]\n",
      "macro:\n",
      "precision: 0.8369868901783795, recall: 0.6625386996904025, f1score_macro: 0.6723923412656043\n",
      " \n",
      "0.79\n",
      "precision: [0.74468085 0.92929293], recall: [0.98684211 0.33823529], f1score: [0.84882781 0.49595687]\n",
      "macro:\n",
      "precision: 0.8369868901783795, recall: 0.6625386996904025, f1score_macro: 0.6723923412656043\n",
      " \n",
      "0.795\n",
      "precision: [0.74362606 0.92857143], recall: [0.98684211 0.33455882], f1score: [0.84814216 0.49189189]\n",
      "macro:\n",
      "precision: 0.8360987454471873, recall: 0.6607004643962848, f1score_macro: 0.670017028336899\n",
      " \n",
      "0.8\n",
      "precision: [0.74362606 0.92857143], recall: [0.98684211 0.33455882], f1score: [0.84814216 0.49189189]\n",
      "macro:\n",
      "precision: 0.8360987454471873, recall: 0.6607004643962848, f1score_macro: 0.670017028336899\n",
      " \n",
      "0.805\n",
      "precision: [0.74188999 0.93684211], recall: [0.9887218  0.32720588], f1score: [0.84770346 0.48501362]\n",
      "macro:\n",
      "precision: 0.8393660455793928, recall: 0.6579638434321097, f1score_macro: 0.6663585444629122\n",
      " \n",
      "0.81\n",
      "precision: [0.73913043 0.94505495], recall: [0.9906015  0.31617647], f1score: [0.84658635 0.4738292 ]\n",
      "macro:\n",
      "precision: 0.8420926899187768, recall: 0.6533889871738169, f1score_macro: 0.6602077732417272\n",
      " \n",
      "0.8150000000000001\n",
      "precision: [0.73913043 0.94505495], recall: [0.9906015  0.31617647], f1score: [0.84658635 0.4738292 ]\n",
      "macro:\n",
      "precision: 0.8420926899187768, recall: 0.6533889871738169, f1score_macro: 0.6602077732417272\n",
      " \n",
      "0.8200000000000001\n",
      "precision: [0.73913043 0.94505495], recall: [0.9906015  0.31617647], f1score: [0.84658635 0.4738292 ]\n",
      "macro:\n",
      "precision: 0.8420926899187768, recall: 0.6533889871738169, f1score_macro: 0.6602077732417272\n",
      " \n",
      "0.8250000000000001\n",
      "precision: [0.73913043 0.94505495], recall: [0.9906015  0.31617647], f1score: [0.84658635 0.4738292 ]\n",
      "macro:\n",
      "precision: 0.8420926899187768, recall: 0.6533889871738169, f1score_macro: 0.6602077732417272\n",
      " \n",
      "0.8300000000000001\n",
      "precision: [0.73809524 0.94444444], recall: [0.9906015 0.3125   ], f1score: [0.8459069  0.46961326]\n",
      "macro:\n",
      "precision: 0.8412698412698413, recall: 0.6515507518796992, f1score_macro: 0.6577600808775929\n",
      " \n",
      "0.8350000000000001\n",
      "precision: [0.73500697 0.94252874], recall: [0.9906015  0.30147059], f1score: [0.8438751  0.45682451]\n",
      "macro:\n",
      "precision: 0.8387678545664405, recall: 0.6460360459973463, f1score_macro: 0.6503498063074415\n",
      " \n",
      "0.84\n",
      "precision: [0.73194444 0.94047619], recall: [0.9906015  0.29044118], f1score: [0.84185304 0.44382022]\n",
      "macro:\n",
      "precision: 0.8362103174603175, recall: 0.6405213401149934, f1score_macro: 0.6428366299314355\n",
      " \n",
      "0.845\n",
      "precision: [0.72790055 0.9375    ], recall: [0.9906015  0.27573529], f1score: [0.83917197 0.42613636]\n",
      "macro:\n",
      "precision: 0.832700276243094, recall: 0.6331683989385228, f1score_macro: 0.6326541690793283\n",
      " \n",
      "0.85\n",
      "precision: [0.72489684 0.93506494], recall: [0.9906015  0.26470588], f1score: [0.83717236 0.41260745]\n",
      "macro:\n",
      "precision: 0.8299808856892763, recall: 0.6276536930561698, f1score_macro: 0.6248899044359124\n",
      " \n",
      "0.855\n",
      "precision: [0.72093023 0.93150685], recall: [0.9906015 0.25     ], f1score: [0.83452098 0.3942029 ]\n",
      "macro:\n",
      "precision: 0.826218540936604, recall: 0.6203007518796992, f1score_macro: 0.6143619401700575\n",
      " \n",
      "0.86\n",
      "precision: [0.71896317 0.92957746], recall: [0.9906015  0.24264706], f1score: [0.83320158 0.38483965]\n",
      "macro:\n",
      "precision: 0.8242703149318833, recall: 0.616624281291464, f1score_macro: 0.6090206155867203\n",
      " \n",
      "0.865\n",
      "precision: [0.71798365 0.92857143], recall: [0.9906015  0.23897059], f1score: [0.83254344 0.38011696]\n",
      "macro:\n",
      "precision: 0.8232775398987933, recall: 0.6147860459973463, f1score_macro: 0.6063302014910894\n",
      " \n",
      "0.87\n",
      "precision: [0.71506106 0.92537313], recall: [0.9906015  0.22794118], f1score: [0.83057526 0.36578171]\n",
      "macro:\n",
      "precision: 0.8202170963364993, recall: 0.6092713401149934, f1score_macro: 0.5981784835108126\n",
      " \n",
      "0.875\n",
      "precision: [0.71409214 0.92424242], recall: [0.9906015  0.22426471], f1score: [0.82992126 0.36094675]\n",
      "macro:\n",
      "precision: 0.8191672825819167, recall: 0.6074331048208758, f1score_macro: 0.5954340027023249\n",
      " \n",
      "0.88\n",
      "precision: [0.71351351 0.9375    ], recall: [0.9924812  0.22058824], f1score: [0.83018868 0.35714286]\n",
      "macro:\n",
      "precision: 0.8255067567567568, recall: 0.6065347191508181, f1score_macro: 0.59366576819407\n",
      " \n",
      "0.885\n",
      "precision: [0.70967742 0.93333333], recall: [0.9924812  0.20588235], f1score: [0.82758621 0.3373494 ]\n",
      "macro:\n",
      "precision: 0.821505376344086, recall: 0.5991817779743476, f1score_macro: 0.5824678022434566\n",
      " \n",
      "0.89\n",
      "precision: [0.70872483 0.93220339], recall: [0.9924812  0.20220588], f1score: [0.82693814 0.33232628]\n",
      "macro:\n",
      "precision: 0.8204641110226367, recall: 0.5973435426802299, f1score_macro: 0.5796322101223836\n",
      " \n",
      "0.895\n",
      "precision: [0.70721925 0.94642857], recall: [0.9943609  0.19485294], f1score: [0.8265625  0.32317073]\n",
      "macro:\n",
      "precision: 0.8268239113827349, recall: 0.5946069217160549, f1score_macro: 0.5748666158536585\n",
      " \n",
      "0.9\n",
      "precision: [0.70439414 0.94339623], recall: [0.9943609  0.18382353], f1score: [0.82462977 0.30769231]\n",
      "macro:\n",
      "precision: 0.8238951837801171, recall: 0.589092215833702, f1score_macro: 0.566161040829786\n",
      " \n",
      "macro 0.7667672313761894\n",
      "F1-valued: 0.6774193548387097\n",
      "recall-valued: 0.6176470588235294\n",
      "0.445\n"
     ]
    }
   ],
   "source": [
    "predictions0=[]\n",
    "predictions1=[]\n",
    "\n",
    "\n",
    "for res in y_pred:\n",
    "    predictions0.append(res[0])\n",
    "    predictions1.append(res[1])\n",
    "\n",
    "    \n",
    "test_data['predict_0']=predictions0\n",
    "test_data['predict_1']=predictions1\n",
    "\n",
    "test_data_predict=TestCutoff(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5787bc8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: [0.82068966 0.75      ], recall: [0.89473684 0.61764706], f1score: [0.85611511 0.67741935]\n"
     ]
    }
   ],
   "source": [
    "precision, recall, f1score = precision_recall_fscore_support(test_data.label_test, test_data['predict'])[:3]\n",
    "\n",
    "print(f'precision: {precision}, recall: {recall}, f1score: {f1score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3f7ccb1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.7853448275862069, recall: 0.7561919504643964, f1score_macro: 0.7667672313761894\n"
     ]
    }
   ],
   "source": [
    "precision, recall, f1score = precision_recall_fscore_support(labels_test, test_data_predict['predict'], average='macro')[:3]\n",
    "\n",
    "print(f'precision: {precision}, recall: {recall}, f1score_macro: {f1score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7791af1a",
   "metadata": {},
   "source": [
    "# Logit Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "88134a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
      "250 fits failed out of a total of 500.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "250 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1091, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 61, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/model_selection/_search.py:953: UserWarning: One or more of the test scores are non-finite: [       nan 0.56538763        nan 0.69783346        nan 0.70081591\n",
      "        nan 0.7059337         nan 0.70811304        nan 0.70815462\n",
      "        nan 0.70824716        nan 0.70877065        nan 0.70872848\n",
      "        nan 0.70738554        nan 0.70625652        nan 0.70544166\n",
      "        nan 0.70435324        nan 0.70528979        nan 0.70478823\n",
      "        nan 0.7061049         nan 0.70490766        nan 0.70450031\n",
      "        nan 0.7037713         nan 0.70409696        nan 0.704674\n",
      "        nan 0.70378991        nan 0.70340885        nan 0.70398976\n",
      "        nan 0.70457864        nan 0.70446504        nan 0.70509339\n",
      "        nan 0.70402572        nan 0.70464343        nan 0.70417681\n",
      "        nan 0.70360259        nan 0.70395179        nan 0.70419702\n",
      "        nan 0.70415469        nan 0.70435194        nan 0.70477095\n",
      "        nan 0.70418905        nan 0.70476973        nan 0.70444199\n",
      "        nan 0.70397203        nan 0.70331091        nan 0.703242\n",
      "        nan 0.70345673        nan 0.70401291        nan 0.70369127\n",
      "        nan 0.7029103         nan 0.70288249        nan 0.70283825\n",
      "        nan 0.70329926        nan 0.70210661]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=LogisticRegression(class_weight={0: 0.7626476825204483,\n",
       "                                                        1: 1.4518454440599768}),\n",
       "             param_grid={&#x27;C&#x27;: array([1.00000000e-04, 2.04179592e-01, 4.08259184e-01, 6.12338776e-01,\n",
       "       8.16418367e-01, 1.02049796e+00, 1.22457755e+00, 1.42865714e+00,\n",
       "       1.63273673e+00, 1.83681633e+00, 2.04089592e+00, 2.24497551e+00,\n",
       "       2.44905510e+00, 2.65313469e+00, 2.85721429e+...\n",
       "       5.71432857e+00, 5.91840816e+00, 6.12248776e+00, 6.32656735e+00,\n",
       "       6.53064694e+00, 6.73472653e+00, 6.93880612e+00, 7.14288571e+00,\n",
       "       7.34696531e+00, 7.55104490e+00, 7.75512449e+00, 7.95920408e+00,\n",
       "       8.16328367e+00, 8.36736327e+00, 8.57144286e+00, 8.77552245e+00,\n",
       "       8.97960204e+00, 9.18368163e+00, 9.38776122e+00, 9.59184082e+00,\n",
       "       9.79592041e+00, 1.00000000e+01]),\n",
       "                         &#x27;penalty&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;]},\n",
       "             scoring=&#x27;f1_macro&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" ><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=LogisticRegression(class_weight={0: 0.7626476825204483,\n",
       "                                                        1: 1.4518454440599768}),\n",
       "             param_grid={&#x27;C&#x27;: array([1.00000000e-04, 2.04179592e-01, 4.08259184e-01, 6.12338776e-01,\n",
       "       8.16418367e-01, 1.02049796e+00, 1.22457755e+00, 1.42865714e+00,\n",
       "       1.63273673e+00, 1.83681633e+00, 2.04089592e+00, 2.24497551e+00,\n",
       "       2.44905510e+00, 2.65313469e+00, 2.85721429e+...\n",
       "       5.71432857e+00, 5.91840816e+00, 6.12248776e+00, 6.32656735e+00,\n",
       "       6.53064694e+00, 6.73472653e+00, 6.93880612e+00, 7.14288571e+00,\n",
       "       7.34696531e+00, 7.55104490e+00, 7.75512449e+00, 7.95920408e+00,\n",
       "       8.16328367e+00, 8.36736327e+00, 8.57144286e+00, 8.77552245e+00,\n",
       "       8.97960204e+00, 9.18368163e+00, 9.38776122e+00, 9.59184082e+00,\n",
       "       9.79592041e+00, 1.00000000e+01]),\n",
       "                         &#x27;penalty&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;]},\n",
       "             scoring=&#x27;f1_macro&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-17\" type=\"checkbox\" ><label for=\"sk-estimator-id-17\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(class_weight={0: 0.7626476825204483, 1: 1.4518454440599768})</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-18\" type=\"checkbox\" ><label for=\"sk-estimator-id-18\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(class_weight={0: 0.7626476825204483, 1: 1.4518454440599768})</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=LogisticRegression(class_weight={0: 0.7626476825204483,\n",
       "                                                        1: 1.4518454440599768}),\n",
       "             param_grid={'C': array([1.00000000e-04, 2.04179592e-01, 4.08259184e-01, 6.12338776e-01,\n",
       "       8.16418367e-01, 1.02049796e+00, 1.22457755e+00, 1.42865714e+00,\n",
       "       1.63273673e+00, 1.83681633e+00, 2.04089592e+00, 2.24497551e+00,\n",
       "       2.44905510e+00, 2.65313469e+00, 2.85721429e+...\n",
       "       5.71432857e+00, 5.91840816e+00, 6.12248776e+00, 6.32656735e+00,\n",
       "       6.53064694e+00, 6.73472653e+00, 6.93880612e+00, 7.14288571e+00,\n",
       "       7.34696531e+00, 7.55104490e+00, 7.75512449e+00, 7.95920408e+00,\n",
       "       8.16328367e+00, 8.36736327e+00, 8.57144286e+00, 8.77552245e+00,\n",
       "       8.97960204e+00, 9.18368163e+00, 9.38776122e+00, 9.59184082e+00,\n",
       "       9.79592041e+00, 1.00000000e+01]),\n",
       "                         'penalty': ['l1', 'l2']},\n",
       "             scoring='f1_macro')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {'C': np.linspace(0.0001, 10, 50), \"penalty\":[\"l1\",\"l2\"]}  #high C means \"Trust this training data a lot\", while a low value says \"This data may not be fully representative of the real world data, so if it's telling you to make a parameter really large, don't listen to it\"\n",
    "grid_search = GridSearchCV(LogisticRegression(class_weight={0:class_weight[0], 1:class_weight[1]}), parameters, cv=5, scoring='f1_macro')\n",
    "grid_search.fit(df_emb_, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "14da1726",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = grid_search.predict_proba(df_emb_test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7b560d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions0=[]\n",
    "predictions1=[]\n",
    "\n",
    "\n",
    "for res in y_pred:\n",
    "    predictions0.append(res[0])\n",
    "    predictions1.append(res[1])\n",
    "\n",
    "    \n",
    "test_data['predict_0']=predictions0\n",
    "test_data['predict_1']=predictions1\n",
    "\n",
    "test_data_predict=TestCutoff(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "354d900f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: [0.85514834 0.81818182], recall: [0.92105263 0.69485294], f1score: [0.88687783 0.75149105]\n"
     ]
    }
   ],
   "source": [
    "precision, recall, f1score = precision_recall_fscore_support(labels_test, test_data_predict['predict'])[:3]\n",
    "\n",
    "print(f'precision: {precision}, recall: {recall}, f1score: {f1score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "221a8fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.8366650801205775, recall: 0.807952786377709, f1score_macro: 0.8191844408661155\n"
     ]
    }
   ],
   "source": [
    "precision, recall, f1score = precision_recall_fscore_support(labels_test, test_data_predict['predict'], average='macro')[:3]\n",
    "\n",
    "print(f'precision: {precision}, recall: {recall}, f1score_macro: {f1score}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
